{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/huno/source/favicon-black-50.png","path":"favicon-black-50.png","modified":0,"renderable":1},{"_id":"themes/huno/source/favicon-black-55.png","path":"favicon-black-55.png","modified":0,"renderable":1},{"_id":"themes/huno/source/favicon-red-50.png","path":"favicon-red-50.png","modified":0,"renderable":1},{"_id":"themes/huno/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/huno/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"source/image/shizhai.jpg","path":"image/shizhai.jpg","modified":0,"renderable":0},{"_id":"themes/huno/source/css/archive.css","path":"css/archive.css","modified":0,"renderable":1},{"_id":"themes/huno/source/css/china-social-icon.css","path":"css/china-social-icon.css","modified":0,"renderable":1},{"_id":"themes/huno/source/css/animate.css","path":"css/animate.css","modified":0,"renderable":1},{"_id":"themes/huno/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/huno/source/css/uno.css","path":"css/uno.css","modified":0,"renderable":1},{"_id":"themes/huno/source/images/totop.png","path":"images/totop.png","modified":0,"renderable":1},{"_id":"themes/huno/source/js/awesome-toc.min.js","path":"js/awesome-toc.min.js","modified":0,"renderable":1},{"_id":"themes/huno/source/js/jquery.githubRepoWidget.min.js","path":"js/jquery.githubRepoWidget.min.js","modified":0,"renderable":1},{"_id":"themes/huno/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/huno/source/js/scale.fix.js","path":"js/scale.fix.js","modified":0,"renderable":1},{"_id":"source/image/Judo/Judo.jpg","path":"image/Judo/Judo.jpg","modified":0,"renderable":0},{"_id":"source/image/Yamaki/Yamaki-2.jpg","path":"image/Yamaki/Yamaki-2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/kodokan.png","path":"image/kodokan/kodokan.png","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_3.jpg","path":"image/sumiao/sumiao_3.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao2/W1.jpg","path":"image/sumiao2/W1.jpg","modified":0,"renderable":0},{"_id":"themes/huno/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0,"renderable":1},{"_id":"source/image/Fuji/Fuji_2.jpg","path":"image/Fuji/Fuji_2.jpg","modified":0,"renderable":0},{"_id":"source/image/Judo/Budokan.jpg","path":"image/Judo/Budokan.jpg","modified":0,"renderable":0},{"_id":"source/image/Judo/ONO SHOHEI.jpg","path":"image/Judo/ONO SHOHEI.jpg","modified":0,"renderable":0},{"_id":"source/image/Kendo/Kendo-1.jpg","path":"image/Kendo/Kendo-1.jpg","modified":0,"renderable":0},{"_id":"source/image/Yamaki/Yamaki-1.jpg","path":"image/Yamaki/Yamaki-1.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_2.jpg","path":"image/sumiao/sumiao_2.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_4.jpg","path":"image/sumiao/sumiao_4.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_5.jpg","path":"image/sumiao/sumiao_5.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_7.jpg","path":"image/sumiao/sumiao_7.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao/sumiao_6.jpg","path":"image/sumiao/sumiao_6.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao2/W.jpg","path":"image/sumiao2/W.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao2/W2.jpg","path":"image/sumiao2/W2.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao2/W3.jpg","path":"image/sumiao2/W3.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao2/W4.jpg","path":"image/sumiao2/W4.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY.jpg","path":"image/sumiao3/ZYY.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_1.jpg","path":"image/sumiao3/ZYY_1.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_2.jpg","path":"image/sumiao3/ZYY_2.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_back.jpg","path":"image/sumiao3/ZYY_back.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_back_0.jpg","path":"image/sumiao3/ZYY_back_0.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ.jpg","path":"image/sumiao4/MZ.jpg","modified":0,"renderable":0},{"_id":"source/image/swim/backstroke_1.gif","path":"image/swim/backstroke_1.gif","modified":0,"renderable":0},{"_id":"source/image/swim/backstroke_2.gif","path":"image/swim/backstroke_2.gif","modified":0,"renderable":0},{"_id":"source/image/swim/backstroke_3.gif","path":"image/swim/backstroke_3.gif","modified":0,"renderable":0},{"_id":"source/image/swim/breaststroke_1.gif","path":"image/swim/breaststroke_1.gif","modified":0,"renderable":0},{"_id":"source/image/swim/breaststroke_2.gif","path":"image/swim/breaststroke_2.gif","modified":0,"renderable":0},{"_id":"source/image/swim/butterfly_1.gif","path":"image/swim/butterfly_1.gif","modified":0,"renderable":0},{"_id":"source/image/swim/butterfly_2.gif","path":"image/swim/butterfly_2.gif","modified":0,"renderable":0},{"_id":"source/image/swim/butterfly_3.gif","path":"image/swim/butterfly_3.gif","modified":0,"renderable":0},{"_id":"source/image/swim/freestyle_1.gif","path":"image/swim/freestyle_1.gif","modified":0,"renderable":0},{"_id":"source/image/swim/freestyle_2.gif","path":"image/swim/freestyle_2.gif","modified":0,"renderable":0},{"_id":"source/image/swim/freestyle_3.gif","path":"image/swim/freestyle_3.gif","modified":0,"renderable":0},{"_id":"themes/huno/source/fonts/china-social/china-social.eot","path":"fonts/china-social/china-social.eot","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/china-social/china-social.ttf","path":"fonts/china-social/china-social.ttf","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/china-social/readme.html","path":"fonts/china-social/readme.html","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/china-social/china-social.woff","path":"fonts/china-social/china-social.woff","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.css","path":"fonts/foundation-icons/foundation-icons.css","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.eot","path":"fonts/foundation-icons/foundation-icons.eot","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.woff","path":"fonts/foundation-icons/foundation-icons.woff","modified":0,"renderable":1},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.ttf","path":"fonts/foundation-icons/foundation-icons.ttf","modified":0,"renderable":1},{"_id":"source/image/Carmen/Carmen_1.jpg","path":"image/Carmen/Carmen_1.jpg","modified":0,"renderable":0},{"_id":"source/image/Carmen/Carmen_2.jpg","path":"image/Carmen/Carmen_2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/1/simple-neuron.png","path":"image/DL/1/simple-neuron.png","modified":0,"renderable":0},{"_id":"source/image/Carmen/Carmen_3.jpg","path":"image/Carmen/Carmen_3.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/2/1.png","path":"image/DL/2/1.png","modified":0,"renderable":0},{"_id":"source/image/DL/2/2.png","path":"image/DL/2/2.png","modified":0,"renderable":0},{"_id":"source/image/DL/2/autoencoder_1.png","path":"image/DL/2/autoencoder_1.png","modified":0,"renderable":0},{"_id":"source/image/DL/4/1-3.png","path":"image/DL/4/1-3.png","modified":0,"renderable":0},{"_id":"source/image/DL/4/1-2.png","path":"image/DL/4/1-2.png","modified":0,"renderable":0},{"_id":"source/image/DL/4/1-4.png","path":"image/DL/4/1-4.png","modified":0,"renderable":0},{"_id":"source/image/DL/4/2-1.png","path":"image/DL/4/2-1.png","modified":0,"renderable":0},{"_id":"source/image/DL/5/1-1.jpg","path":"image/DL/5/1-1.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/5/1-2.jpg","path":"image/DL/5/1-2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/5/2-1.png","path":"image/DL/5/2-1.png","modified":0,"renderable":0},{"_id":"source/image/Fuji/Fuji_1.jpg","path":"image/Fuji/Fuji_1.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/1-1.jpg","path":"image/DL/3/1-1.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/1-2.jpg","path":"image/DL/3/1-2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/1-3.jpg","path":"image/DL/3/1-3.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/2-1.jpg","path":"image/DL/3/2-1.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/2-2.jpg","path":"image/DL/3/2-2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/3-1.jpg","path":"image/DL/3/3-1.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/3-2.jpg","path":"image/DL/3/3-2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/4-2.png","path":"image/DL/3/4-2.png","modified":0,"renderable":0},{"_id":"source/image/DL/3/4-3.jpg","path":"image/DL/3/4-3.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_1.jpg","path":"image/ML/3_1.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_6.jpg","path":"image/ML/3_6.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/2/1.jpg","path":"image/RL/2/1.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/2/3.jpg","path":"image/RL/2/3.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/2/2.jpg","path":"image/RL/2/2.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/3/3.jpg","path":"image/RL/3/3.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/3/4.png","path":"image/RL/3/4.png","modified":0,"renderable":0},{"_id":"source/image/RL/4/0-1.png","path":"image/RL/4/0-1.png","modified":0,"renderable":0},{"_id":"source/image/RL/4/2-1.svg","path":"image/RL/4/2-1.svg","modified":0,"renderable":0},{"_id":"source/image/RL/4/2-2.png","path":"image/RL/4/2-2.png","modified":0,"renderable":0},{"_id":"source/image/RL/4/2-3.svg","path":"image/RL/4/2-3.svg","modified":0,"renderable":0},{"_id":"source/image/RL/5/4-1.png","path":"image/RL/5/4-1.png","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_3.jpg","path":"image/sumiao3/ZYY_3.jpg","modified":0,"renderable":0},{"_id":"source/image/swim/breaststroke_3.gif","path":"image/swim/breaststroke_3.gif","modified":0,"renderable":0},{"_id":"themes/huno/source/fonts/china-social/china-social.svg","path":"fonts/china-social/china-social.svg","modified":0,"renderable":1},{"_id":"source/image/Blockchain/ETH/ETH.jpg","path":"image/Blockchain/ETH/ETH.jpg","modified":0,"renderable":0},{"_id":"source/image/Blockchain/EOS/EOS_DAPP.jpg","path":"image/Blockchain/EOS/EOS_DAPP.jpg","modified":0,"renderable":0},{"_id":"source/image/Blockchain/ETH/ETH_DAPP.jpg","path":"image/Blockchain/ETH/ETH_DAPP.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/2/3.gif","path":"image/DL/2/3.gif","modified":0,"renderable":0},{"_id":"source/image/DL/2/5.png","path":"image/DL/2/5.png","modified":0,"renderable":0},{"_id":"source/image/DL/2/CNN.jpg","path":"image/DL/2/CNN.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/2/cnnarchitecture.jpg","path":"image/DL/2/cnnarchitecture.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/4/1-1.png","path":"image/DL/4/1-1.png","modified":0,"renderable":0},{"_id":"source/image/DL/3/2-3.jpg","path":"image/DL/3/2-3.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/3/4-1.png","path":"image/DL/3/4-1.png","modified":0,"renderable":0},{"_id":"source/image/ML/1_1.jpg","path":"image/ML/1_1.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/1_3.jpg","path":"image/ML/1_3.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_1.jpg","path":"image/ML/2_1.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_3.jpg","path":"image/ML/2_3.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_4.jpg","path":"image/ML/2_4.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_5.jpg","path":"image/ML/2_5.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_7.jpg","path":"image/ML/2_7.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_8.jpg","path":"image/ML/2_8.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_2.jpg","path":"image/ML/3_2.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_3.jpg","path":"image/ML/3_3.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_4.jpg","path":"image/ML/3_4.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/3_5.jpg","path":"image/ML/3_5.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/4_1.jpg","path":"image/ML/4_1.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/5_2.jpg","path":"image/ML/5_2.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/3/1.jpg","path":"image/RL/3/1.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/3/2.jpg","path":"image/RL/3/2.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/5/4-2.png","path":"image/RL/5/4-2.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/4-3.png","path":"image/RL/5/4-3.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/4-4.png","path":"image/RL/5/4-4.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/1/kano_1.jpg","path":"image/kodokan/1/kano_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/two_buildings.jpg","path":"image/kodokan/2/two_buildings.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/3/fee.jpg","path":"image/kodokan/3/fee.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/3/female.jpg","path":"image/kodokan/3/female.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/3/male.jpg","path":"image/kodokan/3/male.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao3/ZYY_4.jpg","path":"image/sumiao3/ZYY_4.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_back.jpg","path":"image/sumiao4/MZ_back.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/5/1-5.png","path":"image/DL/5/1-5.png","modified":0,"renderable":0},{"_id":"source/image/Kendo/Kendo-2.jpg","path":"image/Kendo/Kendo-2.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/1_2.jpg","path":"image/ML/1_2.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_2.jpg","path":"image/ML/2_2.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/2_6.jpg","path":"image/ML/2_6.jpg","modified":0,"renderable":0},{"_id":"source/image/ML/4_2.jpg","path":"image/ML/4_2.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_2.jpg","path":"image/sumiao4/MZ_2.jpg","modified":0,"renderable":0},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.svg","path":"fonts/foundation-icons/foundation-icons.svg","modified":0,"renderable":1},{"_id":"source/image/DL/1/BP.gif","path":"image/DL/1/BP.gif","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/7_kano_2.jpg","path":"image/kodokan/2/7-8/7_kano_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/all_2.jpg","path":"image/kodokan/2/7-8/all_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/newbuilding.jpg","path":"image/kodokan/2/newbuilding.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_1.jpg","path":"image/sumiao4/MZ_1.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_4.jpg","path":"image/sumiao4/MZ_4.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_5.jpg","path":"image/sumiao4/MZ_5.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/4/2-2.png","path":"image/DL/4/2-2.png","modified":0,"renderable":0},{"_id":"source/image/ML/5_1.jpg","path":"image/ML/5_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_kano_3.jpg","path":"image/kodokan/2/1-6/2_kano_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/5_female_2.jpg","path":"image/kodokan/2/1-6/5_female_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/7_0.jpg","path":"image/kodokan/2/7-8/7_0.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_0.jpg","path":"image/kodokan/2/7-8/8_0.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/all.jpg","path":"image/kodokan/2/7-8/all.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/4/eyecatch.jpg","path":"image/kodokan/4/eyecatch.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/5/2-2.png","path":"image/DL/5/2-2.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/1-3.png","path":"image/RL/5/1-3.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/1_shop_3.jpg","path":"image/kodokan/2/1-6/1_shop_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_library_1.jpg","path":"image/kodokan/2/1-6/2_library_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_library_5.jpg","path":"image/kodokan/2/1-6/2_library_5.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/5_female_1.jpg","path":"image/kodokan/2/1-6/5_female_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/6_international.jpg","path":"image/kodokan/2/1-6/6_international.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/6_school_1.jpg","path":"image/kodokan/2/1-6/6_school_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/6_school_2.jpg","path":"image/kodokan/2/1-6/6_school_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/7_00.jpg","path":"image/kodokan/2/7-8/7_00.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_1.jpg","path":"image/kodokan/2/7-8/8_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_4.jpg","path":"image/kodokan/2/7-8/8_4.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_3.jpg","path":"image/kodokan/2/7-8/8_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_5.jpg","path":"image/kodokan/2/7-8/8_5.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_7.jpg","path":"image/kodokan/2/7-8/8_7.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/gym/gym.jpg","path":"image/kodokan/2/gym/gym.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/gym/gym_3.jpg","path":"image/kodokan/2/gym/gym_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/kano1.jpg","path":"image/kodokan/2/main/kano1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/kano2.jpg","path":"image/kodokan/2/main/kano2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/main_1.jpg","path":"image/kodokan/2/main/main_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/main_3.jpg","path":"image/kodokan/2/main/main_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/main_2.jpg","path":"image/kodokan/2/main/main_2.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/5/0-0.png","path":"image/RL/5/0-0.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/1-1.png","path":"image/RL/5/1-1.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/1-4.png","path":"image/RL/5/1-4.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/2-2.png","path":"image/RL/5/2-2.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/3-1.png","path":"image/RL/5/3-1.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/1_shop_2.jpg","path":"image/kodokan/2/1-6/1_shop_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_library_2.jpg","path":"image/kodokan/2/1-6/2_library_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_library_3.jpg","path":"image/kodokan/2/1-6/2_library_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/4.jpg","path":"image/kodokan/2/1-6/4.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/2_library_4.jpg","path":"image/kodokan/2/1-6/2_library_4.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/gym/gym_1.jpg","path":"image/kodokan/2/gym/gym_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/gym/gym_2.jpg","path":"image/kodokan/2/gym/gym_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/kano3.jpg","path":"image/kodokan/2/main/kano3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/main/main_4.jpg","path":"image/kodokan/2/main/main_4.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/3/info.jpg","path":"image/kodokan/3/info.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/4/book.jpg","path":"image/kodokan/4/book.jpg","modified":0,"renderable":0},{"_id":"source/image/sumiao4/MZ_3.jpg","path":"image/sumiao4/MZ_3.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/2/Mask-RCNN.png","path":"image/DL/2/Mask-RCNN.png","modified":0,"renderable":0},{"_id":"source/image/DL/5/1-4.png","path":"image/DL/5/1-4.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/1-2.png","path":"image/RL/5/1-2.png","modified":0,"renderable":0},{"_id":"source/image/RL/5/2-1.png","path":"image/RL/5/2-1.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/6_school_3.jpg","path":"image/kodokan/2/1-6/6_school_3.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_6.jpg","path":"image/kodokan/2/7-8/8_6.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/nanai.jpg","path":"image/kodokan/2/nanai.jpg","modified":0,"renderable":0},{"_id":"source/image/PMP/PMP.png","path":"image/PMP/PMP.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/1_shop_1.jpg","path":"image/kodokan/2/1-6/1_shop_1.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/8_2.jpg","path":"image/kodokan/2/7-8/8_2.jpg","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/1_2.jpg","path":"image/kodokan/2/1-6/1_2.jpg","modified":0,"renderable":0},{"_id":"source/image/DL/2/4.gif","path":"image/DL/2/4.gif","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/1-6/1_1.jpg","path":"image/kodokan/2/1-6/1_1.jpg","modified":0,"renderable":0},{"_id":"source/image/RL/5/3-2.png","path":"image/RL/5/3-2.png","modified":0,"renderable":0},{"_id":"themes/huno/source/images/background-cover.jpg","path":"images/background-cover.jpg","modified":0,"renderable":1},{"_id":"source/image/DL/5/1-3.png","path":"image/DL/5/1-3.png","modified":0,"renderable":0},{"_id":"source/image/kodokan/2/7-8/Summer.png","path":"image/kodokan/2/7-8/Summer.png","modified":0,"renderable":0}],"Cache":[{"_id":"themes/huno/.DS_Store","hash":"97ce4f1655bba19455217d7063147750b20187f3","modified":1605023399047},{"_id":"themes/huno/README.en.md","hash":"0878f26ed7fd9d4cbfeb0ddab5e1e14c93be793a","modified":1480240788000},{"_id":"themes/huno/README.md","hash":"8940f32e649ba30b684f347c04826a066fa9f399","modified":1480240788000},{"_id":"themes/huno/_config.yml","hash":"d65a4f65d4d00b68f77df7bb907558a8958f60e6","modified":1514386043000},{"_id":"source/.DS_Store","hash":"b5655878190b79baef872fb60590b58fed82a74d","modified":1608025077084},{"_id":"themes/huno/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1480240788000},{"_id":"themes/huno/.git/FETCH_HEAD","hash":"f714295aa199d789efb7918e8fa942dc621d1cab","modified":1480240983000},{"_id":"themes/huno/.git/ORIG_HEAD","hash":"3c4512585bce805b5d221ad0b3f02fb85feb8b65","modified":1480240983000},{"_id":"themes/huno/.git/config","hash":"6427fc07164643811f4ecb285662a352db74d4b2","modified":1480240788000},{"_id":"themes/huno/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1480240785000},{"_id":"themes/huno/.git/index","hash":"2fab8c03f3a7c76798b57e18d42c0e3f237950df","modified":1605020064071},{"_id":"themes/huno/.git/packed-refs","hash":"8647383fb9ce21a528fa4a1338a8f150043e6e13","modified":1480240788000},{"_id":"themes/huno/layout/.DS_Store","hash":"9096387ed8f7c917561eff61ed935a7723019e1d","modified":1605019604449},{"_id":"themes/huno/layout/archive.ejs","hash":"23aff325ab97b2c7dde6e757fa3add5226a88b4f","modified":1480240788000},{"_id":"themes/huno/layout/category.ejs","hash":"2b412d1b52b9da8ad19f6a66b1c18ab0b7546214","modified":1480240788000},{"_id":"themes/huno/layout/index.ejs","hash":"12c817a875f353c03d8795c2a7b653e4da22c17e","modified":1480240788000},{"_id":"themes/huno/layout/layout.ejs","hash":"4977031185ae4687c0ba0ec1f7aec5050b203726","modified":1480240788000},{"_id":"themes/huno/layout/page-archive.ejs","hash":"ce5523d829c0fdd6a79984fcf15e33572a220df9","modified":1480240788000},{"_id":"themes/huno/layout/page.ejs","hash":"70a50ecf7e1aa9c658212f1e77783dcb287d56a6","modified":1481441239000},{"_id":"themes/huno/layout/post.ejs","hash":"86c9d13e8abdcff4d368e34595e2d2f11f37e9d4","modified":1480240788000},{"_id":"themes/huno/layout/tag.ejs","hash":"fefa03bed577cae2e1aac19bd5d30c5034f453a0","modified":1480240788000},{"_id":"themes/huno/languages/.DS_Store","hash":"09a48ee43e81970490545f573682abe5dabd585f","modified":1605022367739},{"_id":"themes/huno/languages/default.yml","hash":"bddd7fd79412849a52b9a36aae36af111b2db619","modified":1480240788000},{"_id":"themes/huno/languages/en-US.yml","hash":"8208ede7e1398f933e7798f0d566666778c30128","modified":1605023268977},{"_id":"themes/huno/languages/zh-CN.yml","hash":"fc0fcc3819c03baeecf2aa51cec08257973288cf","modified":1481384022000},{"_id":"themes/huno/languages/zh-TW.yml","hash":"d865666c14c58c70738d9aef04ae14e38e750c14","modified":1480240788000},{"_id":"themes/huno/source/.DS_Store","hash":"d66ed721f9f340b7bfee8e1c754ebe9d97b51a6e","modified":1605017532857},{"_id":"themes/huno/source/favicon-black-50.png","hash":"8db9853d8ff7464288d3f9a051c896d71fa36612","modified":1481457780000},{"_id":"themes/huno/source/favicon-black-55.png","hash":"c2615db27a8cf7ddca6e9fe042061d464889e1d1","modified":1481458499000},{"_id":"themes/huno/source/favicon-red-50.png","hash":"fb4a64e7ce56fefcab17b579047fb9ea336ac456","modified":1481457770000},{"_id":"source/About/index.md","hash":"f0c0b72bcb12d07922158ca16826c1377f4c0e89","modified":1585619346478},{"_id":"source/_posts/.DS_Store","hash":"9921bd16c8faa1b0ff0cde48cfee3eceecab8e59","modified":1608022832656},{"_id":"source/Archive/index.md","hash":"946966a9b360151cd32c94ff91964d4420563498","modified":1481462796000},{"_id":"source/image/.DS_Store","hash":"ba2ff66a5252c5a495444cd35277ccfb492117eb","modified":1608025077083},{"_id":"themes/huno/cs-icon.png","hash":"4ba5bfbacb1a533f3561112406c305e9510aa8dd","modified":1480240788000},{"_id":"themes/huno/layout/_partials/list-posts.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1480240788000},{"_id":"themes/huno/layout/_scripts/site-analytics.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1480240788000},{"_id":"themes/huno/source/favicon.ico","hash":"c39d2a9a646fb119ee08f22f14e06821c051fd6a","modified":1481454294000},{"_id":"themes/huno/source/favicon.png","hash":"201be090f2ef3c79d66c7dfb644bb3fbe6f98c9d","modified":1481453250000},{"_id":"source/_posts/zh/.Rhistory","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1502192220000},{"_id":"source/image/shizhai.jpg","hash":"24705def6d328fb2cf039738e93005377fd8f803","modified":1480751145000},{"_id":"themes/huno/.git/hooks/applypatch-msg.sample","hash":"86b9655a9ebbde13ac8dd5795eb4d5b539edab0f","modified":1480240785000},{"_id":"themes/huno/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1480240785000},{"_id":"themes/huno/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1480240785000},{"_id":"themes/huno/.git/hooks/pre-applypatch.sample","hash":"42fa41564917b44183a50c4d94bb03e1768ddad8","modified":1480240785000},{"_id":"themes/huno/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1480240785000},{"_id":"themes/huno/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1480240785000},{"_id":"themes/huno/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1480240785000},{"_id":"themes/huno/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1480240785000},{"_id":"themes/huno/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1480240785000},{"_id":"themes/huno/.git/logs/HEAD","hash":"1fe88722765e1f2e8f221d797070e4fd8eed1c85","modified":1480240788000},{"_id":"themes/huno/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1480240785000},{"_id":"themes/huno/layout/_partials/.DS_Store","hash":"9096ab201238043ef9b0e6f31b6bc2809ae0ceb9","modified":1481439351000},{"_id":"themes/huno/layout/_partials/archive.ejs","hash":"2c413cde5b1d3a475777e8ff8226762dfe92610c","modified":1480240788000},{"_id":"themes/huno/layout/_partials/article.ejs","hash":"12993f72e718ac81763c2095f64b7ef3523f1777","modified":1480240788000},{"_id":"themes/huno/layout/_partials/disqus.ejs","hash":"30c043cc683c78f345a658cf64b37e55e6521685","modified":1480240788000},{"_id":"themes/huno/layout/_partials/duoshuo.ejs","hash":"7cfe423c088a3bf0565e27a1c17d1b99a260786b","modified":1480240788000},{"_id":"themes/huno/layout/_partials/footer.ejs","hash":"59070ce06159cfd45b13c9499b1e4f0a272d1019","modified":1480240788000},{"_id":"themes/huno/layout/_partials/pagination.ejs","hash":"1240c019aa4df269777c54c0d2124b534ec1f380","modified":1480240788000},{"_id":"themes/huno/layout/_partials/side-panel.ejs","hash":"8d38cb60200b157d24866417686b0479def918de","modified":1480240788000},{"_id":"themes/huno/layout/_partials/social.ejs","hash":"fabe4c72efa1bc660278b512c21bd07de7f42af0","modified":1481439500000},{"_id":"themes/huno/layout/_scripts/.DS_Store","hash":"b272bec50b089fea28c49d414278063f624a8e83","modified":1481372901000},{"_id":"themes/huno/layout/_scripts/awesome-toc.ejs","hash":"b39df57929c246726ca45723ca9ad1a4104738c4","modified":1480240788000},{"_id":"themes/huno/layout/_scripts/github-repo-widget.ejs","hash":"ee64a8e431318cb8fb25f59cfeec0def95b58912","modified":1480240788000},{"_id":"themes/huno/layout/_scripts/killie6.ejs","hash":"2d5db8a53962ac704b8bfc40688319bed2e84ac9","modified":1480240788000},{"_id":"themes/huno/layout/_scripts/mathjax.ejs","hash":"2921f9f3b3c3e40f59c2d023764694a79ac07926","modified":1480240788000},{"_id":"themes/huno/source/css/.DS_Store","hash":"240ae30e369f0ddde9d67939a85b30aa59b5e44e","modified":1481441158000},{"_id":"themes/huno/source/css/archive.css","hash":"7238ea5f6bda859e3bae7aaa546caf27fe1fb0cc","modified":1480240788000},{"_id":"themes/huno/source/css/china-social-icon.css","hash":"2f90442f6d0d289e49c07a85c2dae32cab8b2063","modified":1480240788000},{"_id":"themes/huno/source/css/animate.css","hash":"651fcf046b1bd2bff3ab4cb8947ff7fe058de373","modified":1480240788000},{"_id":"themes/huno/source/css/highlight.styl","hash":"92eabbf94e7a06e968c356bedf4adb04700f1c2e","modified":1480240788000},{"_id":"themes/huno/source/css/uno.css","hash":"dae2b90f40f9f46b8c3c273898de165e5e594f9e","modified":1480240788000},{"_id":"themes/huno/source/fonts/.DS_Store","hash":"11f0c149894cd793afb77db4181684121440770a","modified":1481441163000},{"_id":"themes/huno/source/images/.DS_Store","hash":"db653330cedf18798ecc62d30c6c5db1589ebca5","modified":1481375454000},{"_id":"themes/huno/source/images/totop.png","hash":"4f6cb11941e5a72b03cb00cf9d9d55671b4310eb","modified":1480240788000},{"_id":"themes/huno/source/js/.DS_Store","hash":"11eef27d60c135fb20ba195f5af7df51053cb7d2","modified":1481431407000},{"_id":"themes/huno/source/js/awesome-toc.min.js","hash":"b4d0f2a33f8340eb2543e8b2cee0dfd745cfb54a","modified":1480240788000},{"_id":"themes/huno/source/js/jquery.githubRepoWidget.min.js","hash":"94a141fa474ec5022f7c397b4fd3ff92405ab755","modified":1480240788000},{"_id":"themes/huno/source/js/main.js","hash":"4a477bf23afa10929c4be55ab90c59ce0d9b9842","modified":1480240788000},{"_id":"themes/huno/source/js/scale.fix.js","hash":"ce593f56728cc1cedf2e513cb20b926de3b05e07","modified":1480240788000},{"_id":"source/_posts/en/Carmen_Guitar.md","hash":"e4bbad1f465dedc1f8df2aa0423e023ba02a9efb","modified":1505044439000},{"_id":"source/_posts/en/Active And Passive Vocabulary.md","hash":"b0bfaf17b8beb05d4fe6360f459b43e63fbac504","modified":1481387193000},{"_id":"source/_posts/en/Hello-World.md","hash":"5e8070d6b64127f5f7a293577fb01d6bb6857713","modified":1481386864000},{"_id":"source/_posts/en/My PMP Study.md","hash":"5bb65591e74d8ce28a5901db5a7708abb68b5ebc","modified":1585619831616},{"_id":"source/_posts/ja/EOSのDAPP開発流れ.md","hash":"c27f7fe21129be58402ccb9fb35e4f4cdebc4ced","modified":1534597801000},{"_id":"source/_posts/ja/ETHのDAPP開発流れ.md","hash":"d79df6003866d1409c2749cbdc49347bf7e72372","modified":1534597818000},{"_id":"source/_posts/ja/My Japanese Episode.md","hash":"1151e28daea1bf588b4ffe0f39e61dafc6cf548e","modified":1552390258000},{"_id":"source/_posts/ja/剣道用語-Kendo terminology.md","hash":"129b4cdd0ef2bd85d8f44015a80c6e25ddc95c42","modified":1608025252839},{"_id":"source/_posts/ja/富士登山.md","hash":"44f4d709deb6224855cdc3a398798cee9a96be8b","modified":1504523978000},{"_id":"source/_posts/ja/柔道用語-Judo terminology.md","hash":"000731b8037d9fc6f9803ebdf32c75ec40ca8c77","modified":1605030114823},{"_id":"source/_posts/ja/紙幣の肖像人物.md","hash":"0300bc934c75b960d5faea88e64334ed938e4a21","modified":1512828316000},{"_id":"source/_posts/zh/.DS_Store","hash":"06689f30ba8e98e505495e31ab4d63a3da21a90b","modified":1605031108699},{"_id":"source/_posts/zh/Deep learning笔记1-神经网络入门.md","hash":"8af156fcfb92b517f1f9ab4500ce389bc60e4cab","modified":1514812343000},{"_id":"source/_posts/zh/Deep learning笔记2-CNN卷积神经网络.md","hash":"6cb8c1500f534fb7ece3e4aa44e355cc7c4bf019","modified":1540212441000},{"_id":"source/_posts/zh/Deep learning笔记3-RNN循环神经网络.md","hash":"c576e8ec28df9bd777aa0a345526aa53fa1581c2","modified":1514812346000},{"_id":"source/_posts/zh/Deep learning笔记4-TreeRNN递归神经网络.md","hash":"8d3d2b0fd7652bf87f1d7228d4443a82e5a398a8","modified":1514812348000},{"_id":"source/_posts/zh/Deep learning笔记5-GAN生成式对抗网络.md","hash":"20ccff2da105e466338c4065ef5793956a896f70","modified":1514812463000},{"_id":"source/_posts/zh/JUDO-Kodokan 柔道-講道館.md","hash":"ecee36d399d8a81954e545e382493644bd07bf73","modified":1514812391000},{"_id":"source/_posts/zh/JUDO-ONO SHOHEI 柔道-大野将平.md","hash":"65a2a474322aebe1fc2548eb727bc441cb2a9151","modified":1514812388000},{"_id":"source/_posts/zh/Mac OS X设置默认Python版本.md","hash":"70ed6c52b8335e2fc56aa23717200ab55b0aeed1","modified":1483514592000},{"_id":"source/_posts/zh/Machine learning笔记1-Python.md","hash":"6a04e33b633f42c5ee97847ab67156f33894af08","modified":1514812444000},{"_id":"source/_posts/zh/Machine learning笔记2-Statistic.md","hash":"9d7340b7e7e964c7ba88df1bc85d016738907e8e","modified":1514812454000},{"_id":"source/_posts/zh/Machine learning笔记3-Linear algebra.md","hash":"8694a7c659f04890d6a2c2f60ba1cc969a80ea21","modified":1514812458000},{"_id":"source/_posts/zh/Machine learning笔记4-Data Analysis.md","hash":"9e6ed7a68221bbc83a80bf474768053de2a1a559","modified":1514812466000},{"_id":"source/_posts/zh/Machine learning笔记5-Model.md","hash":"bbd76cdce5140f7dfb9753e56b4675d342a47109","modified":1514812472000},{"_id":"source/_posts/zh/Reinforcement Learning笔记1-MDP.md","hash":"7d0632fb034b57555a0cad795fb079a7243c2247","modified":1514812528000},{"_id":"source/_posts/zh/Reinforcement Learning笔记2-Bellman.md","hash":"8551fbf671a7b45d1105e449848b5c0f57aff60c","modified":1514812528000},{"_id":"source/_posts/zh/Reinforcement Learning笔记3-Dynamic Program.md","hash":"7730c5857b59c95b6970aa15a987c470ee282d5a","modified":1514812528000},{"_id":"source/_posts/zh/Reinforcement Learning笔记4-Monte Carlo.md","hash":"ece0f7d895ed157dd2b76518353f5f3edc88726f","modified":1514812528000},{"_id":"source/_posts/zh/Reinforcement Learning笔记5-Temporal Diff.md","hash":"8284625e8f476a2d55bd009d3bd55e819795df16","modified":1514812528000},{"_id":"source/_posts/zh/“御茶ノ水” 乐器街之行.md","hash":"0909213d0fc1e30a4bfac89b0db5884baf4085a2","modified":1512828324000},{"_id":"source/_posts/zh/人物素描1.0.md","hash":"c02382b041ea9a31d3a27339192becc8d2bdf0cf","modified":1553519759000},{"_id":"source/_posts/zh/人物素描2.0.md","hash":"9c4d9e86560b97725775feae4bedb950d65fb0e3","modified":1553519763000},{"_id":"source/_posts/zh/人物素描3.0.md","hash":"a4f2133f4a581c3276890f5b8cd360d173ea6d31","modified":1553519768000},{"_id":"source/_posts/zh/人物素描4.0.md","hash":"def6c6b3b946a527a31ffe3a9109bfb5d60a3b4a","modified":1553519771000},{"_id":"source/_posts/zh/游泳动图.md","hash":"489a98e338392aba54b20c601d60f91217e1c1b1","modified":1504012483000},{"_id":"source/_posts/zh/石寨古村.md","hash":"f5bc44ec1fe5368330740f1aae8a465dc2ab6565","modified":1541428510000},{"_id":"source/_posts/ar/اللغة الإنجليزية في المطبخ الصيني.md","hash":"8dd371cc0eab47ceed5a59584c241a6262766f38","modified":1481387337000},{"_id":"source/_posts/fr/Un Plan — Le Voyage en France.md","hash":"88a2bc3c4b3227238d6e887e5110196c3bb26ddb","modified":1481387057000},{"_id":"source/image/Judo/Judo.jpg","hash":"3f1779d54ccba9aa3a4512582ff5458684850ac2","modified":1503413237000},{"_id":"source/image/Yamaki/Yamaki-2.jpg","hash":"41657e4c8a6eaa3d16bf16979621915666d9324c","modified":1504007844000},{"_id":"source/image/kodokan/kodokan.png","hash":"093921845c26cc3e599a622de1ecfc1ff01a08aa","modified":1505699484000},{"_id":"source/image/sumiao/sumiao_3.jpg","hash":"79840fcddd550d2f41e717dff0448373d87cdffa","modified":1502422251000},{"_id":"source/image/sumiao2/W1.jpg","hash":"41b2d39ffb81d175245cdeec9aea9435cb0bd948","modified":1502889427000},{"_id":"themes/huno/source/js/jquery.min.js","hash":"8b6babff47b8a9793f37036fd1b1a3ad41d38423","modified":1480240788000},{"_id":"source/image/Fuji/Fuji_2.jpg","hash":"b9e4b3843c9cb1fdc3d94bc61948489f6115a9b7","modified":1506066304000},{"_id":"source/image/Judo/Budokan.jpg","hash":"f2dce6c14d8444232d862e4450fe50b3047b35ae","modified":1504522603000},{"_id":"source/image/Judo/ONO SHOHEI.jpg","hash":"6eb35ddab530b7e0caa343d318d755dda3010399","modified":1504522624000},{"_id":"source/image/Kendo/Kendo-1.jpg","hash":"2f659bc7e053003517bf077057b2cd36890a9863","modified":1608025086444},{"_id":"source/image/Yamaki/Yamaki-1.jpg","hash":"260a52d654b3714059d4e3adb2f1dfcf1a338586","modified":1504007821000},{"_id":"source/image/sumiao/sumiao_2.jpg","hash":"acd80a32757f02f19286eae565196d0d0113b9ca","modified":1502422238000},{"_id":"source/image/sumiao/sumiao_4.jpg","hash":"2ce4ce1bd64b4e008884b5e1a7251961dc061814","modified":1502422266000},{"_id":"source/image/sumiao/sumiao_5.jpg","hash":"a93d05e720cbddf3d34fa1dd6e3dac6a130f7bb3","modified":1502422272000},{"_id":"source/image/sumiao/sumiao_7.jpg","hash":"144de9f90c163e6e9cb31d9d3ee409885d6c0ba0","modified":1502425523000},{"_id":"source/image/sumiao/sumiao_6.jpg","hash":"e335ea201156390d30a5d51e683c2bb5e8f0e526","modified":1502422280000},{"_id":"source/image/sumiao2/W.jpg","hash":"8045939eb02258a4f8fc2f4eb1c5f0f2460d69ab","modified":1502889309000},{"_id":"source/image/sumiao2/W2.jpg","hash":"d626046abb07a20a6ffe478177a2b72a3b98a0fb","modified":1502890097000},{"_id":"source/image/sumiao2/W3.jpg","hash":"1337eb53841f70e1bc9174087736e001ed83e7d2","modified":1502890035000},{"_id":"source/image/sumiao2/W4.jpg","hash":"e0eccdfa9751ea6a198487e762b3414f17617cc6","modified":1502889716000},{"_id":"source/image/sumiao3/ZYY.jpg","hash":"03052e31a9a35c5290af65278de2c60dfbfb41f3","modified":1512748713000},{"_id":"source/image/sumiao3/ZYY_1.jpg","hash":"13ade590e37f71df32d1cdf6b32df63f99ce60a8","modified":1512747987000},{"_id":"source/image/sumiao3/ZYY_2.jpg","hash":"748c7a264c424cb991d9e37aaaac1537e5df5291","modified":1512747987000},{"_id":"source/image/sumiao3/ZYY_back.jpg","hash":"b01dcaeec84f6ad73e830bd1b0601b1f822a63ce","modified":1512747988000},{"_id":"source/image/sumiao3/ZYY_back_0.jpg","hash":"42f17e98e8ed74969da244b94f58f85cf4be48e1","modified":1512827934000},{"_id":"source/image/sumiao4/MZ.jpg","hash":"2db78e05fe23b05a09b098616f47a118ae0d89be","modified":1512907862000},{"_id":"source/image/swim/backstroke_1.gif","hash":"4726a16f171482580be8af2c791a43e6d0d55cae","modified":1504010849000},{"_id":"source/image/swim/backstroke_2.gif","hash":"98ffcc539b6d45523646120e16267d88c0e94e5d","modified":1504011886000},{"_id":"source/image/swim/backstroke_3.gif","hash":"8ae65c6f31e4a71252e7e14e9652617b87f90a1c","modified":1504010870000},{"_id":"source/image/swim/breaststroke_1.gif","hash":"bdad2dbb146d51ba5eb8d6d7a1fbec2108ae04b7","modified":1504011914000},{"_id":"source/image/swim/breaststroke_2.gif","hash":"ab61e2ee3213eee8b02315de5ffb6d2b3ea0d09f","modified":1504010529000},{"_id":"source/image/swim/butterfly_1.gif","hash":"7b43b04db70d8d87e6ec90d4eb47708aa2e8d310","modified":1504010707000},{"_id":"source/image/swim/butterfly_2.gif","hash":"079986281fb045e17ddb20e2513c07537881650f","modified":1504010736000},{"_id":"source/image/swim/butterfly_3.gif","hash":"ef1aa23191500ccece3033a01e8e3022851164a9","modified":1504010742000},{"_id":"source/image/swim/freestyle_1.gif","hash":"fc19506e8267291c582cf4057d6b213accb4485e","modified":1504010569000},{"_id":"source/image/swim/freestyle_2.gif","hash":"0f8c8fdac8b507b0cd1e610406ce854e8296d356","modified":1504010589000},{"_id":"source/image/swim/freestyle_3.gif","hash":"ee1277e8382d01ee79b7e3b4d871a3625fdfe204","modified":1504010596000},{"_id":"themes/huno/.git/objects/pack/pack-50c872e1804ca429688685ab4ff7efcb5e460bca.idx","hash":"5e45c6a708e1b3adcc2d533b2e908d59d39fb307","modified":1480240788000},{"_id":"themes/huno/.git/refs/heads/master","hash":"3c4512585bce805b5d221ad0b3f02fb85feb8b65","modified":1480240788000},{"_id":"themes/huno/source/fonts/china-social/.DS_Store","hash":"5cdecb201c8a5ca188589f72f77c79f10930c16b","modified":1481431396000},{"_id":"themes/huno/source/fonts/china-social/china-social.eot","hash":"a43a5c3d66f0d38639a595ebd02857e152ada475","modified":1480240788000},{"_id":"themes/huno/source/fonts/china-social/china-social.ttf","hash":"2f94360528097df7dcfb39baf8df5393a0d47ca3","modified":1480240788000},{"_id":"themes/huno/source/fonts/china-social/readme.html","hash":"ca335751de11d6ef7721dc907571de39cef6e361","modified":1480240788000},{"_id":"themes/huno/source/fonts/china-social/china-social.woff","hash":"74c0ac5268cf7ffe270faaf7c960b74d483d2df1","modified":1480240788000},{"_id":"themes/huno/source/fonts/foundation-icons/.DS_Store","hash":"42c0738534edbf3332f48b73bfdf5b220cad3d1f","modified":1481431396000},{"_id":"themes/huno/source/fonts/foundation-icons/.fontcustom-data","hash":"3b0cfb3ba2ee7ccae391bc66b3acaa6895932e5f","modified":1480240788000},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.css","hash":"426036f1d554b9dffae8b38acc36cade40d9521a","modified":1480240788000},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.eot","hash":"d584172686583fd510d8f04cf21e6e77fce51435","modified":1480240788000},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.woff","hash":"112fb0e498037f2fea036adb8105e47638159eaa","modified":1480240788000},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.ttf","hash":"4b2bce6c792493a4a5716b6fec2dbefe89492c3f","modified":1480240788000},{"_id":"source/_posts/zh/.ipynb_checkpoints/Deep learning笔记3-RNN循环神经网络-checkpoint.ipynb","hash":"517c9be220d34fa0f590822b99ee9919079f8615","modified":1508341652000},{"_id":"source/_posts/zh/.ipynb_checkpoints/Deep learning笔记4-TreeRNN递归神经网络-checkpoint.ipynb","hash":"e169629ee6e54cada94fbb6ee01f1c58dd7dbd86","modified":1508341738000},{"_id":"source/_posts/zh/DL/Deep learning笔记1-神经网络入门.ipynb","hash":"3e25e1164337699a118b90b8544b5014bcac4f94","modified":1509024347000},{"_id":"source/_posts/zh/DL/Deep learning笔记2-CNN卷积神经网络.ipynb","hash":"033aed58c4e10cdae7fd074d56f1acea51460489","modified":1508413208000},{"_id":"source/_posts/zh/DL/Deep learning笔记3-RNN循环神经网络.ipynb","hash":"34eb860e4c5d524d71765e4cc0c9590a090ca9e4","modified":1509326043000},{"_id":"source/_posts/zh/DL/Deep learning笔记4-TreeRNN递归神经网络.ipynb","hash":"880ac0a66cb444e94295b2f501494d54957dbc7a","modified":1508760195000},{"_id":"source/_posts/zh/DL/Deep learning笔记5-GAN生成式对抗网络.ipynb","hash":"c2a8444b549eb7a3719a4a3e53c7b9ad859223be","modified":1510103319000},{"_id":"source/_posts/zh/RL/Reinforcement Learning笔记1-MDP.ipynb","hash":"ba7ef81226dd64f57ea2c4c2e5eb73e286e20762","modified":1508938321000},{"_id":"source/_posts/zh/RL/Reinforcement Learning笔记2-Bellman.ipynb","hash":"2a304ec34f6aaf21d2cf9c3faf733b91caaf7c25","modified":1508842082000},{"_id":"source/_posts/zh/RL/Reinforcement Learning笔记3-Dynamic Program.ipynb","hash":"497265f2e6d4c1f5d0b5cb2cbc9f867b488ac7cd","modified":1508938410000},{"_id":"source/_posts/zh/RL/Reinforcement Learning笔记4-Monte Carlo.ipynb","hash":"ad699efcfe1089694d5335a160b96a92a51fe2e7","modified":1509024719000},{"_id":"source/_posts/zh/RL/Reinforcement Learning笔记5-Temporal Diff.ipynb","hash":"761461137e0ad73f538feabec165bd16f6c243ef","modified":1509086893000},{"_id":"source/image/Carmen/Carmen_1.jpg","hash":"e1b248acf8fc918cccf6baae06e2350fb666ba6f","modified":1505041721000},{"_id":"source/image/Carmen/Carmen_2.jpg","hash":"b6e5f16dc208bb3c4a16ba5a11c53dac2ae94914","modified":1505041750000},{"_id":"source/image/DL/1/simple-neuron.png","hash":"9bc311cbe617d0f6e170700dcf4ca4ad6fffa005","modified":1502177457000},{"_id":"source/image/Carmen/Carmen_3.jpg","hash":"d1f64986ca226dee5d4c8ea935475dee8b743156","modified":1505041775000},{"_id":"source/image/DL/2/1.png","hash":"9b5084ccb69a7e45501e958021fe68b8934e5dcf","modified":1505637528000},{"_id":"source/image/DL/2/2.png","hash":"78a0750ec6c90e1b05f0921a380f526c106fc787","modified":1505642223000},{"_id":"source/image/DL/2/autoencoder_1.png","hash":"38926dfd0c4a4186479d974ba9ce27ce08173c35","modified":1503357566000},{"_id":"source/image/DL/4/1-3.png","hash":"ead2ff0b5f04cb8b951fa99c055abbd0d05d30f5","modified":1508337914000},{"_id":"source/image/DL/4/1-2.png","hash":"a2d44bce2370290e018649795a6733f33e5cc0eb","modified":1508337914000},{"_id":"source/image/DL/4/1-4.png","hash":"b451f2196b9131043c1a79c3b28bf867c58f8b18","modified":1508337914000},{"_id":"source/image/DL/4/2-1.png","hash":"e8b6b577a805d4814217782eda7fe800805725b0","modified":1508337914000},{"_id":"source/image/DL/5/1-1.jpg","hash":"6f66a6574454d73be6ae3d180733ee5558d8d3d4","modified":1508760252000},{"_id":"source/image/DL/5/1-2.jpg","hash":"8853e9144a9703d567e8962701c7cbeb321147f5","modified":1508760283000},{"_id":"source/image/DL/5/2-1.png","hash":"ed0def464880d0d8f8126edbf97f46177c7bad3c","modified":1508746632000},{"_id":"source/image/Fuji/Fuji_1.jpg","hash":"f1bde66eff6aa3bd100f852ed824700ec8f8ffc4","modified":1506066280000},{"_id":"source/image/DL/3/1-1.jpg","hash":"d773a5e06c3ca759faf52d1dc18ff54d5a4c42ee","modified":1508330403000},{"_id":"source/image/DL/3/1-2.jpg","hash":"6e87c264d76e431a3f4aa83402b651fa12c9eba9","modified":1508330425000},{"_id":"source/image/DL/3/1-3.jpg","hash":"9d577ef529ae80d3171206391b2e5c49fd61c58a","modified":1508332029000},{"_id":"source/image/DL/3/2-1.jpg","hash":"c519937588cf8b4955da9f5d734684c467157fc2","modified":1508333815000},{"_id":"source/image/DL/3/2-2.jpg","hash":"4794126963d6e35953786ae4be2aa674b7b32554","modified":1508333947000},{"_id":"source/image/DL/3/3-1.jpg","hash":"3e0b8339e9f286312cf59723ac00b865bf09c06e","modified":1508337914000},{"_id":"source/image/DL/3/3-2.jpg","hash":"177284be67b45f1a67e805e9bee799dbcf12034f","modified":1508337914000},{"_id":"source/image/DL/3/4-2.png","hash":"de557b778e92300f5e943078e1f1256f9d10ea57","modified":1508337914000},{"_id":"source/image/DL/3/4-3.jpg","hash":"22ceea95d5db169e97656a9b8053192ede101a20","modified":1509292386000},{"_id":"source/image/ML/3_1.jpg","hash":"ed22f29596dad580730d6560ffa116107043479a","modified":1505032716000},{"_id":"source/image/ML/3_6.jpg","hash":"7e8a247633436f6c8cd355eadc1ffa870fce6c36","modified":1505031798000},{"_id":"source/image/RL/2/1.jpg","hash":"dfa3f65d56f28ec18206df404c0da32272875e13","modified":1508824022000},{"_id":"source/image/RL/2/3.jpg","hash":"5e19502712d0d320990f72890b9a061d0e2cc34f","modified":1508824742000},{"_id":"source/image/RL/2/2.jpg","hash":"a3a335dd4164b8bd3963930b162c0a8d28e42c93","modified":1508824088000},{"_id":"source/image/RL/3/3.jpg","hash":"5eb8dd943d509e59ea061739b58635f8078637e6","modified":1508828412000},{"_id":"source/image/RL/3/4.png","hash":"5f4e811abbca628809b60d173be0ca4ecb6edb85","modified":1508828794000},{"_id":"source/image/RL/4/0-1.png","hash":"0863e29a555ba3e72ff62fa1c87a0fb008325c77","modified":1508978700000},{"_id":"source/image/RL/4/2-1.svg","hash":"3c0d2ada216d01fa2ac32cfd0deb103de4690636","modified":1508983768000},{"_id":"source/image/RL/4/2-2.png","hash":"f868c4f0f62d686d8c8968f8ae33eed68b924ecb","modified":1508983778000},{"_id":"source/image/RL/4/2-3.svg","hash":"43f78cabff19809b1e09634e833593f4b5334e1f","modified":1508984590000},{"_id":"source/image/RL/5/4-1.png","hash":"cb68c87ad36d3b3687ec8bef3cb8159421c3d3da","modified":1508989496000},{"_id":"source/image/sumiao3/ZYY_3.jpg","hash":"7c031519239ac556782942d42e91d099ad09becf","modified":1512747987000},{"_id":"source/image/swim/breaststroke_3.gif","hash":"58044f449783c591cf94d0446817c948768b17b1","modified":1504011933000},{"_id":"themes/huno/source/fonts/china-social/china-social.svg","hash":"4bad780e6a31f4fa9fef037a3d9ecb8623042a6e","modified":1480240788000},{"_id":"source/image/Blockchain/ETH/ETH.jpg","hash":"8af4232a2c19e973d6bfd58e01f171c62898050a","modified":1534597056000},{"_id":"source/image/Blockchain/EOS/EOS_DAPP.jpg","hash":"6b9c7c0d826e742b482a01a0361ebf0005696f4f","modified":1534597085000},{"_id":"source/image/Blockchain/ETH/ETH_DAPP.jpg","hash":"c57f998c55bc4595c06f4370ec494bad6f62c66f","modified":1534599189000},{"_id":"source/image/DL/2/3.gif","hash":"05c0c84c210f5d25a8250ff1455d897f74aa372d","modified":1505642256000},{"_id":"source/image/DL/2/5.png","hash":"9ae433c9e12800e7bfa44f85c5f2a946b5cc3f09","modified":1505643790000},{"_id":"source/image/DL/2/CNN.jpg","hash":"ccd44500fe439c7255da395f1bd2ca6d2fc91b5f","modified":1506222375000},{"_id":"source/image/DL/2/cnnarchitecture.jpg","hash":"23822fc936fc007e5ece3383a7d79c8dcfffdd22","modified":1503357566000},{"_id":"source/image/DL/4/1-1.png","hash":"4b49e025ba481676345901bdf017ffdeca77ce24","modified":1508337914000},{"_id":"source/image/DL/3/2-3.jpg","hash":"a52bb3a0c1d5e462c45686b2b657623d41199104","modified":1508333898000},{"_id":"source/image/DL/3/4-1.png","hash":"b13785b9c0595823509bb9137f5df647a94e507f","modified":1508337914000},{"_id":"source/image/ML/1_1.jpg","hash":"5112d4a5e006725a305f198383f924a53a7f9001","modified":1505031795000},{"_id":"source/image/ML/1_3.jpg","hash":"f8c55c5a7dc74c1e919293829c98767b68ffce51","modified":1505031795000},{"_id":"source/image/ML/2_1.jpg","hash":"805c5fcebcd31fef759457d78dd0ec3dfd5165b9","modified":1505031796000},{"_id":"source/image/ML/2_3.jpg","hash":"781ea0a07c7de7599c866626a678106f3eb75154","modified":1505031796000},{"_id":"source/image/ML/2_4.jpg","hash":"89c17c9ba1509baebfc8d85aa5d8c5f4acb3f837","modified":1505031796000},{"_id":"source/image/ML/2_5.jpg","hash":"185c65d084c261455e2d3741c9a33d223bf494e9","modified":1505031796000},{"_id":"source/image/ML/2_7.jpg","hash":"56863570df64416c54342c9db4145320fc00a48f","modified":1505032754000},{"_id":"source/image/ML/2_8.jpg","hash":"ddc8ae99e8baebd95aaf24ad1e7c309e9aaf2549","modified":1505031797000},{"_id":"source/image/ML/3_2.jpg","hash":"e7ccb357e737e9069b98373db4c1d8186f9544b0","modified":1505031797000},{"_id":"source/image/ML/3_3.jpg","hash":"fa3a465c288dd5e3a28217fa9a665b1219c2816a","modified":1505031798000},{"_id":"source/image/ML/3_4.jpg","hash":"fc92778cd1bf24204e5937d2630834f8bbc92fba","modified":1505031798000},{"_id":"source/image/ML/3_5.jpg","hash":"84a919974f1f792a66d21a0fc5d2ab4851e1e971","modified":1505031798000},{"_id":"source/image/ML/4_1.jpg","hash":"c09e18845df36a431465a4a5baec592649a1866d","modified":1505031798000},{"_id":"source/image/ML/5_2.jpg","hash":"96e9241e3b8bcac2815107a3866c779ce7f77ea0","modified":1505031799000},{"_id":"source/image/RL/3/1.jpg","hash":"bee410ef6c618e53327472d25463cc6fc39ebb00","modified":1508827956000},{"_id":"source/image/RL/3/2.jpg","hash":"4b1aac50a74e052045a5c86b3a46af3b44af2af3","modified":1508827964000},{"_id":"source/image/RL/5/4-2.png","hash":"4f4238dc42d7f3dbc20411976ded9d97f469108a","modified":1508989512000},{"_id":"source/image/RL/5/4-3.png","hash":"3f0deb2369782f568bb66959272449cecc6e91ce","modified":1508989102000},{"_id":"source/image/RL/5/4-4.png","hash":"8c5a4b2d1ae7c4f2fbc332cb130b2b8f347238b8","modified":1508989106000},{"_id":"source/image/kodokan/1/kano_1.jpg","hash":"7f27da0efd29dd573ddda0c6423b5f56fa0cf0e6","modified":1505726930000},{"_id":"source/image/kodokan/2/two_buildings.jpg","hash":"95874bcc189db3b44bf3adf9e9a7bcf82c3227ee","modified":1505729826000},{"_id":"source/image/kodokan/3/fee.jpg","hash":"93b74ee593b2b15802665d1c8147629682b47322","modified":1505726929000},{"_id":"source/image/kodokan/3/female.jpg","hash":"127b0ebc7abd5ef99ca8b5cc373935550a486687","modified":1505751352000},{"_id":"source/image/kodokan/3/male.jpg","hash":"6d2c6c7d4e72780690f2212521926d9b5be83c2e","modified":1505751325000},{"_id":"source/image/sumiao3/ZYY_4.jpg","hash":"33ffb223cea746188303b4748ee80d1a12d4d502","modified":1512747988000},{"_id":"source/image/sumiao4/MZ_back.jpg","hash":"76834c37d8b141c8d3be19afc0426b2953f4c20a","modified":1513095306000},{"_id":"themes/huno/.git/logs/refs/heads/master","hash":"1fe88722765e1f2e8f221d797070e4fd8eed1c85","modified":1480240788000},{"_id":"themes/huno/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1480240788000},{"_id":"source/_posts/zh/DL/.ipynb_checkpoints/Deep learning笔记1-神经网络入门-checkpoint.ipynb","hash":"3e25e1164337699a118b90b8544b5014bcac4f94","modified":1509024347000},{"_id":"source/_posts/zh/DL/.ipynb_checkpoints/Deep learning笔记2-CNN卷积神经网络-checkpoint.ipynb","hash":"076325e0d090307070e4939947f319ffeca08042","modified":1508413104000},{"_id":"source/_posts/zh/DL/.ipynb_checkpoints/Deep learning笔记3-RNN循环神经网络-checkpoint.ipynb","hash":"25d0e694dd78280dafa60066c68e435a2ffe7b37","modified":1508593787000},{"_id":"source/_posts/zh/DL/.ipynb_checkpoints/Deep learning笔记4-TreeRNN递归神经网络-checkpoint.ipynb","hash":"880ac0a66cb444e94295b2f501494d54957dbc7a","modified":1508413811000},{"_id":"source/_posts/zh/DL/.ipynb_checkpoints/Deep learning笔记5-GAN生成式对抗网络-checkpoint.ipynb","hash":"c2a8444b549eb7a3719a4a3e53c7b9ad859223be","modified":1510103319000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning1-MDP-checkpoint.ipynb","hash":"bb371c363b841cf7a125bc3d9e8326829431202b","modified":1508840607000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning2-Bellman-checkpoint.ipynb","hash":"0083478554c89e11dad87d9b2a3456d49d8c6b3f","modified":1508840786000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning3-Dynamic Program-checkpoint.ipynb","hash":"22749058f5deaa036636567849fa2319b4ab95fd","modified":1508840846000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning4-Temporal Diff-checkpoint.ipynb","hash":"1ed53e2c788b9f7dffe18a197129d703b9cfc1e2","modified":1508840861000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning5-Monte Carlo-checkpoint.ipynb","hash":"e1a8ce6de7454eb8098dc90660ab754ebd370d3c","modified":1508840971000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记1-MDP-checkpoint.ipynb","hash":"ba7ef81226dd64f57ea2c4c2e5eb73e286e20762","modified":1508938321000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记2-Bellman-checkpoint.ipynb","hash":"2a304ec34f6aaf21d2cf9c3faf733b91caaf7c25","modified":1508842082000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记3-Dynamic Program-checkpoint.ipynb","hash":"497265f2e6d4c1f5d0b5cb2cbc9f867b488ac7cd","modified":1508938410000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记4-Monte Carlo-checkpoint.ipynb","hash":"ad699efcfe1089694d5335a160b96a92a51fe2e7","modified":1509024608000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记4-Temporal Diff-checkpoint.ipynb","hash":"74a8cc6e4833f2f93ed6c9e919ea53a6db98408f","modified":1508842075000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记5-Monte Carlo-checkpoint.ipynb","hash":"dbf493983a664b5ff582dfb65f00f42c8fd66d3a","modified":1508842072000},{"_id":"source/_posts/zh/RL/.ipynb_checkpoints/Reinforcement Learning笔记5-Temporal Diff-checkpoint.ipynb","hash":"761461137e0ad73f538feabec165bd16f6c243ef","modified":1509086893000},{"_id":"source/image/DL/5/1-5.png","hash":"b15a9910328ff24fa1b40e06afc4572f5133d287","modified":1508763520000},{"_id":"source/image/Kendo/Kendo-2.jpg","hash":"f8281d16d80f527e89f3a1698033e1267b50bc5e","modified":1608025101790},{"_id":"source/image/ML/1_2.jpg","hash":"44f3dd898341c9901417dc6ad208314c8a705d0d","modified":1505031795000},{"_id":"source/image/ML/2_2.jpg","hash":"cf04892f100d2c771908ac3cbc024613820d92f4","modified":1505031796000},{"_id":"source/image/ML/2_6.jpg","hash":"9c16ae716064b5a569bebaee3dcdefdec04da5c4","modified":1505031797000},{"_id":"source/image/ML/4_2.jpg","hash":"00db019c83664c04509949c0a14bfdadb5bde4dc","modified":1505031799000},{"_id":"source/image/sumiao4/MZ_2.jpg","hash":"ee991a70cbda9fdc5d262dc6c33e4c1566056c50","modified":1513095306000},{"_id":"themes/huno/source/fonts/foundation-icons/foundation-icons.svg","hash":"bdca38f453f9935203fe8cd071e97d7f8576e0be","modified":1480240788000},{"_id":"source/image/DL/1/BP.gif","hash":"feb6eaa9f328a918d130241588a42bb67ef6ae46","modified":1504270630000},{"_id":"source/image/kodokan/2/7-8/7_kano_2.jpg","hash":"014dd6338a01a7e0091cd274f360d7bcf2f387fe","modified":1505729932000},{"_id":"source/image/kodokan/2/7-8/all_2.jpg","hash":"7c27a664789a9fd14c9abf65a07f362c7d1ab1a6","modified":1505730169000},{"_id":"source/image/kodokan/2/newbuilding.jpg","hash":"2532dc69a5707232bd7f9bd30ff1527001412e2b","modified":1505911113000},{"_id":"source/image/sumiao4/MZ_1.jpg","hash":"6bc14bfc92354d0274ed247e380768887421928a","modified":1513095306000},{"_id":"source/image/sumiao4/MZ_4.jpg","hash":"7e75e468142df159026bfe8f888ad45adc9325ea","modified":1513095306000},{"_id":"source/image/sumiao4/MZ_5.jpg","hash":"e7530195ba3b7e2f51537d69a92eb2ab5e829d3c","modified":1513095306000},{"_id":"themes/huno/.git/logs/refs/remotes/origin/HEAD","hash":"1fe88722765e1f2e8f221d797070e4fd8eed1c85","modified":1480240788000},{"_id":"source/image/DL/4/2-2.png","hash":"b661a261ed4fb44377e0bb174ca7e69c36ac6e4d","modified":1508337914000},{"_id":"source/image/ML/5_1.jpg","hash":"6fdbb2c55b14687b79dddc10e94be80fe22d7822","modified":1505031799000},{"_id":"source/image/kodokan/2/1-6/2_kano_3.jpg","hash":"7a8f2d97c737b624db474fc6aa5902ec8c1792d7","modified":1505729869000},{"_id":"source/image/kodokan/2/1-6/5_female_2.jpg","hash":"c7738c5fa6057f34543bd0abfb82d52e7c9f4f10","modified":1505726926000},{"_id":"source/image/kodokan/2/7-8/7_0.jpg","hash":"3864a487a0945883c4175c8bf118de2bd02c153c","modified":1505726928000},{"_id":"source/image/kodokan/2/7-8/8_0.jpg","hash":"1a9c9850a06742131ce6485ac8d6568d4b5df611","modified":1505743334000},{"_id":"source/image/kodokan/2/7-8/all.jpg","hash":"620842b94166a6544065c729a941851bbd5c27c8","modified":1505730140000},{"_id":"source/image/kodokan/4/eyecatch.jpg","hash":"9441af6d28ac0f236e69a62041fe91e7eb2991e4","modified":1505726929000},{"_id":"source/image/DL/5/2-2.png","hash":"270e3c34dccde4362c772a51569b8cf0cda3932f","modified":1508746642000},{"_id":"source/image/RL/5/1-3.png","hash":"d1bb2c439eeb28a7eb9e0f1afb85f2e8c7f835c9","modified":1508992246000},{"_id":"source/image/kodokan/2/1-6/1_shop_3.jpg","hash":"fda5ae0de75882b30f6987f7d68fba77bd83c938","modified":1505740323000},{"_id":"source/image/kodokan/2/1-6/2_library_1.jpg","hash":"7d98025a5de25e161b0524e257171f9f0138476b","modified":1505726925000},{"_id":"source/image/kodokan/2/1-6/2_library_5.jpg","hash":"7dac8f034ceb5da079ea9473fa87d1ce2e2fffef","modified":1505726925000},{"_id":"source/image/kodokan/2/1-6/5_female_1.jpg","hash":"d1ceea501e876eb3211410097946776e02fcc87d","modified":1505726926000},{"_id":"source/image/kodokan/2/1-6/6_international.jpg","hash":"a63e4ba3ec8921ed09eaba8eb6be8f18a68e3516","modified":1505726926000},{"_id":"source/image/kodokan/2/1-6/6_school_1.jpg","hash":"13f545aa31170c75d6ca9a43e33cf40a991f5e57","modified":1505726926000},{"_id":"source/image/kodokan/2/1-6/6_school_2.jpg","hash":"33fb163d40c92fe930be0d1fe7a27ff3a3f5ba83","modified":1505726926000},{"_id":"source/image/kodokan/2/7-8/7_00.jpg","hash":"b8a22ee89ed60020e43f9b9e79c5aff5e89a6de6","modified":1505726928000},{"_id":"source/image/kodokan/2/7-8/8_1.jpg","hash":"a31be4f64d98f045a871b2ddb360f66230c069f1","modified":1505726927000},{"_id":"source/image/kodokan/2/7-8/8_4.jpg","hash":"1d1b88595b802d2b7fee66f710972118e9edd029","modified":1505726927000},{"_id":"source/image/kodokan/2/7-8/8_3.jpg","hash":"a0a76a5f298bfbd99b0e62582549798afdb660f1","modified":1505913447000},{"_id":"source/image/kodokan/2/7-8/8_5.jpg","hash":"7feec067ce5856a2fe1c891c735fc02957a495dd","modified":1505726928000},{"_id":"source/image/kodokan/2/7-8/8_7.jpg","hash":"63cd3db011918f8f9fc84eb4664a61f9e77f27b9","modified":1505726928000},{"_id":"source/image/kodokan/2/gym/gym.jpg","hash":"fe5dd63dc07c36a93a6023ddc2f143a8b8eadaee","modified":1505729214000},{"_id":"source/image/kodokan/2/gym/gym_3.jpg","hash":"7a490501302678b1fe29b31907e36bc13d7b4f6f","modified":1505726929000},{"_id":"source/image/kodokan/2/main/kano1.jpg","hash":"0a34313802c9129cd315b4b19a55deeab80094c6","modified":1505729384000},{"_id":"source/image/kodokan/2/main/kano2.jpg","hash":"ec83bd51d3c80391b5362950b86f9d298ee65c26","modified":1505729363000},{"_id":"source/image/kodokan/2/main/main_1.jpg","hash":"0df44174bd43df3bf3c531637c8120b858b69c8e","modified":1505729330000},{"_id":"source/image/kodokan/2/main/main_3.jpg","hash":"5a87e2b6304936308a4a067c26f4743d08776aee","modified":1505729274000},{"_id":"source/image/kodokan/2/main/main_2.jpg","hash":"bd2e28d92dfe89f422d545e1437c001187953853","modified":1505729301000},{"_id":"source/image/RL/5/0-0.png","hash":"24babb92689875e5be84e15fabad63bb6f40d984","modified":1508991992000},{"_id":"source/image/RL/5/1-1.png","hash":"9311e8d22c41b441441321de972bd50f06efed16","modified":1508992232000},{"_id":"source/image/RL/5/1-4.png","hash":"e076e1b4abb65dc1b3626e1c54fda7e358dd55ab","modified":1508992252000},{"_id":"source/image/RL/5/2-2.png","hash":"6001f2bd3f7ae5517be76f57782f602b89e97c3e","modified":1508992572000},{"_id":"source/image/RL/5/3-1.png","hash":"2fea9b9ea5566fdfaa4c302d3b283dbaa078b213","modified":1508990906000},{"_id":"source/image/kodokan/2/1-6/1_shop_2.jpg","hash":"80cc331b132b0d13edfe651ad21655e5ff07275f","modified":1505726925000},{"_id":"source/image/kodokan/2/1-6/2_library_2.jpg","hash":"21851e5607f00d4cab8b29e7938c6ab8f3a78337","modified":1505726925000},{"_id":"source/image/kodokan/2/1-6/2_library_3.jpg","hash":"aa8923ca96ccc94ea3381020efa1b9fd3b360c60","modified":1505726925000},{"_id":"source/image/kodokan/2/1-6/4.jpg","hash":"e0c6a025136c11edb1d1529914d1aacca2335f75","modified":1505726926000},{"_id":"source/image/kodokan/2/1-6/2_library_4.jpg","hash":"f1f4a3616d81a9138a4316f4633e517a7625bf5d","modified":1505726925000},{"_id":"source/image/kodokan/2/gym/gym_1.jpg","hash":"819c49985da2cbfe89182909efb0dc1c387bd252","modified":1505726929000},{"_id":"source/image/kodokan/2/gym/gym_2.jpg","hash":"d6cdff21644a6d67a1c8101e2f9345d540327647","modified":1505726929000},{"_id":"source/image/kodokan/2/main/kano3.jpg","hash":"3c092a41a8f7c23ed9511c89c8ec8981aaaf3d56","modified":1505728883000},{"_id":"source/image/kodokan/2/main/main_4.jpg","hash":"f1661c598d4448dae91c6ebe40a132c58671c1ef","modified":1505728883000},{"_id":"source/image/kodokan/3/info.jpg","hash":"af7358f8e3770de39cc15eff022d0c0ef7c47ad6","modified":1505726930000},{"_id":"source/image/kodokan/4/book.jpg","hash":"37f4f34e07ad1f181815c9bda3578f65fa468408","modified":1505726929000},{"_id":"source/image/sumiao4/MZ_3.jpg","hash":"470fb107cd4c6a5d2d16fa7d2143b46cb23814bc","modified":1513095306000},{"_id":"source/image/DL/2/Mask-RCNN.png","hash":"daa6c11cfb42d38ace3b87056ab3f0ec43c3c904","modified":1508412945000},{"_id":"source/image/DL/5/1-4.png","hash":"32db8207ee8d107faf8dd31fdc5a23cdde0233d4","modified":1508763936000},{"_id":"source/image/RL/5/1-2.png","hash":"fd329541d5f14fb5f4516ed82416092cca394454","modified":1508992238000},{"_id":"source/image/RL/5/2-1.png","hash":"928d3fb916caaf166d3cfb4cc2bca71651dbf334","modified":1508992566000},{"_id":"source/image/kodokan/2/1-6/6_school_3.jpg","hash":"3938d565030c317e1e646b33c2b307d3782fd516","modified":1505726927000},{"_id":"source/image/kodokan/2/7-8/8_6.jpg","hash":"ee5924472e6b0b1fa486c6a0f2fd4125bf63180b","modified":1505726928000},{"_id":"source/image/kodokan/2/nanai.jpg","hash":"027e39982444d3f8f33cd26a04b3c7557905d5d6","modified":1505726932000},{"_id":"source/image/PMP/PMP.png","hash":"0ea0404f9758c2687197ed06db64b799b713da69","modified":1585617816605},{"_id":"source/image/kodokan/2/1-6/1_shop_1.jpg","hash":"3d458ac2ab4ea6673958562d4dc24e168d2f0a43","modified":1505726925000},{"_id":"source/image/kodokan/2/7-8/8_2.jpg","hash":"b74a4692730ea0bbdf9603aaa9a3d82b41b8068c","modified":1505726927000},{"_id":"source/image/kodokan/2/1-6/1_2.jpg","hash":"03d1a52b4b515e2c699b120afe4a5a470fc477c9","modified":1505726925000},{"_id":"source/image/DL/2/4.gif","hash":"2e8faa1f16aafb0b54c13da4cc608af57aaa9672","modified":1505643597000},{"_id":"source/image/kodokan/2/1-6/1_1.jpg","hash":"c9b54ac3a8d74ca70fe91abc9162080f39aa1871","modified":1505726925000},{"_id":"source/image/RL/5/3-2.png","hash":"668b5191318bd7b39dde1ec1667bb4be93f78c16","modified":1508990912000},{"_id":"themes/huno/source/images/background-cover.jpg","hash":"08162d5ddd6c081f2713bfd10261e62f0752a600","modified":1480240788000},{"_id":"source/image/DL/5/1-3.png","hash":"d3c79cf82e8054361f7da5a63829ba67cc588b80","modified":1508763924000},{"_id":"themes/huno/demo.gif","hash":"cf6121b4aa0f3b07a2f919099942d97206202aa8","modified":1480240788000},{"_id":"source/image/kodokan/2/7-8/Summer.png","hash":"c66fdc3702a9d10cd31c835090a258509d71ccd3","modified":1505718582000},{"_id":"themes/huno/.git/objects/pack/pack-50c872e1804ca429688685ab4ff7efcb5e460bca.pack","hash":"c741cf3cc121d2f40a7937b4fdc82f74633498db","modified":1480240788000},{"_id":"public/About/index.html","hash":"289d3a751fa25833a4ab84d535341f2cc00a8874","modified":1608025273244},{"_id":"public/Archive/index.html","hash":"843daa51c5f58c4abb6ebd66cd38ed27f8e8ace5","modified":1608025273244},{"_id":"public/2020/12/14/ja-剣道用語-Kendo-terminology/ja/index.html","hash":"78b5657281e508d5254284bdb38b9380de5e2658","modified":1608025273244},{"_id":"public/2020/04/01/en-My-PMP-Study/en/index.html","hash":"4727e6d12156f98c0043446bb45ddc6aeb92c7ec","modified":1608025273244},{"_id":"public/2019/03/10/ja-My-Japanese-Episode/ja/index.html","hash":"99a0f746725cf726c644a5a6a5d490182090649f","modified":1608025273244},{"_id":"public/2018/08/18/ja-EOSのDAPP開発流れ/ja/index.html","hash":"68916a4c0e77a9e6982f3fe5ed974e654739ee16","modified":1608025273244},{"_id":"public/2018/07/07/ja-ETHのDAPP開発流れ/ja/index.html","hash":"c029c2785bb8d60aa59ab245bbd62cebbb1bc691","modified":1608025273244},{"_id":"public/2017/12/12/zh-人物素描4-0/zh/index.html","hash":"826ff2ad323e60baa8cdcb52e175403109c3395e","modified":1608025273245},{"_id":"public/2017/12/08/zh-人物素描3-0/zh/index.html","hash":"9b3b56aaab7c23bb4a39da26cf52013b13428027","modified":1608025273245},{"_id":"public/2017/11/12/zh-Reinforcement-Learning笔记2-Bellman/zh/index.html","hash":"33a3a15c7ab23b424c52cc605a59fd0ca860a5a6","modified":1608025273245},{"_id":"public/2017/11/11/zh-Reinforcement-Learning笔记1-MDP/zh/index.html","hash":"cf8ea4ac12d2d3114b8c916c8a6735b7ebb2b21d","modified":1608025273245},{"_id":"public/2017/08/24/zh-JUDO-ONO-SHOHEI-柔道-大野将平/zh/index.html","hash":"6b3d96e81eed970ddcba839edc8b469a226cb469","modified":1608025273245},{"_id":"public/2017/07/28/zh-Machine-learning笔记5-Model/zh/index.html","hash":"36fe560cca9782338d7514bdda55d439291ca824","modified":1608025273245},{"_id":"public/2017/07/28/zh-Machine-learning笔记4-Data-Analysis/zh/index.html","hash":"679c6e0715d72fa8dd723273ca4cb99dfd8e753e","modified":1608025273245},{"_id":"public/2017/07/28/zh-Machine-learning笔记3-Linear-algebra/zh/index.html","hash":"f647bd38d3b45c71a14421875583739dac28c7e5","modified":1608025273245},{"_id":"public/2017/07/28/zh-Machine-learning笔记2-Statistic/zh/index.html","hash":"e31314ebd91b2c3207d12cb91917148f0b4b74f0","modified":1608025273245},{"_id":"public/2017/07/28/zh-Machine-learning笔记1-Python/zh/index.html","hash":"4c3dc16cc6cd08c8c1ee013f7c906818f8706bbb","modified":1608025273245},{"_id":"public/2017/01/04/zh-Mac-OS-X设置默认Python版本/undefined/index.html","hash":"3aaf5d77cffd8f92d410117e5ba5eb799b585565","modified":1608025273245},{"_id":"public/2017/01/01/en-Carmen-Guitar/en/index.html","hash":"c04a51b1571e8abe28c6e9f9795d8f9e20364e94","modified":1608025273245},{"_id":"public/2016/12/10/ar-اللغة-الإنجليزية-في-المطبخ-الصيني/fr/index.html","hash":"263301f4eb0e333d97093c175a2760bf548e4e83","modified":1608025273245},{"_id":"public/2016/12/10/fr-Un-Plan-—-Le-Voyage-en-France/fr/index.html","hash":"266c5bfad4abfd769f901bf300f4f0a3739ed394","modified":1608025273245},{"_id":"public/2016/12/10/en-Active-And-Passive-Vocabulary/us/index.html","hash":"b0e3a561005ffd2072bd4504e1908234969feda1","modified":1608025273245},{"_id":"public/2016/11/30/zh-“御茶ノ水”-乐器街之行/zh/index.html","hash":"927e8f7621337e022193ba4872a4ddbdcb4459a2","modified":1608025273245},{"_id":"public/2016/11/30/ja-紙幣の肖像人物/ja/index.html","hash":"429172e5444fe0cda3728adda34a0571a2026fdc","modified":1608025273245},{"_id":"public/2016/11/30/zh-人物素描2-0/zh/index.html","hash":"b98cdbf1ee3c2aa9582656c9ab4de2c7609f6c9c","modified":1608025273246},{"_id":"public/2016/11/30/zh-人物素描1-0/zh/index.html","hash":"6c7655c81832c10ec969154cb7a8b79b5b12deeb","modified":1608025273246},{"_id":"public/2016/11/30/zh-游泳动图/zh/index.html","hash":"5a32b46b0c4b8c0fd3cfacddcfc6f8fb321a790f","modified":1608025273246},{"_id":"public/2016/11/30/ja-富士登山/ja/index.html","hash":"01e2dee4404baeb382ee7f9afb5a8d8a8a1b62b2","modified":1608025273246},{"_id":"public/2016/11/30/zh-石寨古村/zh/index.html","hash":"bc69a87a318eb47ea647b54bb363d4d833e2d3f6","modified":1608025273246},{"_id":"public/2016/11/28/en-Hello-World/us/index.html","hash":"d43fa8a2c12484fc0924d26b135e8179bb310bc1","modified":1608025273246},{"_id":"public/index.html","hash":"6dc9de21db9852dbe4e532605867058c38f02877","modified":1608025273246},{"_id":"public/page/2/index.html","hash":"976c635cb526270fee157dbe590b12c92fec84ed","modified":1608025273246},{"_id":"public/page/3/index.html","hash":"0b7377133a31dadab3f0f2740cf53359d2308ffe","modified":1608025273246},{"_id":"public/page/4/index.html","hash":"25fb54972f6a07f827745172904a1c1f05ee6341","modified":1608025273246},{"_id":"public/tags/PM/index.html","hash":"df3230ee5ea338da1691014b7c7c7bc1263972e8","modified":1608025273246},{"_id":"public/tags/Blockchain/index.html","hash":"b6b432e4a11f1cec15c80f9123dcd447ae8cb358","modified":1608025273246},{"_id":"public/tags/Kendo/index.html","hash":"b4d78da813a104f233b3aeddf4ba7d2ff4194651","modified":1608025273246},{"_id":"public/tags/Deep-Learning/index.html","hash":"641dedbfc8c83c1945284309948b54802f720507","modified":1608025273246},{"_id":"public/tags/Judo/index.html","hash":"acff784fc928193a6dd6b7859277a7ba928cc1e2","modified":1608025273246},{"_id":"public/tags/Machine-Learning/index.html","hash":"231c971a7f394df4840c42ee1730f98d123a9a02","modified":1608025273246},{"_id":"public/tags/Reinforcement-Learning/index.html","hash":"7665d7a3e7468053f04b9e40e85268c67dde1ce4","modified":1608025273246},{"_id":"public/tags/Sketches/index.html","hash":"21b23f16a73b2131cd6d6b9060b3f01db56d365b","modified":1608025273247},{"_id":"public/archives/index.html","hash":"c379cbfc3c76d579a6b0c2271b5cf6b332e419db","modified":1608025273247},{"_id":"public/archives/page/2/index.html","hash":"f8284e528c32fcad01b5411f7d65f59e98f24561","modified":1608025273247},{"_id":"public/archives/page/3/index.html","hash":"6b91fcef62bb3932a0e44cf68d35986ebcc7a1d0","modified":1608025273247},{"_id":"public/archives/page/4/index.html","hash":"cc9b3207d82d71cf31ce66ee50c06585476c8161","modified":1608025273247},{"_id":"public/archives/2016/index.html","hash":"91e9563f744723d59b25b4012f7a81ec76962b1a","modified":1608025273247},{"_id":"public/archives/2016/page/2/index.html","hash":"e4a0b5e6c4c0467be93521eca90b40d43837b5da","modified":1608025273247},{"_id":"public/archives/2016/11/index.html","hash":"493d2c03047ee6b1f495182fc2f84adba7b3b785","modified":1608025273247},{"_id":"public/archives/2016/12/index.html","hash":"f3569f2a890f23a3dfbafeaaa4029957d4abfee0","modified":1608025273247},{"_id":"public/archives/2017/index.html","hash":"a7f007bc83b4515ca16dce9ace0169cac8d8ce96","modified":1608025273247},{"_id":"public/archives/2017/page/2/index.html","hash":"0a3437f0ee59b66171952395a23b271cca004fb1","modified":1608025273247},{"_id":"public/archives/2017/page/3/index.html","hash":"5c8f55f815588dd2065aa3af34c8d0b358184426","modified":1608025273247},{"_id":"public/archives/2017/01/index.html","hash":"1806d122038665b15c167d131ec2a6439b86c190","modified":1608025273247},{"_id":"public/archives/2017/07/index.html","hash":"94f5dec5c3bc783b7e03ec407556503f5cc05344","modified":1608025273247},{"_id":"public/archives/2017/08/index.html","hash":"4bbb3c45d25e77643a8c14f110a604ca202185e0","modified":1608025273248},{"_id":"public/archives/2017/09/index.html","hash":"d240316d8a5e593535e9b0799637133151a09b77","modified":1608025273248},{"_id":"public/archives/2017/10/index.html","hash":"150d0e9f1a7b5d616c93ac85f707b15fc7b73f3d","modified":1608025273248},{"_id":"public/archives/2017/11/index.html","hash":"d375d5c56c004eb24f736d62501b099f86edb84f","modified":1608025273248},{"_id":"public/archives/2017/12/index.html","hash":"97e343442c4570d3d1ca512b02e9e1297e523d5a","modified":1608025273248},{"_id":"public/archives/2018/index.html","hash":"3f7c2fd4fbadb350cb851ba59bc68cafc5b59a77","modified":1608025273248},{"_id":"public/archives/2018/07/index.html","hash":"4f92c05729f6d3e016ddb4de7a705353947817f0","modified":1608025273248},{"_id":"public/archives/2018/08/index.html","hash":"bedde3eb65e255aa2e77d88ed166bae53cf3d9fa","modified":1608025273248},{"_id":"public/archives/2019/index.html","hash":"03156ac19c4cdb4a835d71b336b743892b8b6b28","modified":1608025273248},{"_id":"public/archives/2019/03/index.html","hash":"2ab95e78ba15446e9b8be50d6a3748729cc0c536","modified":1608025273248},{"_id":"public/archives/2020/index.html","hash":"287c30ccb219a600bf765a514f135f2ec928e919","modified":1608025273248},{"_id":"public/archives/2020/04/index.html","hash":"99d87ef186293f07ee4853c836a216e2f8549783","modified":1608025273248},{"_id":"public/archives/2020/12/index.html","hash":"7f4fba3662dc74ea628d1afbfaf5aa77cbc31b20","modified":1608025273248},{"_id":"public/2017/11/15/zh-Reinforcement-Learning笔记5-Temporal-Diff/zh/index.html","hash":"5c51351ee82c69ef8974f40dafbee57cb324e5aa","modified":1608025273248},{"_id":"public/2017/11/14/zh-Reinforcement-Learning笔记4-Monte-Carlo/zh/index.html","hash":"7b248d3adab81eab1f2c978dbf727a2f992e26cb","modified":1608025273248},{"_id":"public/2017/11/13/zh-Reinforcement-Learning笔记3-Dynamic-Program/zh/index.html","hash":"4c7dec113b23c9b2a61676ca80a059f1f7890dd9","modified":1608025273248},{"_id":"public/2017/11/02/zh-JUDO-Kodokan-柔道-講道館/zh/index.html","hash":"d6c4949f3181fc797acff04550ab8925aa047b28","modified":1608025273248},{"_id":"public/2017/11/01/zh-Deep-learning笔记5-GAN生成式对抗网络/zh/index.html","hash":"9dacda808458c66d5bbba00448990218041b7819","modified":1608025273248},{"_id":"public/2017/10/19/zh-Deep-learning笔记4-TreeRNN递归神经网络/zh/index.html","hash":"8e3727d2b7779c3217c9cf26061c12174e7fe886","modified":1608025273249},{"_id":"public/2017/10/16/zh-Deep-learning笔记3-RNN循环神经网络/zh/index.html","hash":"7a4601f76ee113836036639153a1944bddbfd301","modified":1608025273249},{"_id":"public/2017/09/16/zh-Deep-learning笔记2-CNN卷积神经网络/zh/index.html","hash":"e3c1b45018112ebc4d0ab9c4e653eee399845ecd","modified":1608025273249},{"_id":"public/2017/08/28/zh-Deep-learning笔记1-神经网络入门/zh/index.html","hash":"cece7f7ecbd860762ae2bd365f7eeb92b76cfcd5","modified":1608025273249},{"_id":"public/2016/11/30/ja-柔道用語-Judo-terminology/ja/index.html","hash":"e9efac79edd86810af5d25a6ece6fd74dea57aa5","modified":1608025273249},{"_id":"public/favicon-black-50.png","hash":"8db9853d8ff7464288d3f9a051c896d71fa36612","modified":1608025273272},{"_id":"public/favicon-black-55.png","hash":"c2615db27a8cf7ddca6e9fe042061d464889e1d1","modified":1608025273272},{"_id":"public/favicon-red-50.png","hash":"fb4a64e7ce56fefcab17b579047fb9ea336ac456","modified":1608025273272},{"_id":"public/images/totop.png","hash":"4f6cb11941e5a72b03cb00cf9d9d55671b4310eb","modified":1608025273272},{"_id":"public/image/Judo/Judo.jpg","hash":"3f1779d54ccba9aa3a4512582ff5458684850ac2","modified":1608025273272},{"_id":"public/image/kodokan/kodokan.png","hash":"093921845c26cc3e599a622de1ecfc1ff01a08aa","modified":1608025273273},{"_id":"public/image/Yamaki/Yamaki-2.jpg","hash":"41657e4c8a6eaa3d16bf16979621915666d9324c","modified":1608025273273},{"_id":"public/image/sumiao/sumiao_3.jpg","hash":"79840fcddd550d2f41e717dff0448373d87cdffa","modified":1608025273273},{"_id":"public/image/sumiao2/W1.jpg","hash":"41b2d39ffb81d175245cdeec9aea9435cb0bd948","modified":1608025273273},{"_id":"public/fonts/china-social/china-social.eot","hash":"a43a5c3d66f0d38639a595ebd02857e152ada475","modified":1608025273273},{"_id":"public/fonts/china-social/china-social.ttf","hash":"2f94360528097df7dcfb39baf8df5393a0d47ca3","modified":1608025273273},{"_id":"public/fonts/china-social/china-social.woff","hash":"74c0ac5268cf7ffe270faaf7c960b74d483d2df1","modified":1608025273273},{"_id":"public/fonts/foundation-icons/foundation-icons.woff","hash":"112fb0e498037f2fea036adb8105e47638159eaa","modified":1608025273273},{"_id":"public/fonts/foundation-icons/foundation-icons.eot","hash":"d584172686583fd510d8f04cf21e6e77fce51435","modified":1608025273273},{"_id":"public/fonts/foundation-icons/foundation-icons.ttf","hash":"4b2bce6c792493a4a5716b6fec2dbefe89492c3f","modified":1608025273273},{"_id":"public/image/DL/1/simple-neuron.png","hash":"9bc311cbe617d0f6e170700dcf4ca4ad6fffa005","modified":1608025273273},{"_id":"public/image/DL/2/1.png","hash":"9b5084ccb69a7e45501e958021fe68b8934e5dcf","modified":1608025273273},{"_id":"public/image/DL/2/2.png","hash":"78a0750ec6c90e1b05f0921a380f526c106fc787","modified":1608025273273},{"_id":"public/image/DL/2/autoencoder_1.png","hash":"38926dfd0c4a4186479d974ba9ce27ce08173c35","modified":1608025273273},{"_id":"public/image/DL/4/1-3.png","hash":"ead2ff0b5f04cb8b951fa99c055abbd0d05d30f5","modified":1608025273273},{"_id":"public/image/DL/4/1-2.png","hash":"a2d44bce2370290e018649795a6733f33e5cc0eb","modified":1608025273273},{"_id":"public/image/DL/4/1-4.png","hash":"b451f2196b9131043c1a79c3b28bf867c58f8b18","modified":1608025273273},{"_id":"public/image/DL/4/2-1.png","hash":"e8b6b577a805d4814217782eda7fe800805725b0","modified":1608025273274},{"_id":"public/image/DL/5/1-1.jpg","hash":"6f66a6574454d73be6ae3d180733ee5558d8d3d4","modified":1608025273274},{"_id":"public/image/DL/5/1-2.jpg","hash":"8853e9144a9703d567e8962701c7cbeb321147f5","modified":1608025273274},{"_id":"public/image/DL/5/2-1.png","hash":"ed0def464880d0d8f8126edbf97f46177c7bad3c","modified":1608025273274},{"_id":"public/image/DL/3/1-1.jpg","hash":"d773a5e06c3ca759faf52d1dc18ff54d5a4c42ee","modified":1608025273274},{"_id":"public/image/DL/3/1-2.jpg","hash":"6e87c264d76e431a3f4aa83402b651fa12c9eba9","modified":1608025273274},{"_id":"public/image/DL/3/1-3.jpg","hash":"9d577ef529ae80d3171206391b2e5c49fd61c58a","modified":1608025273274},{"_id":"public/image/DL/3/2-2.jpg","hash":"4794126963d6e35953786ae4be2aa674b7b32554","modified":1608025273274},{"_id":"public/image/DL/3/2-1.jpg","hash":"c519937588cf8b4955da9f5d734684c467157fc2","modified":1608025273274},{"_id":"public/image/DL/3/3-1.jpg","hash":"3e0b8339e9f286312cf59723ac00b865bf09c06e","modified":1608025273274},{"_id":"public/image/DL/3/3-2.jpg","hash":"177284be67b45f1a67e805e9bee799dbcf12034f","modified":1608025273274},{"_id":"public/image/DL/3/4-2.png","hash":"de557b778e92300f5e943078e1f1256f9d10ea57","modified":1608025273274},{"_id":"public/image/RL/2/1.jpg","hash":"dfa3f65d56f28ec18206df404c0da32272875e13","modified":1608025273274},{"_id":"public/image/DL/3/4-3.jpg","hash":"22ceea95d5db169e97656a9b8053192ede101a20","modified":1608025273274},{"_id":"public/image/RL/2/3.jpg","hash":"5e19502712d0d320990f72890b9a061d0e2cc34f","modified":1608025273274},{"_id":"public/image/RL/2/2.jpg","hash":"a3a335dd4164b8bd3963930b162c0a8d28e42c93","modified":1608025273275},{"_id":"public/image/RL/3/4.png","hash":"5f4e811abbca628809b60d173be0ca4ecb6edb85","modified":1608025273275},{"_id":"public/image/RL/4/0-1.png","hash":"0863e29a555ba3e72ff62fa1c87a0fb008325c77","modified":1608025273275},{"_id":"public/image/RL/4/2-1.svg","hash":"3c0d2ada216d01fa2ac32cfd0deb103de4690636","modified":1608025273275},{"_id":"public/image/RL/3/3.jpg","hash":"5eb8dd943d509e59ea061739b58635f8078637e6","modified":1608025273275},{"_id":"public/image/RL/4/2-3.svg","hash":"43f78cabff19809b1e09634e833593f4b5334e1f","modified":1608025273275},{"_id":"public/image/RL/4/2-2.png","hash":"f868c4f0f62d686d8c8968f8ae33eed68b924ecb","modified":1608025273275},{"_id":"public/image/RL/5/4-1.png","hash":"cb68c87ad36d3b3687ec8bef3cb8159421c3d3da","modified":1608025273275},{"_id":"public/fonts/china-social/china-social.svg","hash":"4bad780e6a31f4fa9fef037a3d9ecb8623042a6e","modified":1608025273275},{"_id":"public/image/DL/2/3.gif","hash":"05c0c84c210f5d25a8250ff1455d897f74aa372d","modified":1608025273275},{"_id":"public/image/DL/4/1-1.png","hash":"4b49e025ba481676345901bdf017ffdeca77ce24","modified":1608025273275},{"_id":"public/favicon.ico","hash":"c39d2a9a646fb119ee08f22f14e06821c051fd6a","modified":1608025273470},{"_id":"public/favicon.png","hash":"201be090f2ef3c79d66c7dfb644bb3fbe6f98c9d","modified":1608025273470},{"_id":"public/image/shizhai.jpg","hash":"24705def6d328fb2cf039738e93005377fd8f803","modified":1608025273471},{"_id":"public/image/Judo/Budokan.jpg","hash":"f2dce6c14d8444232d862e4450fe50b3047b35ae","modified":1608025273471},{"_id":"public/image/Fuji/Fuji_2.jpg","hash":"b9e4b3843c9cb1fdc3d94bc61948489f6115a9b7","modified":1608025273471},{"_id":"public/image/Judo/ONO SHOHEI.jpg","hash":"6eb35ddab530b7e0caa343d318d755dda3010399","modified":1608025273471},{"_id":"public/image/Kendo/Kendo-1.jpg","hash":"2f659bc7e053003517bf077057b2cd36890a9863","modified":1608025273471},{"_id":"public/image/Yamaki/Yamaki-1.jpg","hash":"260a52d654b3714059d4e3adb2f1dfcf1a338586","modified":1608025273471},{"_id":"public/image/sumiao/sumiao_4.jpg","hash":"2ce4ce1bd64b4e008884b5e1a7251961dc061814","modified":1608025273471},{"_id":"public/image/sumiao/sumiao_2.jpg","hash":"acd80a32757f02f19286eae565196d0d0113b9ca","modified":1608025273471},{"_id":"public/image/sumiao/sumiao_5.jpg","hash":"a93d05e720cbddf3d34fa1dd6e3dac6a130f7bb3","modified":1608025273471},{"_id":"public/image/sumiao/sumiao_7.jpg","hash":"144de9f90c163e6e9cb31d9d3ee409885d6c0ba0","modified":1608025273472},{"_id":"public/image/sumiao/sumiao_6.jpg","hash":"e335ea201156390d30a5d51e683c2bb5e8f0e526","modified":1608025273472},{"_id":"public/image/sumiao2/W.jpg","hash":"8045939eb02258a4f8fc2f4eb1c5f0f2460d69ab","modified":1608025273472},{"_id":"public/image/sumiao2/W3.jpg","hash":"1337eb53841f70e1bc9174087736e001ed83e7d2","modified":1608025273472},{"_id":"public/image/sumiao2/W2.jpg","hash":"d626046abb07a20a6ffe478177a2b72a3b98a0fb","modified":1608025273472},{"_id":"public/image/sumiao3/ZYY.jpg","hash":"03052e31a9a35c5290af65278de2c60dfbfb41f3","modified":1608025273472},{"_id":"public/image/sumiao2/W4.jpg","hash":"e0eccdfa9751ea6a198487e762b3414f17617cc6","modified":1608025273472},{"_id":"public/image/sumiao3/ZYY_1.jpg","hash":"13ade590e37f71df32d1cdf6b32df63f99ce60a8","modified":1608025273472},{"_id":"public/image/sumiao3/ZYY_2.jpg","hash":"748c7a264c424cb991d9e37aaaac1537e5df5291","modified":1608025273472},{"_id":"public/image/sumiao3/ZYY_back_0.jpg","hash":"42f17e98e8ed74969da244b94f58f85cf4be48e1","modified":1608025273472},{"_id":"public/image/sumiao3/ZYY_back.jpg","hash":"b01dcaeec84f6ad73e830bd1b0601b1f822a63ce","modified":1608025273472},{"_id":"public/image/sumiao4/MZ.jpg","hash":"2db78e05fe23b05a09b098616f47a118ae0d89be","modified":1608025273473},{"_id":"public/image/swim/backstroke_1.gif","hash":"4726a16f171482580be8af2c791a43e6d0d55cae","modified":1608025273473},{"_id":"public/image/swim/backstroke_2.gif","hash":"98ffcc539b6d45523646120e16267d88c0e94e5d","modified":1608025273473},{"_id":"public/image/swim/breaststroke_1.gif","hash":"bdad2dbb146d51ba5eb8d6d7a1fbec2108ae04b7","modified":1608025273473},{"_id":"public/image/swim/backstroke_3.gif","hash":"8ae65c6f31e4a71252e7e14e9652617b87f90a1c","modified":1608025273473},{"_id":"public/image/swim/butterfly_1.gif","hash":"7b43b04db70d8d87e6ec90d4eb47708aa2e8d310","modified":1608025273473},{"_id":"public/image/swim/breaststroke_2.gif","hash":"ab61e2ee3213eee8b02315de5ffb6d2b3ea0d09f","modified":1608025273473},{"_id":"public/image/swim/butterfly_2.gif","hash":"079986281fb045e17ddb20e2513c07537881650f","modified":1608025273473},{"_id":"public/image/swim/butterfly_3.gif","hash":"ef1aa23191500ccece3033a01e8e3022851164a9","modified":1608025273473},{"_id":"public/image/swim/freestyle_1.gif","hash":"fc19506e8267291c582cf4057d6b213accb4485e","modified":1608025273473},{"_id":"public/image/swim/freestyle_2.gif","hash":"0f8c8fdac8b507b0cd1e610406ce854e8296d356","modified":1608025273473},{"_id":"public/image/swim/freestyle_3.gif","hash":"ee1277e8382d01ee79b7e3b4d871a3625fdfe204","modified":1608025273474},{"_id":"public/image/Blockchain/ETH/ETH.jpg","hash":"8af4232a2c19e973d6bfd58e01f171c62898050a","modified":1608025273474},{"_id":"public/image/Blockchain/EOS/EOS_DAPP.jpg","hash":"6b9c7c0d826e742b482a01a0361ebf0005696f4f","modified":1608025273474},{"_id":"public/image/Blockchain/ETH/ETH_DAPP.jpg","hash":"c57f998c55bc4595c06f4370ec494bad6f62c66f","modified":1608025273474},{"_id":"public/image/DL/2/5.png","hash":"9ae433c9e12800e7bfa44f85c5f2a946b5cc3f09","modified":1608025273474},{"_id":"public/image/DL/2/cnnarchitecture.jpg","hash":"23822fc936fc007e5ece3383a7d79c8dcfffdd22","modified":1608025273474},{"_id":"public/image/DL/2/CNN.jpg","hash":"ccd44500fe439c7255da395f1bd2ca6d2fc91b5f","modified":1608025273474},{"_id":"public/image/DL/3/2-3.jpg","hash":"a52bb3a0c1d5e462c45686b2b657623d41199104","modified":1608025273474},{"_id":"public/image/DL/3/4-1.png","hash":"b13785b9c0595823509bb9137f5df647a94e507f","modified":1608025273474},{"_id":"public/image/RL/3/1.jpg","hash":"bee410ef6c618e53327472d25463cc6fc39ebb00","modified":1608025273474},{"_id":"public/image/RL/3/2.jpg","hash":"4b1aac50a74e052045a5c86b3a46af3b44af2af3","modified":1608025273475},{"_id":"public/image/RL/5/4-2.png","hash":"4f4238dc42d7f3dbc20411976ded9d97f469108a","modified":1608025273475},{"_id":"public/image/RL/5/4-3.png","hash":"3f0deb2369782f568bb66959272449cecc6e91ce","modified":1608025273475},{"_id":"public/image/RL/5/4-4.png","hash":"8c5a4b2d1ae7c4f2fbc332cb130b2b8f347238b8","modified":1608025273475},{"_id":"public/image/kodokan/2/two_buildings.jpg","hash":"95874bcc189db3b44bf3adf9e9a7bcf82c3227ee","modified":1608025273475},{"_id":"public/image/kodokan/1/kano_1.jpg","hash":"7f27da0efd29dd573ddda0c6423b5f56fa0cf0e6","modified":1608025273475},{"_id":"public/image/kodokan/3/fee.jpg","hash":"93b74ee593b2b15802665d1c8147629682b47322","modified":1608025273475},{"_id":"public/image/kodokan/3/female.jpg","hash":"127b0ebc7abd5ef99ca8b5cc373935550a486687","modified":1608025273475},{"_id":"public/image/kodokan/3/male.jpg","hash":"6d2c6c7d4e72780690f2212521926d9b5be83c2e","modified":1608025273475},{"_id":"public/image/kodokan/2/7-8/7_kano_2.jpg","hash":"014dd6338a01a7e0091cd274f360d7bcf2f387fe","modified":1608025273475},{"_id":"public/image/kodokan/2/7-8/all_2.jpg","hash":"7c27a664789a9fd14c9abf65a07f362c7d1ab1a6","modified":1608025273476},{"_id":"public/image/kodokan/2/1-6/2_kano_3.jpg","hash":"7a8f2d97c737b624db474fc6aa5902ec8c1792d7","modified":1608025273476},{"_id":"public/image/kodokan/2/7-8/all.jpg","hash":"620842b94166a6544065c729a941851bbd5c27c8","modified":1608025273476},{"_id":"public/image/kodokan/2/7-8/8_0.jpg","hash":"1a9c9850a06742131ce6485ac8d6568d4b5df611","modified":1608025273476},{"_id":"public/css/archive.css","hash":"88005d441d52e143cd2ab4da53b1ec50e22f9ade","modified":1608025273490},{"_id":"public/css/china-social-icon.css","hash":"2f90442f6d0d289e49c07a85c2dae32cab8b2063","modified":1608025273491},{"_id":"public/css/highlight.css","hash":"701da6788bee02c165236174b6767ae06957f051","modified":1608025273492},{"_id":"public/js/awesome-toc.min.js","hash":"b4d0f2a33f8340eb2543e8b2cee0dfd745cfb54a","modified":1608025273492},{"_id":"public/js/jquery.githubRepoWidget.min.js","hash":"94a141fa474ec5022f7c397b4fd3ff92405ab755","modified":1608025273492},{"_id":"public/js/scale.fix.js","hash":"bbf62458c63fca2535965f9dfaacfbc6bc697e03","modified":1608025273492},{"_id":"public/js/main.js","hash":"bafb068e72c56db9e66ac148512b1b21cf5547e1","modified":1608025273492},{"_id":"public/image/Carmen/Carmen_3.jpg","hash":"d1f64986ca226dee5d4c8ea935475dee8b743156","modified":1608025273492},{"_id":"public/image/Fuji/Fuji_1.jpg","hash":"f1bde66eff6aa3bd100f852ed824700ec8f8ffc4","modified":1608025273492},{"_id":"public/image/ML/3_6.jpg","hash":"7e8a247633436f6c8cd355eadc1ffa870fce6c36","modified":1608025273492},{"_id":"public/image/swim/breaststroke_3.gif","hash":"58044f449783c591cf94d0446817c948768b17b1","modified":1608025273492},{"_id":"public/image/sumiao3/ZYY_3.jpg","hash":"7c031519239ac556782942d42e91d099ad09becf","modified":1608025273492},{"_id":"public/image/sumiao3/ZYY_4.jpg","hash":"33ffb223cea746188303b4748ee80d1a12d4d502","modified":1608025273493},{"_id":"public/image/DL/5/1-5.png","hash":"b15a9910328ff24fa1b40e06afc4572f5133d287","modified":1608025273493},{"_id":"public/fonts/foundation-icons/foundation-icons.svg","hash":"bdca38f453f9935203fe8cd071e97d7f8576e0be","modified":1608025273493},{"_id":"public/image/kodokan/2/1-6/1_shop_3.jpg","hash":"fda5ae0de75882b30f6987f7d68fba77bd83c938","modified":1608025273493},{"_id":"public/image/kodokan/2/1-6/2_library_1.jpg","hash":"7d98025a5de25e161b0524e257171f9f0138476b","modified":1608025273493},{"_id":"public/image/kodokan/2/1-6/6_international.jpg","hash":"a63e4ba3ec8921ed09eaba8eb6be8f18a68e3516","modified":1608025273493},{"_id":"public/image/kodokan/2/1-6/6_school_1.jpg","hash":"13f545aa31170c75d6ca9a43e33cf40a991f5e57","modified":1608025273493},{"_id":"public/image/kodokan/2/gym/gym.jpg","hash":"fe5dd63dc07c36a93a6023ddc2f143a8b8eadaee","modified":1608025273493},{"_id":"public/image/kodokan/2/1-6/2_library_2.jpg","hash":"21851e5607f00d4cab8b29e7938c6ab8f3a78337","modified":1608025273494},{"_id":"public/image/kodokan/2/1-6/2_library_3.jpg","hash":"aa8923ca96ccc94ea3381020efa1b9fd3b360c60","modified":1608025273494},{"_id":"public/fonts/china-social/readme.html","hash":"9ac3d63c7a1d6c6defe7151358bcfcacbfadf2fe","modified":1608025273509},{"_id":"public/image/Carmen/Carmen_1.jpg","hash":"e1b248acf8fc918cccf6baae06e2350fb666ba6f","modified":1608025273509},{"_id":"public/image/ML/2_7.jpg","hash":"56863570df64416c54342c9db4145320fc00a48f","modified":1608025273509},{"_id":"public/image/ML/3_4.jpg","hash":"fc92778cd1bf24204e5937d2630834f8bbc92fba","modified":1608025273509},{"_id":"public/image/DL/1/BP.gif","hash":"feb6eaa9f328a918d130241588a42bb67ef6ae46","modified":1608025273509},{"_id":"public/image/DL/4/2-2.png","hash":"b661a261ed4fb44377e0bb174ca7e69c36ac6e4d","modified":1608025273509},{"_id":"public/image/kodokan/2/7-8/7_0.jpg","hash":"3864a487a0945883c4175c8bf118de2bd02c153c","modified":1608025273509},{"_id":"public/image/kodokan/2/1-6/2_library_5.jpg","hash":"7dac8f034ceb5da079ea9473fa87d1ce2e2fffef","modified":1608025273510},{"_id":"public/image/kodokan/2/1-6/6_school_2.jpg","hash":"33fb163d40c92fe930be0d1fe7a27ff3a3f5ba83","modified":1608025273510},{"_id":"public/image/kodokan/2/7-8/8_4.jpg","hash":"1d1b88595b802d2b7fee66f710972118e9edd029","modified":1608025273510},{"_id":"public/image/kodokan/2/7-8/8_1.jpg","hash":"a31be4f64d98f045a871b2ddb360f66230c069f1","modified":1608025273510},{"_id":"public/image/kodokan/2/7-8/7_00.jpg","hash":"b8a22ee89ed60020e43f9b9e79c5aff5e89a6de6","modified":1608025273510},{"_id":"public/image/kodokan/2/7-8/8_5.jpg","hash":"7feec067ce5856a2fe1c891c735fc02957a495dd","modified":1608025273510},{"_id":"public/image/kodokan/2/7-8/8_3.jpg","hash":"a0a76a5f298bfbd99b0e62582549798afdb660f1","modified":1608025273511},{"_id":"public/image/kodokan/2/7-8/8_7.jpg","hash":"63cd3db011918f8f9fc84eb4664a61f9e77f27b9","modified":1608025273511},{"_id":"public/image/kodokan/2/main/kano1.jpg","hash":"0a34313802c9129cd315b4b19a55deeab80094c6","modified":1608025273511},{"_id":"public/image/kodokan/2/main/kano2.jpg","hash":"ec83bd51d3c80391b5362950b86f9d298ee65c26","modified":1608025273511},{"_id":"public/image/kodokan/2/main/main_3.jpg","hash":"5a87e2b6304936308a4a067c26f4743d08776aee","modified":1608025273511},{"_id":"public/image/kodokan/2/main/main_1.jpg","hash":"0df44174bd43df3bf3c531637c8120b858b69c8e","modified":1608025273512},{"_id":"public/image/kodokan/2/main/main_2.jpg","hash":"bd2e28d92dfe89f422d545e1437c001187953853","modified":1608025273512},{"_id":"public/image/kodokan/2/1-6/1_shop_2.jpg","hash":"80cc331b132b0d13edfe651ad21655e5ff07275f","modified":1608025273512},{"_id":"public/image/kodokan/2/1-6/4.jpg","hash":"e0c6a025136c11edb1d1529914d1aacca2335f75","modified":1608025273512},{"_id":"public/image/kodokan/2/1-6/2_library_4.jpg","hash":"f1f4a3616d81a9138a4316f4633e517a7625bf5d","modified":1608025273512},{"_id":"public/image/kodokan/2/gym/gym_1.jpg","hash":"819c49985da2cbfe89182909efb0dc1c387bd252","modified":1608025273512},{"_id":"public/fonts/foundation-icons/foundation-icons.css","hash":"2ad3bb2b4b7d1d67b4d40a9672193638c168c12a","modified":1608025273522},{"_id":"public/image/Carmen/Carmen_2.jpg","hash":"b6e5f16dc208bb3c4a16ba5a11c53dac2ae94914","modified":1608025273522},{"_id":"public/image/ML/3_1.jpg","hash":"ed22f29596dad580730d6560ffa116107043479a","modified":1608025273522},{"_id":"public/image/ML/2_3.jpg","hash":"781ea0a07c7de7599c866626a678106f3eb75154","modified":1608025273522},{"_id":"public/image/ML/2_4.jpg","hash":"89c17c9ba1509baebfc8d85aa5d8c5f4acb3f837","modified":1608025273522},{"_id":"public/image/ML/2_5.jpg","hash":"185c65d084c261455e2d3741c9a33d223bf494e9","modified":1608025273522},{"_id":"public/image/ML/3_2.jpg","hash":"e7ccb357e737e9069b98373db4c1d8186f9544b0","modified":1608025273523},{"_id":"public/image/ML/2_8.jpg","hash":"ddc8ae99e8baebd95aaf24ad1e7c309e9aaf2549","modified":1608025273523},{"_id":"public/image/ML/3_3.jpg","hash":"fa3a465c288dd5e3a28217fa9a665b1219c2816a","modified":1608025273523},{"_id":"public/image/ML/3_5.jpg","hash":"84a919974f1f792a66d21a0fc5d2ab4851e1e971","modified":1608025273523},{"_id":"public/image/ML/4_1.jpg","hash":"c09e18845df36a431465a4a5baec592649a1866d","modified":1608025273523},{"_id":"public/image/ML/5_2.jpg","hash":"96e9241e3b8bcac2815107a3866c779ce7f77ea0","modified":1608025273524},{"_id":"public/image/ML/2_2.jpg","hash":"cf04892f100d2c771908ac3cbc024613820d92f4","modified":1608025273524},{"_id":"public/image/ML/1_2.jpg","hash":"44f3dd898341c9901417dc6ad208314c8a705d0d","modified":1608025273524},{"_id":"public/image/sumiao4/MZ_2.jpg","hash":"ee991a70cbda9fdc5d262dc6c33e4c1566056c50","modified":1608025273524},{"_id":"public/image/sumiao4/MZ_1.jpg","hash":"6bc14bfc92354d0274ed247e380768887421928a","modified":1608025273524},{"_id":"public/image/kodokan/2/1-6/5_female_2.jpg","hash":"c7738c5fa6057f34543bd0abfb82d52e7c9f4f10","modified":1608025273524},{"_id":"public/image/kodokan/2/1-6/5_female_1.jpg","hash":"d1ceea501e876eb3211410097946776e02fcc87d","modified":1608025273525},{"_id":"public/image/kodokan/2/gym/gym_3.jpg","hash":"7a490501302678b1fe29b31907e36bc13d7b4f6f","modified":1608025273525},{"_id":"public/image/kodokan/2/gym/gym_2.jpg","hash":"d6cdff21644a6d67a1c8101e2f9345d540327647","modified":1608025273525},{"_id":"public/image/kodokan/2/main/kano3.jpg","hash":"3c092a41a8f7c23ed9511c89c8ec8981aaaf3d56","modified":1608025273525},{"_id":"public/image/kodokan/2/main/main_4.jpg","hash":"f1661c598d4448dae91c6ebe40a132c58671c1ef","modified":1608025273525},{"_id":"public/image/kodokan/3/info.jpg","hash":"af7358f8e3770de39cc15eff022d0c0ef7c47ad6","modified":1608025273525},{"_id":"public/image/kodokan/2/1-6/6_school_3.jpg","hash":"3938d565030c317e1e646b33c2b307d3782fd516","modified":1608025273526},{"_id":"public/image/kodokan/2/7-8/8_6.jpg","hash":"ee5924472e6b0b1fa486c6a0f2fd4125bf63180b","modified":1608025273526},{"_id":"public/image/ML/1_1.jpg","hash":"5112d4a5e006725a305f198383f924a53a7f9001","modified":1608025273534},{"_id":"public/image/ML/2_1.jpg","hash":"805c5fcebcd31fef759457d78dd0ec3dfd5165b9","modified":1608025273534},{"_id":"public/image/ML/4_2.jpg","hash":"00db019c83664c04509949c0a14bfdadb5bde4dc","modified":1608025273534},{"_id":"public/image/ML/2_6.jpg","hash":"9c16ae716064b5a569bebaee3dcdefdec04da5c4","modified":1608025273535},{"_id":"public/image/kodokan/2/newbuilding.jpg","hash":"2532dc69a5707232bd7f9bd30ff1527001412e2b","modified":1608025273535},{"_id":"public/image/sumiao4/MZ_5.jpg","hash":"e7530195ba3b7e2f51537d69a92eb2ab5e829d3c","modified":1608025273535},{"_id":"public/image/ML/5_1.jpg","hash":"6fdbb2c55b14687b79dddc10e94be80fe22d7822","modified":1608025273535},{"_id":"public/image/kodokan/4/eyecatch.jpg","hash":"9441af6d28ac0f236e69a62041fe91e7eb2991e4","modified":1608025273535},{"_id":"public/image/RL/5/1-3.png","hash":"d1bb2c439eeb28a7eb9e0f1afb85f2e8c7f835c9","modified":1608025273536},{"_id":"public/image/sumiao4/MZ_3.jpg","hash":"470fb107cd4c6a5d2d16fa7d2143b46cb23814bc","modified":1608025273536},{"_id":"public/image/DL/2/Mask-RCNN.png","hash":"daa6c11cfb42d38ace3b87056ab3f0ec43c3c904","modified":1608025273536},{"_id":"public/image/DL/5/1-4.png","hash":"32db8207ee8d107faf8dd31fdc5a23cdde0233d4","modified":1608025273536},{"_id":"public/image/RL/5/2-1.png","hash":"928d3fb916caaf166d3cfb4cc2bca71651dbf334","modified":1608025273537},{"_id":"public/image/kodokan/2/1-6/1_shop_1.jpg","hash":"3d458ac2ab4ea6673958562d4dc24e168d2f0a43","modified":1608025273537},{"_id":"public/image/kodokan/2/1-6/1_1.jpg","hash":"c9b54ac3a8d74ca70fe91abc9162080f39aa1871","modified":1608025273537},{"_id":"public/css/uno.css","hash":"e302f76eef45c8b42d780b82a8771fa46a1077d2","modified":1608025273549},{"_id":"public/image/sumiao4/MZ_back.jpg","hash":"76834c37d8b141c8d3be19afc0426b2953f4c20a","modified":1608025273549},{"_id":"public/image/Kendo/Kendo-2.jpg","hash":"f8281d16d80f527e89f3a1698033e1267b50bc5e","modified":1608025273549},{"_id":"public/image/DL/5/2-2.png","hash":"270e3c34dccde4362c772a51569b8cf0cda3932f","modified":1608025273549},{"_id":"public/image/RL/5/2-2.png","hash":"6001f2bd3f7ae5517be76f57782f602b89e97c3e","modified":1608025273550},{"_id":"public/image/RL/5/1-2.png","hash":"fd329541d5f14fb5f4516ed82416092cca394454","modified":1608025273550},{"_id":"public/image/kodokan/2/7-8/8_2.jpg","hash":"b74a4692730ea0bbdf9603aaa9a3d82b41b8068c","modified":1608025273550},{"_id":"public/image/RL/5/1-4.png","hash":"e076e1b4abb65dc1b3626e1c54fda7e358dd55ab","modified":1608025273562},{"_id":"public/image/kodokan/4/book.jpg","hash":"37f4f34e07ad1f181815c9bda3578f65fa468408","modified":1608025273562},{"_id":"public/image/ML/1_3.jpg","hash":"f8c55c5a7dc74c1e919293829c98767b68ffce51","modified":1608025273572},{"_id":"public/image/kodokan/2/1-6/1_2.jpg","hash":"03d1a52b4b515e2c699b120afe4a5a470fc477c9","modified":1608025273572},{"_id":"public/image/DL/2/4.gif","hash":"2e8faa1f16aafb0b54c13da4cc608af57aaa9672","modified":1608025273572},{"_id":"public/css/animate.css","hash":"a2f854fbdc52a5657dd5e2b95cc96c0da32f3ea2","modified":1608025273584},{"_id":"public/image/sumiao4/MZ_4.jpg","hash":"7e75e468142df159026bfe8f888ad45adc9325ea","modified":1608025273584},{"_id":"public/image/RL/5/0-0.png","hash":"24babb92689875e5be84e15fabad63bb6f40d984","modified":1608025273584},{"_id":"public/image/RL/5/1-1.png","hash":"9311e8d22c41b441441321de972bd50f06efed16","modified":1608025273584},{"_id":"public/image/kodokan/2/nanai.jpg","hash":"027e39982444d3f8f33cd26a04b3c7557905d5d6","modified":1608025273585},{"_id":"public/image/RL/5/3-1.png","hash":"2fea9b9ea5566fdfaa4c302d3b283dbaa078b213","modified":1608025273585},{"_id":"public/image/RL/5/3-2.png","hash":"668b5191318bd7b39dde1ec1667bb4be93f78c16","modified":1608025273586},{"_id":"public/image/PMP/PMP.png","hash":"0ea0404f9758c2687197ed06db64b799b713da69","modified":1608025273610},{"_id":"public/image/DL/5/1-3.png","hash":"d3c79cf82e8054361f7da5a63829ba67cc588b80","modified":1608025273611},{"_id":"public/js/jquery.min.js","hash":"06e872300088b9ba8a08427d28ed0efcdf9c6ff5","modified":1608025273619},{"_id":"public/images/background-cover.jpg","hash":"08162d5ddd6c081f2713bfd10261e62f0752a600","modified":1608025273623},{"_id":"public/image/kodokan/2/7-8/Summer.png","hash":"c66fdc3702a9d10cd31c835090a258509d71ccd3","modified":1608025273633}],"Category":[],"Data":[],"Page":[{"title":"About","date":"2016-12-11T03:59:30.000Z","_content":"\nWelcome to [H.J.T. Github](https://hjtso.github.io).\n\n### H.J.T. Github\n\n``` bash\nhttps://hjtso.github.io\n```\n\nMore info: [link](https://hjtso.github.io)\n\n### H.J.T. Home\n\n``` bash\nhttps://www.hjt.so\n```\n\nMore info: [link](https://www.hjt.so)\n","source":"About/index.md","raw":"---\ntitle: About\ndate: 2016-12-11 12:59:30\n---\n\nWelcome to [H.J.T. Github](https://hjtso.github.io).\n\n### H.J.T. Github\n\n``` bash\nhttps://hjtso.github.io\n```\n\nMore info: [link](https://hjtso.github.io)\n\n### H.J.T. Home\n\n``` bash\nhttps://www.hjt.so\n```\n\nMore info: [link](https://www.hjt.so)\n","updated":"2020-03-31T01:49:06.478Z","path":"About/index.html","comments":1,"layout":"page","_id":"ckipsq8wi0000og64b6trf4ui","content":"<p>Welcome to <a href=\"https://hjtso.github.io\" target=\"_blank\" rel=\"external\">H.J.T. Github</a>.</p>\n<h3 id=\"H-J-T-Github\"><a href=\"#H-J-T-Github\" class=\"headerlink\" title=\"H.J.T. Github\"></a>H.J.T. Github</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">https://hjtso.github.io</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hjtso.github.io\" target=\"_blank\" rel=\"external\">link</a></p>\n<h3 id=\"H-J-T-Home\"><a href=\"#H-J-T-Home\" class=\"headerlink\" title=\"H.J.T. Home\"></a>H.J.T. Home</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">https://www.hjt.so</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://www.hjt.so\" target=\"_blank\" rel=\"external\">link</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hjtso.github.io\">H.J.T. Github</a>.</p>\n<h3 id=\"H-J-T-Github\"><a href=\"#H-J-T-Github\" class=\"headerlink\" title=\"H.J.T. Github\"></a>H.J.T. Github</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">https://hjtso.github.io</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hjtso.github.io\">link</a></p>\n<h3 id=\"H-J-T-Home\"><a href=\"#H-J-T-Home\" class=\"headerlink\" title=\"H.J.T. Home\"></a>H.J.T. Home</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">https://www.hjt.so</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://www.hjt.so\">link</a></p>\n"},{"title":"Archive","date":"2016-12-11T03:48:14.000Z","layout":"page-archive","_content":"","source":"Archive/index.md","raw":"---\ntitle: Archive\ndate: 2016-12-11 12:48:14\nlayout: page-archive\n---\n","updated":"2016-12-11T13:26:36.000Z","path":"Archive/index.html","comments":1,"_id":"ckipsq8wk0001og64iakhw8pw","content":"","excerpt":"","more":""}],"Post":[{"title":"Carmen_Guitar","lang":"en","date":"2016-12-31T16:00:00.000Z","_content":"\n<center>![pic](/image/Carmen/Carmen_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/Carmen/Carmen_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/Carmen/Carmen_3.jpg)</center>   \n","source":"_posts/en/Carmen_Guitar.md","raw":"\n---\ntitle: Carmen_Guitar\nlang: en\ndate: 2017-01-01 01:00:00\n---\n\n<center>![pic](/image/Carmen/Carmen_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/Carmen/Carmen_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/Carmen/Carmen_3.jpg)</center>   \n","slug":"en-Carmen-Guitar","published":1,"updated":"2017-09-10T11:53:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8y70002og643839oz68","content":"<center><img src=\"/image/Carmen/Carmen_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/Carmen/Carmen_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/Carmen/Carmen_3.jpg\" alt=\"pic\"></center>   \n","excerpt":"","more":"<center><img src=\"/image/Carmen/Carmen_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/Carmen/Carmen_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/Carmen/Carmen_3.jpg\" alt=\"pic\"></center>   \n"},{"title":"Active And Passive Vocabulary","lang":"us","date":"2016-12-09T16:12:34.000Z","_content":"Communications between human are largely based upon language. Learn a new language involves many processes. One step is to learn and build a vocabulary. Vocabulary is comprised of the different words used by speakers to express themselves. Linguists recognize two different types of vocabulary: passive and active.\n\nThe term passive vocabulary refers to the words that one has learned or is familiar with. Words that comprise a person's passive vocabulary are used frequently for several reasons. First, their definitions are not always completely known. In other cases, speakers avoid certain words or terms that they do not need to use often in conversation. For most people, their passive vocabulary is much larger than their active vocabulary. This is because passive vocabulary continues to expand throughout a person's lifetime.\n\nActive vocabulary is the set of words that a person can produce, such as when speaking or writing. Many linguists agree that a person's active vocabulary is usually smaller than his or her passive vocabulary. This is because words are only added to the active vocabulary as they are needed. In other words, a person will not add a word to active vocabulary unless he or she needs it to communicate. ","source":"_posts/en/Active And Passive Vocabulary.md","raw":"---\ntitle: Active And Passive Vocabulary\nlang: us\ndate: 2016-12-10 01:12:34\ntags:\n---\nCommunications between human are largely based upon language. Learn a new language involves many processes. One step is to learn and build a vocabulary. Vocabulary is comprised of the different words used by speakers to express themselves. Linguists recognize two different types of vocabulary: passive and active.\n\nThe term passive vocabulary refers to the words that one has learned or is familiar with. Words that comprise a person's passive vocabulary are used frequently for several reasons. First, their definitions are not always completely known. In other cases, speakers avoid certain words or terms that they do not need to use often in conversation. For most people, their passive vocabulary is much larger than their active vocabulary. This is because passive vocabulary continues to expand throughout a person's lifetime.\n\nActive vocabulary is the set of words that a person can produce, such as when speaking or writing. Many linguists agree that a person's active vocabulary is usually smaller than his or her passive vocabulary. This is because words are only added to the active vocabulary as they are needed. In other words, a person will not add a word to active vocabulary unless he or she needs it to communicate. ","slug":"en-Active-And-Passive-Vocabulary","published":1,"updated":"2016-12-10T16:26:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8ya0003og64cnvn6j3i","content":"<p>Communications between human are largely based upon language. Learn a new language involves many processes. One step is to learn and build a vocabulary. Vocabulary is comprised of the different words used by speakers to express themselves. Linguists recognize two different types of vocabulary: passive and active.</p>\n<p>The term passive vocabulary refers to the words that one has learned or is familiar with. Words that comprise a person’s passive vocabulary are used frequently for several reasons. First, their definitions are not always completely known. In other cases, speakers avoid certain words or terms that they do not need to use often in conversation. For most people, their passive vocabulary is much larger than their active vocabulary. This is because passive vocabulary continues to expand throughout a person’s lifetime.</p>\n<p>Active vocabulary is the set of words that a person can produce, such as when speaking or writing. Many linguists agree that a person’s active vocabulary is usually smaller than his or her passive vocabulary. This is because words are only added to the active vocabulary as they are needed. In other words, a person will not add a word to active vocabulary unless he or she needs it to communicate. </p>\n","excerpt":"","more":"<p>Communications between human are largely based upon language. Learn a new language involves many processes. One step is to learn and build a vocabulary. Vocabulary is comprised of the different words used by speakers to express themselves. Linguists recognize two different types of vocabulary: passive and active.</p>\n<p>The term passive vocabulary refers to the words that one has learned or is familiar with. Words that comprise a person’s passive vocabulary are used frequently for several reasons. First, their definitions are not always completely known. In other cases, speakers avoid certain words or terms that they do not need to use often in conversation. For most people, their passive vocabulary is much larger than their active vocabulary. This is because passive vocabulary continues to expand throughout a person’s lifetime.</p>\n<p>Active vocabulary is the set of words that a person can produce, such as when speaking or writing. Many linguists agree that a person’s active vocabulary is usually smaller than his or her passive vocabulary. This is because words are only added to the active vocabulary as they are needed. In other words, a person will not add a word to active vocabulary unless he or she needs it to communicate. </p>\n"},{"title":"Hello World","lang":"us","date":"2016-11-27T16:12:34.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/en/Hello-World.md","raw":"---\ntitle: Hello World\nlang: us\ndate: 2016-11-28 01:12:34\ntags:\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"en-Hello-World","published":1,"updated":"2016-12-10T16:21:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yb0004og64w84vurwd","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"title":"My PMP Study Notes","lang":"en","date":"2020-04-01T02:18:52.000Z","_content":"\n<center>![PMP](/image/PMP/PMP.png)</center> \n\n----------------------------------------  \n\n#### Reference\n\n- [PMI Org](https://www.pmi.org/ \"Title\") \n- [Study Notes and Tips](https://edward-designer.com/web/pmp/ \"Title\")\n- [PMBOK Tutorial(BiliBili)](https://www.bilibili.com/video/BV1js411K7F6?p=1 \"Title\") \n- [PMP Exam Prep Seminar(Udemy)](https://www.udemy.com/pmp-pmbok6-35-pdus/learn/v4/overview \"Title\") ","source":"_posts/en/My PMP Study.md","raw":"---\ntitle: My PMP Study Notes\nlang: en\ndate: 2020-04-01 11:18:52\ntags: PM\n---\n\n<center>![PMP](/image/PMP/PMP.png)</center> \n\n----------------------------------------  \n\n#### Reference\n\n- [PMI Org](https://www.pmi.org/ \"Title\") \n- [Study Notes and Tips](https://edward-designer.com/web/pmp/ \"Title\")\n- [PMBOK Tutorial(BiliBili)](https://www.bilibili.com/video/BV1js411K7F6?p=1 \"Title\") \n- [PMP Exam Prep Seminar(Udemy)](https://www.udemy.com/pmp-pmbok6-35-pdus/learn/v4/overview \"Title\") ","slug":"en-My-PMP-Study","published":1,"updated":"2020-03-31T01:57:11.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yb0005og64sczpjjti","content":"<center><img src=\"/image/PMP/PMP.png\" alt=\"PMP\"></center> \n\n<hr>\n<h4 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h4><ul>\n<li><a href=\"https://www.pmi.org/\" title=\"Title\" target=\"_blank\" rel=\"external\">PMI Org</a> </li>\n<li><a href=\"https://edward-designer.com/web/pmp/\" title=\"Title\" target=\"_blank\" rel=\"external\">Study Notes and Tips</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1js411K7F6?p=1\" title=\"Title\" target=\"_blank\" rel=\"external\">PMBOK Tutorial(BiliBili)</a> </li>\n<li><a href=\"https://www.udemy.com/pmp-pmbok6-35-pdus/learn/v4/overview\" title=\"Title\" target=\"_blank\" rel=\"external\">PMP Exam Prep Seminar(Udemy)</a> </li>\n</ul>\n","excerpt":"","more":"<center><img src=\"/image/PMP/PMP.png\" alt=\"PMP\"></center> \n\n<hr>\n<h4 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h4><ul>\n<li><a href=\"https://www.pmi.org/\" title=\"Title\">PMI Org</a> </li>\n<li><a href=\"https://edward-designer.com/web/pmp/\" title=\"Title\">Study Notes and Tips</a></li>\n<li><a href=\"https://www.bilibili.com/video/BV1js411K7F6?p=1\" title=\"Title\">PMBOK Tutorial(BiliBili)</a> </li>\n<li><a href=\"https://www.udemy.com/pmp-pmbok6-35-pdus/learn/v4/overview\" title=\"Title\">PMP Exam Prep Seminar(Udemy)</a> </li>\n</ul>\n"},{"title":"EOSのDAPP開発流れ","lang":"ja","date":"2018-08-18T09:18:56.000Z","_content":"\n\n#### 1. コードを実行し効果を調べる\n\n● C++等言語を開発する\n\n#### 2. ツールを通してコンパイルする\n\n*「ツール」：cleos、keosd、nodeos、eosiocpp\n① コントラクトファイルを作成する\n● eosiocpp -n hello\n（パス：/opt/eosio/bin/data-dir/contracts）\n（チェック：docker exec -it docker_nodeosd_1 ls /hello）\n② ファイルをコンパイルする\n● eosio cpp -o hello.wast hello.cpp\n● eosio cpp -g hello.abi hello.cpp\n③ コントラクトをマイグレートする\n● cleos set contract eosio /hello/ /hello/hello.wast /hello/hello.abi -p eosio@active\n④ コントラクトを呼び出す\n● cleos push action hello hi '[\"user\"]' -p eosio@active \n● あるいは「eosjs」を通してコンパイルする（nodejsを使う）\n\n#### 3. 関連するフロントエンド画面を完成する\n\n● React.js/JavaScript/HTML/CSS\n\n#### 4. EOSのメインWebサイトにデプロイする\n\n● 「config.ini」ファイルを修正し、コントラクトをマイグレートする（RAMがかかる）\n\n-------------------------------------\n<center>![EOS_DAPP](/image/Blockchain/EOS/EOS_DAPP.jpg)</center> \n\n-------------------------------------\n##### Githubプロジェクト（Github）\n\n- [Github Link](https://github.com/HJTSO/Team_C \"Title\") \n\n##### 参考資料（Reference）\n\n- [EOS智能合约介绍](https://github.com/shanlusun/blockchain/tree/master/eos/05 \"Title\") \n- [EOS开发调试环境搭建](https://blog.csdn.net/caokun_8341/article/details/80713851 \"Title\") \n- [EOS源码学习系列](https://www.jianshu.com/p/24e1607ac7a2 \"Title\") ","source":"_posts/ja/EOSのDAPP開発流れ.md","raw":"\n---\ntitle: EOSのDAPP開発流れ\nlang: ja\ndate: 2018-08-18 18:18:56\ntags: Blockchain\n---\n\n\n#### 1. コードを実行し効果を調べる\n\n● C++等言語を開発する\n\n#### 2. ツールを通してコンパイルする\n\n*「ツール」：cleos、keosd、nodeos、eosiocpp\n① コントラクトファイルを作成する\n● eosiocpp -n hello\n（パス：/opt/eosio/bin/data-dir/contracts）\n（チェック：docker exec -it docker_nodeosd_1 ls /hello）\n② ファイルをコンパイルする\n● eosio cpp -o hello.wast hello.cpp\n● eosio cpp -g hello.abi hello.cpp\n③ コントラクトをマイグレートする\n● cleos set contract eosio /hello/ /hello/hello.wast /hello/hello.abi -p eosio@active\n④ コントラクトを呼び出す\n● cleos push action hello hi '[\"user\"]' -p eosio@active \n● あるいは「eosjs」を通してコンパイルする（nodejsを使う）\n\n#### 3. 関連するフロントエンド画面を完成する\n\n● React.js/JavaScript/HTML/CSS\n\n#### 4. EOSのメインWebサイトにデプロイする\n\n● 「config.ini」ファイルを修正し、コントラクトをマイグレートする（RAMがかかる）\n\n-------------------------------------\n<center>![EOS_DAPP](/image/Blockchain/EOS/EOS_DAPP.jpg)</center> \n\n-------------------------------------\n##### Githubプロジェクト（Github）\n\n- [Github Link](https://github.com/HJTSO/Team_C \"Title\") \n\n##### 参考資料（Reference）\n\n- [EOS智能合约介绍](https://github.com/shanlusun/blockchain/tree/master/eos/05 \"Title\") \n- [EOS开发调试环境搭建](https://blog.csdn.net/caokun_8341/article/details/80713851 \"Title\") \n- [EOS源码学习系列](https://www.jianshu.com/p/24e1607ac7a2 \"Title\") ","slug":"ja-EOSのDAPP開発流れ","published":1,"updated":"2018-08-18T13:10:01.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yc0006og644301qpmg","content":"<h4 id=\"1-コードを実行し効果を調べる\"><a href=\"#1-コードを実行し効果を調べる\" class=\"headerlink\" title=\"1. コードを実行し効果を調べる\"></a>1. コードを実行し効果を調べる</h4><p>● C++等言語を開発する</p>\n<h4 id=\"2-ツールを通してコンパイルする\"><a href=\"#2-ツールを通してコンパイルする\" class=\"headerlink\" title=\"2. ツールを通してコンパイルする\"></a>2. ツールを通してコンパイルする</h4><p>*「ツール」：cleos、keosd、nodeos、eosiocpp<br>① コントラクトファイルを作成する<br>● eosiocpp -n hello<br>（パス：/opt/eosio/bin/data-dir/contracts）<br>（チェック：docker exec -it docker_nodeosd_1 ls /hello）<br>② ファイルをコンパイルする<br>● eosio cpp -o hello.wast hello.cpp<br>● eosio cpp -g hello.abi hello.cpp<br>③ コントラクトをマイグレートする<br>● cleos set contract eosio /hello/ /hello/hello.wast /hello/hello.abi -p eosio@active<br>④ コントラクトを呼び出す<br>● cleos push action hello hi ‘[“user”]’ -p eosio@active<br>● あるいは「eosjs」を通してコンパイルする（nodejsを使う）</p>\n<h4 id=\"3-関連するフロントエンド画面を完成する\"><a href=\"#3-関連するフロントエンド画面を完成する\" class=\"headerlink\" title=\"3. 関連するフロントエンド画面を完成する\"></a>3. 関連するフロントエンド画面を完成する</h4><p>● React.js/JavaScript/HTML/CSS</p>\n<h4 id=\"4-EOSのメインWebサイトにデプロイする\"><a href=\"#4-EOSのメインWebサイトにデプロイする\" class=\"headerlink\" title=\"4. EOSのメインWebサイトにデプロイする\"></a>4. EOSのメインWebサイトにデプロイする</h4><p>● 「config.ini」ファイルを修正し、コントラクトをマイグレートする（RAMがかかる）</p>\n<hr>\n<center><img src=\"/image/Blockchain/EOS/EOS_DAPP.jpg\" alt=\"EOS_DAPP\"></center> \n\n<hr>\n<h5 id=\"Githubプロジェクト（Github）\"><a href=\"#Githubプロジェクト（Github）\" class=\"headerlink\" title=\"Githubプロジェクト（Github）\"></a>Githubプロジェクト（Github）</h5><ul>\n<li><a href=\"https://github.com/HJTSO/Team_C\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<h5 id=\"参考資料（Reference）\"><a href=\"#参考資料（Reference）\" class=\"headerlink\" title=\"参考資料（Reference）\"></a>参考資料（Reference）</h5><ul>\n<li><a href=\"https://github.com/shanlusun/blockchain/tree/master/eos/05\" title=\"Title\" target=\"_blank\" rel=\"external\">EOS智能合约介绍</a> </li>\n<li><a href=\"https://blog.csdn.net/caokun_8341/article/details/80713851\" title=\"Title\" target=\"_blank\" rel=\"external\">EOS开发调试环境搭建</a> </li>\n<li><a href=\"https://www.jianshu.com/p/24e1607ac7a2\" title=\"Title\" target=\"_blank\" rel=\"external\">EOS源码学习系列</a> </li>\n</ul>\n","excerpt":"","more":"<h4 id=\"1-コードを実行し効果を調べる\"><a href=\"#1-コードを実行し効果を調べる\" class=\"headerlink\" title=\"1. コードを実行し効果を調べる\"></a>1. コードを実行し効果を調べる</h4><p>● C++等言語を開発する</p>\n<h4 id=\"2-ツールを通してコンパイルする\"><a href=\"#2-ツールを通してコンパイルする\" class=\"headerlink\" title=\"2. ツールを通してコンパイルする\"></a>2. ツールを通してコンパイルする</h4><p>*「ツール」：cleos、keosd、nodeos、eosiocpp<br>① コントラクトファイルを作成する<br>● eosiocpp -n hello<br>（パス：/opt/eosio/bin/data-dir/contracts）<br>（チェック：docker exec -it docker_nodeosd_1 ls /hello）<br>② ファイルをコンパイルする<br>● eosio cpp -o hello.wast hello.cpp<br>● eosio cpp -g hello.abi hello.cpp<br>③ コントラクトをマイグレートする<br>● cleos set contract eosio /hello/ /hello/hello.wast /hello/hello.abi -p eosio@active<br>④ コントラクトを呼び出す<br>● cleos push action hello hi ‘[“user”]’ -p eosio@active<br>● あるいは「eosjs」を通してコンパイルする（nodejsを使う）</p>\n<h4 id=\"3-関連するフロントエンド画面を完成する\"><a href=\"#3-関連するフロントエンド画面を完成する\" class=\"headerlink\" title=\"3. 関連するフロントエンド画面を完成する\"></a>3. 関連するフロントエンド画面を完成する</h4><p>● React.js/JavaScript/HTML/CSS</p>\n<h4 id=\"4-EOSのメインWebサイトにデプロイする\"><a href=\"#4-EOSのメインWebサイトにデプロイする\" class=\"headerlink\" title=\"4. EOSのメインWebサイトにデプロイする\"></a>4. EOSのメインWebサイトにデプロイする</h4><p>● 「config.ini」ファイルを修正し、コントラクトをマイグレートする（RAMがかかる）</p>\n<hr>\n<center><img src=\"/image/Blockchain/EOS/EOS_DAPP.jpg\" alt=\"EOS_DAPP\"></center> \n\n<hr>\n<h5 id=\"Githubプロジェクト（Github）\"><a href=\"#Githubプロジェクト（Github）\" class=\"headerlink\" title=\"Githubプロジェクト（Github）\"></a>Githubプロジェクト（Github）</h5><ul>\n<li><a href=\"https://github.com/HJTSO/Team_C\" title=\"Title\">Github Link</a> </li>\n</ul>\n<h5 id=\"参考資料（Reference）\"><a href=\"#参考資料（Reference）\" class=\"headerlink\" title=\"参考資料（Reference）\"></a>参考資料（Reference）</h5><ul>\n<li><a href=\"https://github.com/shanlusun/blockchain/tree/master/eos/05\" title=\"Title\">EOS智能合约介绍</a> </li>\n<li><a href=\"https://blog.csdn.net/caokun_8341/article/details/80713851\" title=\"Title\">EOS开发调试环境搭建</a> </li>\n<li><a href=\"https://www.jianshu.com/p/24e1607ac7a2\" title=\"Title\">EOS源码学习系列</a> </li>\n</ul>\n"},{"title":"ETHのDAPP開発流れ","lang":"ja","date":"2018-07-07T09:18:56.000Z","_content":"\n#### 1. コードをRemixに実行し効果を調べる\n\n● Solidity或はSerpent、Vyper等言語を開発する\n\n#### 2. Truffleを通してコンパイルする\n\n● turffle compile\n● turffle migrate (ganache-cli)\n● turffle console (web3.jsとtruffle box)\n● turffle test (JavaScriptとSolidity)\n\n#### 3. 関連するフロントエンド画面を完成する\n\n● React.js/JavaScript/HTML/CSS\n\n#### 4. EthereumのメインWebサイトにデプロイする\n\n● Remix + MetaMask + MyEtherWallet (Ropsten Test Network)\n● Truffle + Infura (turffle migrate --reset --network ropsten)\n● Truffle + Ethereum Full-node (gethとparity)（GASがかかる）\n\n-------------------------------------\n<center>![ETH_DAPP](/image/Blockchain/ETH/ETH_DAPP.jpg)</center> \n\n-------------------------------------\n##### Githubプロジェクト（Github）\n\n- [Github Link](https://github.com/HJTSO/Team_C \"Title\") \n\n##### 参考資料（Reference）\n\n- [Solidity学習ヒント集](https://qiita.com/Yukiya025/items/b0b67c44b2878998015c \"Title\") \n- [これからEthereumでDApps開発する人にオススメサイト](https://qiita.com/yukatou/items/05652ba149266f81d4fe \"Title\") ","source":"_posts/ja/ETHのDAPP開発流れ.md","raw":"\n---\ntitle: ETHのDAPP開発流れ\nlang: ja\ndate: 2018-07-07 18:18:56\ntags: Blockchain\n---\n\n#### 1. コードをRemixに実行し効果を調べる\n\n● Solidity或はSerpent、Vyper等言語を開発する\n\n#### 2. Truffleを通してコンパイルする\n\n● turffle compile\n● turffle migrate (ganache-cli)\n● turffle console (web3.jsとtruffle box)\n● turffle test (JavaScriptとSolidity)\n\n#### 3. 関連するフロントエンド画面を完成する\n\n● React.js/JavaScript/HTML/CSS\n\n#### 4. EthereumのメインWebサイトにデプロイする\n\n● Remix + MetaMask + MyEtherWallet (Ropsten Test Network)\n● Truffle + Infura (turffle migrate --reset --network ropsten)\n● Truffle + Ethereum Full-node (gethとparity)（GASがかかる）\n\n-------------------------------------\n<center>![ETH_DAPP](/image/Blockchain/ETH/ETH_DAPP.jpg)</center> \n\n-------------------------------------\n##### Githubプロジェクト（Github）\n\n- [Github Link](https://github.com/HJTSO/Team_C \"Title\") \n\n##### 参考資料（Reference）\n\n- [Solidity学習ヒント集](https://qiita.com/Yukiya025/items/b0b67c44b2878998015c \"Title\") \n- [これからEthereumでDApps開発する人にオススメサイト](https://qiita.com/yukatou/items/05652ba149266f81d4fe \"Title\") ","slug":"ja-ETHのDAPP開発流れ","published":1,"updated":"2018-08-18T13:10:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8ye0008og64nqoh4sjz","content":"<h4 id=\"1-コードをRemixに実行し効果を調べる\"><a href=\"#1-コードをRemixに実行し効果を調べる\" class=\"headerlink\" title=\"1. コードをRemixに実行し効果を調べる\"></a>1. コードをRemixに実行し効果を調べる</h4><p>● Solidity或はSerpent、Vyper等言語を開発する</p>\n<h4 id=\"2-Truffleを通してコンパイルする\"><a href=\"#2-Truffleを通してコンパイルする\" class=\"headerlink\" title=\"2. Truffleを通してコンパイルする\"></a>2. Truffleを通してコンパイルする</h4><p>● turffle compile<br>● turffle migrate (ganache-cli)<br>● turffle console (web3.jsとtruffle box)<br>● turffle test (JavaScriptとSolidity)</p>\n<h4 id=\"3-関連するフロントエンド画面を完成する\"><a href=\"#3-関連するフロントエンド画面を完成する\" class=\"headerlink\" title=\"3. 関連するフロントエンド画面を完成する\"></a>3. 関連するフロントエンド画面を完成する</h4><p>● React.js/JavaScript/HTML/CSS</p>\n<h4 id=\"4-EthereumのメインWebサイトにデプロイする\"><a href=\"#4-EthereumのメインWebサイトにデプロイする\" class=\"headerlink\" title=\"4. EthereumのメインWebサイトにデプロイする\"></a>4. EthereumのメインWebサイトにデプロイする</h4><p>● Remix + MetaMask + MyEtherWallet (Ropsten Test Network)<br>● Truffle + Infura (turffle migrate –reset –network ropsten)<br>● Truffle + Ethereum Full-node (gethとparity)（GASがかかる）</p>\n<hr>\n<center><img src=\"/image/Blockchain/ETH/ETH_DAPP.jpg\" alt=\"ETH_DAPP\"></center> \n\n<hr>\n<h5 id=\"Githubプロジェクト（Github）\"><a href=\"#Githubプロジェクト（Github）\" class=\"headerlink\" title=\"Githubプロジェクト（Github）\"></a>Githubプロジェクト（Github）</h5><ul>\n<li><a href=\"https://github.com/HJTSO/Team_C\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<h5 id=\"参考資料（Reference）\"><a href=\"#参考資料（Reference）\" class=\"headerlink\" title=\"参考資料（Reference）\"></a>参考資料（Reference）</h5><ul>\n<li><a href=\"https://qiita.com/Yukiya025/items/b0b67c44b2878998015c\" title=\"Title\" target=\"_blank\" rel=\"external\">Solidity学習ヒント集</a> </li>\n<li><a href=\"https://qiita.com/yukatou/items/05652ba149266f81d4fe\" title=\"Title\" target=\"_blank\" rel=\"external\">これからEthereumでDApps開発する人にオススメサイト</a> </li>\n</ul>\n","excerpt":"","more":"<h4 id=\"1-コードをRemixに実行し効果を調べる\"><a href=\"#1-コードをRemixに実行し効果を調べる\" class=\"headerlink\" title=\"1. コードをRemixに実行し効果を調べる\"></a>1. コードをRemixに実行し効果を調べる</h4><p>● Solidity或はSerpent、Vyper等言語を開発する</p>\n<h4 id=\"2-Truffleを通してコンパイルする\"><a href=\"#2-Truffleを通してコンパイルする\" class=\"headerlink\" title=\"2. Truffleを通してコンパイルする\"></a>2. Truffleを通してコンパイルする</h4><p>● turffle compile<br>● turffle migrate (ganache-cli)<br>● turffle console (web3.jsとtruffle box)<br>● turffle test (JavaScriptとSolidity)</p>\n<h4 id=\"3-関連するフロントエンド画面を完成する\"><a href=\"#3-関連するフロントエンド画面を完成する\" class=\"headerlink\" title=\"3. 関連するフロントエンド画面を完成する\"></a>3. 関連するフロントエンド画面を完成する</h4><p>● React.js/JavaScript/HTML/CSS</p>\n<h4 id=\"4-EthereumのメインWebサイトにデプロイする\"><a href=\"#4-EthereumのメインWebサイトにデプロイする\" class=\"headerlink\" title=\"4. EthereumのメインWebサイトにデプロイする\"></a>4. EthereumのメインWebサイトにデプロイする</h4><p>● Remix + MetaMask + MyEtherWallet (Ropsten Test Network)<br>● Truffle + Infura (turffle migrate –reset –network ropsten)<br>● Truffle + Ethereum Full-node (gethとparity)（GASがかかる）</p>\n<hr>\n<center><img src=\"/image/Blockchain/ETH/ETH_DAPP.jpg\" alt=\"ETH_DAPP\"></center> \n\n<hr>\n<h5 id=\"Githubプロジェクト（Github）\"><a href=\"#Githubプロジェクト（Github）\" class=\"headerlink\" title=\"Githubプロジェクト（Github）\"></a>Githubプロジェクト（Github）</h5><ul>\n<li><a href=\"https://github.com/HJTSO/Team_C\" title=\"Title\">Github Link</a> </li>\n</ul>\n<h5 id=\"参考資料（Reference）\"><a href=\"#参考資料（Reference）\" class=\"headerlink\" title=\"参考資料（Reference）\"></a>参考資料（Reference）</h5><ul>\n<li><a href=\"https://qiita.com/Yukiya025/items/b0b67c44b2878998015c\" title=\"Title\">Solidity学習ヒント集</a> </li>\n<li><a href=\"https://qiita.com/yukatou/items/05652ba149266f81d4fe\" title=\"Title\">これからEthereumでDApps開発する人にオススメサイト</a> </li>\n</ul>\n"},{"title":"My Japanese Episode","lang":"ja","date":"2019-03-10T02:13:52.000Z","_content":"\n----------------------------------------  \n\n●　かわいそう\n日本に来たばかりの時、現場の同僚（山田さん）が自分のかわいい子供の写真を見せてくれた。\n「かわいらしい」という意味で話したかったのですが、僕は「かわいそう〜」と言ってしまいました。\n　山田さん：「かわいそう？」\n　僕：「そう、そう、かわいそう〜」\nその時なんだか雰囲気がちょっと気まずくなりました。\n側にいる何か分かった中国同僚は笑い出しました。\nその後、同僚が山田さんに私の言葉を直しくれました。山田さんも笑っていました。\n「おいしい」と「おいしそう」は意味が似ているけど、「かわいい」と「かわいそう」は意味が似ていないです。\n「かわいい」は「かわいらしい」という意味、「かわいそう」は「気の毒」という意味で使われているということがその時初めて分かりました。\n日本語を勉強している人なら、この間違えは知っていると思います。\nまた、「かわいい」という言葉についても、日本語初心者だったころ、「かわいいね」と言う時、全て「こわいね」と言っていました。\n\n----------------------------------------  \n\n●　お参りしに行く\n他にも敬語をよく間違えてしまうこともありました。\n友達のAさんが病気のため入院した時のことです。\n　周りの友達に「一緒にAさんをお参りしに行く？」と言ってしまいました。\n　周りの友達は：「Aさんはまだ死んでないよ。」と言いました。\n「入院中の人をお見舞いに行く」、「神社にはお参りしに行く」 という使い方があることが分かりました。\n\n----------------------------------------  \n\n人は間違いから学びます。間違いから成長すると思います。\n今ではこの失敗は私の宝物です。\nそして間違いを指摘してくれた皆様に感謝しています。","source":"_posts/ja/My Japanese Episode.md","raw":"---\ntitle: My Japanese Episode\nlang: ja\ndate: 2019-03-10 11:13:52\n---\n\n----------------------------------------  \n\n●　かわいそう\n日本に来たばかりの時、現場の同僚（山田さん）が自分のかわいい子供の写真を見せてくれた。\n「かわいらしい」という意味で話したかったのですが、僕は「かわいそう〜」と言ってしまいました。\n　山田さん：「かわいそう？」\n　僕：「そう、そう、かわいそう〜」\nその時なんだか雰囲気がちょっと気まずくなりました。\n側にいる何か分かった中国同僚は笑い出しました。\nその後、同僚が山田さんに私の言葉を直しくれました。山田さんも笑っていました。\n「おいしい」と「おいしそう」は意味が似ているけど、「かわいい」と「かわいそう」は意味が似ていないです。\n「かわいい」は「かわいらしい」という意味、「かわいそう」は「気の毒」という意味で使われているということがその時初めて分かりました。\n日本語を勉強している人なら、この間違えは知っていると思います。\nまた、「かわいい」という言葉についても、日本語初心者だったころ、「かわいいね」と言う時、全て「こわいね」と言っていました。\n\n----------------------------------------  \n\n●　お参りしに行く\n他にも敬語をよく間違えてしまうこともありました。\n友達のAさんが病気のため入院した時のことです。\n　周りの友達に「一緒にAさんをお参りしに行く？」と言ってしまいました。\n　周りの友達は：「Aさんはまだ死んでないよ。」と言いました。\n「入院中の人をお見舞いに行く」、「神社にはお参りしに行く」 という使い方があることが分かりました。\n\n----------------------------------------  \n\n人は間違いから学びます。間違いから成長すると思います。\n今ではこの失敗は私の宝物です。\nそして間違いを指摘してくれた皆様に感謝しています。","slug":"ja-My-Japanese-Episode","published":1,"updated":"2019-03-12T11:30:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yf0009og64bxof5har","content":"<hr>\n<p>●　かわいそう<br>日本に来たばかりの時、現場の同僚（山田さん）が自分のかわいい子供の写真を見せてくれた。<br>「かわいらしい」という意味で話したかったのですが、僕は「かわいそう〜」と言ってしまいました。<br>　山田さん：「かわいそう？」<br>　僕：「そう、そう、かわいそう〜」<br>その時なんだか雰囲気がちょっと気まずくなりました。<br>側にいる何か分かった中国同僚は笑い出しました。<br>その後、同僚が山田さんに私の言葉を直しくれました。山田さんも笑っていました。<br>「おいしい」と「おいしそう」は意味が似ているけど、「かわいい」と「かわいそう」は意味が似ていないです。<br>「かわいい」は「かわいらしい」という意味、「かわいそう」は「気の毒」という意味で使われているということがその時初めて分かりました。<br>日本語を勉強している人なら、この間違えは知っていると思います。<br>また、「かわいい」という言葉についても、日本語初心者だったころ、「かわいいね」と言う時、全て「こわいね」と言っていました。</p>\n<hr>\n<p>●　お参りしに行く<br>他にも敬語をよく間違えてしまうこともありました。<br>友達のAさんが病気のため入院した時のことです。<br>　周りの友達に「一緒にAさんをお参りしに行く？」と言ってしまいました。<br>　周りの友達は：「Aさんはまだ死んでないよ。」と言いました。<br>「入院中の人をお見舞いに行く」、「神社にはお参りしに行く」 という使い方があることが分かりました。</p>\n<hr>\n<p>人は間違いから学びます。間違いから成長すると思います。<br>今ではこの失敗は私の宝物です。<br>そして間違いを指摘してくれた皆様に感謝しています。</p>\n","excerpt":"","more":"<hr>\n<p>●　かわいそう<br>日本に来たばかりの時、現場の同僚（山田さん）が自分のかわいい子供の写真を見せてくれた。<br>「かわいらしい」という意味で話したかったのですが、僕は「かわいそう〜」と言ってしまいました。<br>　山田さん：「かわいそう？」<br>　僕：「そう、そう、かわいそう〜」<br>その時なんだか雰囲気がちょっと気まずくなりました。<br>側にいる何か分かった中国同僚は笑い出しました。<br>その後、同僚が山田さんに私の言葉を直しくれました。山田さんも笑っていました。<br>「おいしい」と「おいしそう」は意味が似ているけど、「かわいい」と「かわいそう」は意味が似ていないです。<br>「かわいい」は「かわいらしい」という意味、「かわいそう」は「気の毒」という意味で使われているということがその時初めて分かりました。<br>日本語を勉強している人なら、この間違えは知っていると思います。<br>また、「かわいい」という言葉についても、日本語初心者だったころ、「かわいいね」と言う時、全て「こわいね」と言っていました。</p>\n<hr>\n<p>●　お参りしに行く<br>他にも敬語をよく間違えてしまうこともありました。<br>友達のAさんが病気のため入院した時のことです。<br>　周りの友達に「一緒にAさんをお参りしに行く？」と言ってしまいました。<br>　周りの友達は：「Aさんはまだ死んでないよ。」と言いました。<br>「入院中の人をお見舞いに行く」、「神社にはお参りしに行く」 という使い方があることが分かりました。</p>\n<hr>\n<p>人は間違いから学びます。間違いから成長すると思います。<br>今ではこの失敗は私の宝物です。<br>そして間違いを指摘してくれた皆様に感謝しています。</p>\n"},{"title":"剣道用語-Kendo terminology","lang":"ja","date":"2020-12-14T11:20:30.000Z","_content":"\n--------------------------------- \n剣道の技には、ただ単に、「面、小手、突き、胴」を打っていくだけではありません。\n相手がいて、相手の動きや、相手の状態に合わせて、さまざまな種類の技を仕掛けていくことになります。\n基本的には、大きく「仕掛け技」と「応じ技」の二つにわけられます。剣道の技の種類をメモします。\n\n--------------------------------- \n# 一 仕掛け技\n仕掛け技とは、相手が動く前に自分が仕掛けることで隙を作る技のことです。\n\n--------------------------------- \n\n<center>![W](/image/Kendo/Kendo-1.jpg)</center>\n\n--------------------------------- \n##  1.連続技\n一本目の打突で技が決まらなかったときに、始めの一本で相手が崩れ隙ができたところを、二本目、三本目の打突を決めていく技です。\n（二三段の技：二撃目、三撃目で有効打を決めるバリエーションのことです）\n##  2.出ばな技\n出ばな技とは、相手が打とうとして「動いた瞬間に」相手よりも早く打つ技です。相手が動いた瞬間の隙を突くことが重要な技です。\n##  3.払い技\n払い技とは、相手の竹刀を払うことで隙を作る技のことです。\n払い面、払い胴、払い小手、払い突きがあり、いずれも相手の竹刀を払い上げる形になります。技が決まれば大きく相手を崩すことができます。\n##  4.担ぎ技\n担ぎ技とは、大きく竹刀を担ぐ様に振り上げる技です。その瞬間の相手の反応により、空いた部位を狙って打ちます。相手の竹刀に触れることなく、相手の構えを崩すことができます。\n##  5.引き技\n引き技とは、鍔迫り合い状態から一瞬離れる(引く)タイミングを打つ技です。多くは名前の通り、体を引きながら打ち込みます。\nしかし、うまく決まらなかった場合は相手の反撃を受けやすいので、引き技を放った際にはすぐに間合いを取ることも重要です。\n##  6.片手技\n片手技とは、ほとんどの場合では片手で行い、「右面」、「左面」や「突き」などを狙います。\n上段や片手上段、または二刀流、二刀の構えなどは最初から片手打ちをすることが前提ですが、諸手の中段構えからでも片手の技を使うことができるのです。\n\n--------------------------------- \n# 二 応じ技\n応じ技とは、相手の打ち込んでくる力を利用して反撃する技の総称です。仕掛け技と違い、相手の攻撃に対して動く、カウンターの様な技です。\n\n--------------------------------- \n\n<center>![W](/image/Kendo/Kendo-2.jpg)</center>\n\n--------------------------------- \n## 1.抜き技\n抜き技とは、相手が面や小手を打ってきた際に使う技です。相手が打ってきたところで体ごと避け、相手の竹刀を空振りさせます。空振りの際にできた隙を突き、打ち込みます。しかし、一歩間違えると相手の有効打を決められてしまう恐　れもあります。\n## 2.返し技\n返し技とは、相手の竹刀を受けた瞬間に、攻撃に転じる技です。受ける動きと返す動きを一息で行うため、体さばきや足捌きのスピードも重要となります。また、相手の動きに反応する必要があるので、集中力もなくてはなりません。\n## 3.すりあげ技\nすりあげ技とは、相手が打ち下ろして竹刀を自分の竹刀の「鎬」を使って擦り上げ、相手の攻撃線を右か左にそらす技です。相手の打ち込みの力が強ければ強いほど、大きく逸らすことができます。\n攻撃線を大きく逸らされ　た相手は、体勢を崩し、すぐに立て直すことはできません。その隙を打ち込むことで、有効打を決めることができます。\n## 4.打ち落とし技\n相手が打ってくる竹刀を斜め下に打ち落としながら、隙を狙って打っていく技です。\n打ち落とした瞬間に打っていかないと、相手と間合いが詰まって有効打突となりませんので、力を抜いて手首の力で打ち落とすことで、間合いが詰まる前に打ち返すことができます。\nまた、こちらの姿勢が崩れていると、これも打ち返すのが遅くなる原因となりますので、姿勢を正したまま体をさばきながら打ち落とすことが重要です。\n\n----------------------------------------  \n\n#### Reference\n\n- [初心者必見！簡単にわかる剣道の技12選](https://sposhiru.com/86012#-6 \"Title\") \n- [剣道の技について](https://www.budosuki.com/category23/entry101.html \"Title\") \n- [剣道の技種類一覧](https://budo-kendo.com/kendo-waza-list-general \"Title\") ","source":"_posts/ja/剣道用語-Kendo terminology.md","raw":"---\ntitle: 剣道用語-Kendo terminology\nlang: ja\ndate: 2020-12-14 20:20:30\ntags: Kendo\n---\n\n--------------------------------- \n剣道の技には、ただ単に、「面、小手、突き、胴」を打っていくだけではありません。\n相手がいて、相手の動きや、相手の状態に合わせて、さまざまな種類の技を仕掛けていくことになります。\n基本的には、大きく「仕掛け技」と「応じ技」の二つにわけられます。剣道の技の種類をメモします。\n\n--------------------------------- \n# 一 仕掛け技\n仕掛け技とは、相手が動く前に自分が仕掛けることで隙を作る技のことです。\n\n--------------------------------- \n\n<center>![W](/image/Kendo/Kendo-1.jpg)</center>\n\n--------------------------------- \n##  1.連続技\n一本目の打突で技が決まらなかったときに、始めの一本で相手が崩れ隙ができたところを、二本目、三本目の打突を決めていく技です。\n（二三段の技：二撃目、三撃目で有効打を決めるバリエーションのことです）\n##  2.出ばな技\n出ばな技とは、相手が打とうとして「動いた瞬間に」相手よりも早く打つ技です。相手が動いた瞬間の隙を突くことが重要な技です。\n##  3.払い技\n払い技とは、相手の竹刀を払うことで隙を作る技のことです。\n払い面、払い胴、払い小手、払い突きがあり、いずれも相手の竹刀を払い上げる形になります。技が決まれば大きく相手を崩すことができます。\n##  4.担ぎ技\n担ぎ技とは、大きく竹刀を担ぐ様に振り上げる技です。その瞬間の相手の反応により、空いた部位を狙って打ちます。相手の竹刀に触れることなく、相手の構えを崩すことができます。\n##  5.引き技\n引き技とは、鍔迫り合い状態から一瞬離れる(引く)タイミングを打つ技です。多くは名前の通り、体を引きながら打ち込みます。\nしかし、うまく決まらなかった場合は相手の反撃を受けやすいので、引き技を放った際にはすぐに間合いを取ることも重要です。\n##  6.片手技\n片手技とは、ほとんどの場合では片手で行い、「右面」、「左面」や「突き」などを狙います。\n上段や片手上段、または二刀流、二刀の構えなどは最初から片手打ちをすることが前提ですが、諸手の中段構えからでも片手の技を使うことができるのです。\n\n--------------------------------- \n# 二 応じ技\n応じ技とは、相手の打ち込んでくる力を利用して反撃する技の総称です。仕掛け技と違い、相手の攻撃に対して動く、カウンターの様な技です。\n\n--------------------------------- \n\n<center>![W](/image/Kendo/Kendo-2.jpg)</center>\n\n--------------------------------- \n## 1.抜き技\n抜き技とは、相手が面や小手を打ってきた際に使う技です。相手が打ってきたところで体ごと避け、相手の竹刀を空振りさせます。空振りの際にできた隙を突き、打ち込みます。しかし、一歩間違えると相手の有効打を決められてしまう恐　れもあります。\n## 2.返し技\n返し技とは、相手の竹刀を受けた瞬間に、攻撃に転じる技です。受ける動きと返す動きを一息で行うため、体さばきや足捌きのスピードも重要となります。また、相手の動きに反応する必要があるので、集中力もなくてはなりません。\n## 3.すりあげ技\nすりあげ技とは、相手が打ち下ろして竹刀を自分の竹刀の「鎬」を使って擦り上げ、相手の攻撃線を右か左にそらす技です。相手の打ち込みの力が強ければ強いほど、大きく逸らすことができます。\n攻撃線を大きく逸らされ　た相手は、体勢を崩し、すぐに立て直すことはできません。その隙を打ち込むことで、有効打を決めることができます。\n## 4.打ち落とし技\n相手が打ってくる竹刀を斜め下に打ち落としながら、隙を狙って打っていく技です。\n打ち落とした瞬間に打っていかないと、相手と間合いが詰まって有効打突となりませんので、力を抜いて手首の力で打ち落とすことで、間合いが詰まる前に打ち返すことができます。\nまた、こちらの姿勢が崩れていると、これも打ち返すのが遅くなる原因となりますので、姿勢を正したまま体をさばきながら打ち落とすことが重要です。\n\n----------------------------------------  \n\n#### Reference\n\n- [初心者必見！簡単にわかる剣道の技12選](https://sposhiru.com/86012#-6 \"Title\") \n- [剣道の技について](https://www.budosuki.com/category23/entry101.html \"Title\") \n- [剣道の技種類一覧](https://budo-kendo.com/kendo-waza-list-general \"Title\") ","slug":"ja-剣道用語-Kendo-terminology","published":1,"updated":"2020-12-15T09:40:52.839Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yf000aog64qpy67cu0","content":"<hr>\n<p>剣道の技には、ただ単に、「面、小手、突き、胴」を打っていくだけではありません。<br>相手がいて、相手の動きや、相手の状態に合わせて、さまざまな種類の技を仕掛けていくことになります。<br>基本的には、大きく「仕掛け技」と「応じ技」の二つにわけられます。剣道の技の種類をメモします。</p>\n<hr>\n<h1 id=\"一-仕掛け技\"><a href=\"#一-仕掛け技\" class=\"headerlink\" title=\"一 仕掛け技\"></a>一 仕掛け技</h1><p>仕掛け技とは、相手が動く前に自分が仕掛けることで隙を作る技のことです。</p>\n<hr>\n<center><img src=\"/image/Kendo/Kendo-1.jpg\" alt=\"W\"></center>\n\n<hr>\n<h2 id=\"1-連続技\"><a href=\"#1-連続技\" class=\"headerlink\" title=\"1.連続技\"></a>1.連続技</h2><p>一本目の打突で技が決まらなかったときに、始めの一本で相手が崩れ隙ができたところを、二本目、三本目の打突を決めていく技です。<br>（二三段の技：二撃目、三撃目で有効打を決めるバリエーションのことです）</p>\n<h2 id=\"2-出ばな技\"><a href=\"#2-出ばな技\" class=\"headerlink\" title=\"2.出ばな技\"></a>2.出ばな技</h2><p>出ばな技とは、相手が打とうとして「動いた瞬間に」相手よりも早く打つ技です。相手が動いた瞬間の隙を突くことが重要な技です。</p>\n<h2 id=\"3-払い技\"><a href=\"#3-払い技\" class=\"headerlink\" title=\"3.払い技\"></a>3.払い技</h2><p>払い技とは、相手の竹刀を払うことで隙を作る技のことです。<br>払い面、払い胴、払い小手、払い突きがあり、いずれも相手の竹刀を払い上げる形になります。技が決まれば大きく相手を崩すことができます。</p>\n<h2 id=\"4-担ぎ技\"><a href=\"#4-担ぎ技\" class=\"headerlink\" title=\"4.担ぎ技\"></a>4.担ぎ技</h2><p>担ぎ技とは、大きく竹刀を担ぐ様に振り上げる技です。その瞬間の相手の反応により、空いた部位を狙って打ちます。相手の竹刀に触れることなく、相手の構えを崩すことができます。</p>\n<h2 id=\"5-引き技\"><a href=\"#5-引き技\" class=\"headerlink\" title=\"5.引き技\"></a>5.引き技</h2><p>引き技とは、鍔迫り合い状態から一瞬離れる(引く)タイミングを打つ技です。多くは名前の通り、体を引きながら打ち込みます。<br>しかし、うまく決まらなかった場合は相手の反撃を受けやすいので、引き技を放った際にはすぐに間合いを取ることも重要です。</p>\n<h2 id=\"6-片手技\"><a href=\"#6-片手技\" class=\"headerlink\" title=\"6.片手技\"></a>6.片手技</h2><p>片手技とは、ほとんどの場合では片手で行い、「右面」、「左面」や「突き」などを狙います。<br>上段や片手上段、または二刀流、二刀の構えなどは最初から片手打ちをすることが前提ですが、諸手の中段構えからでも片手の技を使うことができるのです。</p>\n<hr>\n<h1 id=\"二-応じ技\"><a href=\"#二-応じ技\" class=\"headerlink\" title=\"二 応じ技\"></a>二 応じ技</h1><p>応じ技とは、相手の打ち込んでくる力を利用して反撃する技の総称です。仕掛け技と違い、相手の攻撃に対して動く、カウンターの様な技です。</p>\n<hr>\n<center><img src=\"/image/Kendo/Kendo-2.jpg\" alt=\"W\"></center>\n\n<hr>\n<h2 id=\"1-抜き技\"><a href=\"#1-抜き技\" class=\"headerlink\" title=\"1.抜き技\"></a>1.抜き技</h2><p>抜き技とは、相手が面や小手を打ってきた際に使う技です。相手が打ってきたところで体ごと避け、相手の竹刀を空振りさせます。空振りの際にできた隙を突き、打ち込みます。しかし、一歩間違えると相手の有効打を決められてしまう恐　れもあります。</p>\n<h2 id=\"2-返し技\"><a href=\"#2-返し技\" class=\"headerlink\" title=\"2.返し技\"></a>2.返し技</h2><p>返し技とは、相手の竹刀を受けた瞬間に、攻撃に転じる技です。受ける動きと返す動きを一息で行うため、体さばきや足捌きのスピードも重要となります。また、相手の動きに反応する必要があるので、集中力もなくてはなりません。</p>\n<h2 id=\"3-すりあげ技\"><a href=\"#3-すりあげ技\" class=\"headerlink\" title=\"3.すりあげ技\"></a>3.すりあげ技</h2><p>すりあげ技とは、相手が打ち下ろして竹刀を自分の竹刀の「鎬」を使って擦り上げ、相手の攻撃線を右か左にそらす技です。相手の打ち込みの力が強ければ強いほど、大きく逸らすことができます。<br>攻撃線を大きく逸らされ　た相手は、体勢を崩し、すぐに立て直すことはできません。その隙を打ち込むことで、有効打を決めることができます。</p>\n<h2 id=\"4-打ち落とし技\"><a href=\"#4-打ち落とし技\" class=\"headerlink\" title=\"4.打ち落とし技\"></a>4.打ち落とし技</h2><p>相手が打ってくる竹刀を斜め下に打ち落としながら、隙を狙って打っていく技です。<br>打ち落とした瞬間に打っていかないと、相手と間合いが詰まって有効打突となりませんので、力を抜いて手首の力で打ち落とすことで、間合いが詰まる前に打ち返すことができます。<br>また、こちらの姿勢が崩れていると、これも打ち返すのが遅くなる原因となりますので、姿勢を正したまま体をさばきながら打ち落とすことが重要です。</p>\n<hr>\n<h4 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h4><ul>\n<li><a href=\"https://sposhiru.com/86012#-6\" title=\"Title\" target=\"_blank\" rel=\"external\">初心者必見！簡単にわかる剣道の技12選</a> </li>\n<li><a href=\"https://www.budosuki.com/category23/entry101.html\" title=\"Title\" target=\"_blank\" rel=\"external\">剣道の技について</a> </li>\n<li><a href=\"https://budo-kendo.com/kendo-waza-list-general\" title=\"Title\" target=\"_blank\" rel=\"external\">剣道の技種類一覧</a> </li>\n</ul>\n","excerpt":"","more":"<hr>\n<p>剣道の技には、ただ単に、「面、小手、突き、胴」を打っていくだけではありません。<br>相手がいて、相手の動きや、相手の状態に合わせて、さまざまな種類の技を仕掛けていくことになります。<br>基本的には、大きく「仕掛け技」と「応じ技」の二つにわけられます。剣道の技の種類をメモします。</p>\n<hr>\n<h1 id=\"一-仕掛け技\"><a href=\"#一-仕掛け技\" class=\"headerlink\" title=\"一 仕掛け技\"></a>一 仕掛け技</h1><p>仕掛け技とは、相手が動く前に自分が仕掛けることで隙を作る技のことです。</p>\n<hr>\n<center><img src=\"/image/Kendo/Kendo-1.jpg\" alt=\"W\"></center>\n\n<hr>\n<h2 id=\"1-連続技\"><a href=\"#1-連続技\" class=\"headerlink\" title=\"1.連続技\"></a>1.連続技</h2><p>一本目の打突で技が決まらなかったときに、始めの一本で相手が崩れ隙ができたところを、二本目、三本目の打突を決めていく技です。<br>（二三段の技：二撃目、三撃目で有効打を決めるバリエーションのことです）</p>\n<h2 id=\"2-出ばな技\"><a href=\"#2-出ばな技\" class=\"headerlink\" title=\"2.出ばな技\"></a>2.出ばな技</h2><p>出ばな技とは、相手が打とうとして「動いた瞬間に」相手よりも早く打つ技です。相手が動いた瞬間の隙を突くことが重要な技です。</p>\n<h2 id=\"3-払い技\"><a href=\"#3-払い技\" class=\"headerlink\" title=\"3.払い技\"></a>3.払い技</h2><p>払い技とは、相手の竹刀を払うことで隙を作る技のことです。<br>払い面、払い胴、払い小手、払い突きがあり、いずれも相手の竹刀を払い上げる形になります。技が決まれば大きく相手を崩すことができます。</p>\n<h2 id=\"4-担ぎ技\"><a href=\"#4-担ぎ技\" class=\"headerlink\" title=\"4.担ぎ技\"></a>4.担ぎ技</h2><p>担ぎ技とは、大きく竹刀を担ぐ様に振り上げる技です。その瞬間の相手の反応により、空いた部位を狙って打ちます。相手の竹刀に触れることなく、相手の構えを崩すことができます。</p>\n<h2 id=\"5-引き技\"><a href=\"#5-引き技\" class=\"headerlink\" title=\"5.引き技\"></a>5.引き技</h2><p>引き技とは、鍔迫り合い状態から一瞬離れる(引く)タイミングを打つ技です。多くは名前の通り、体を引きながら打ち込みます。<br>しかし、うまく決まらなかった場合は相手の反撃を受けやすいので、引き技を放った際にはすぐに間合いを取ることも重要です。</p>\n<h2 id=\"6-片手技\"><a href=\"#6-片手技\" class=\"headerlink\" title=\"6.片手技\"></a>6.片手技</h2><p>片手技とは、ほとんどの場合では片手で行い、「右面」、「左面」や「突き」などを狙います。<br>上段や片手上段、または二刀流、二刀の構えなどは最初から片手打ちをすることが前提ですが、諸手の中段構えからでも片手の技を使うことができるのです。</p>\n<hr>\n<h1 id=\"二-応じ技\"><a href=\"#二-応じ技\" class=\"headerlink\" title=\"二 応じ技\"></a>二 応じ技</h1><p>応じ技とは、相手の打ち込んでくる力を利用して反撃する技の総称です。仕掛け技と違い、相手の攻撃に対して動く、カウンターの様な技です。</p>\n<hr>\n<center><img src=\"/image/Kendo/Kendo-2.jpg\" alt=\"W\"></center>\n\n<hr>\n<h2 id=\"1-抜き技\"><a href=\"#1-抜き技\" class=\"headerlink\" title=\"1.抜き技\"></a>1.抜き技</h2><p>抜き技とは、相手が面や小手を打ってきた際に使う技です。相手が打ってきたところで体ごと避け、相手の竹刀を空振りさせます。空振りの際にできた隙を突き、打ち込みます。しかし、一歩間違えると相手の有効打を決められてしまう恐　れもあります。</p>\n<h2 id=\"2-返し技\"><a href=\"#2-返し技\" class=\"headerlink\" title=\"2.返し技\"></a>2.返し技</h2><p>返し技とは、相手の竹刀を受けた瞬間に、攻撃に転じる技です。受ける動きと返す動きを一息で行うため、体さばきや足捌きのスピードも重要となります。また、相手の動きに反応する必要があるので、集中力もなくてはなりません。</p>\n<h2 id=\"3-すりあげ技\"><a href=\"#3-すりあげ技\" class=\"headerlink\" title=\"3.すりあげ技\"></a>3.すりあげ技</h2><p>すりあげ技とは、相手が打ち下ろして竹刀を自分の竹刀の「鎬」を使って擦り上げ、相手の攻撃線を右か左にそらす技です。相手の打ち込みの力が強ければ強いほど、大きく逸らすことができます。<br>攻撃線を大きく逸らされ　た相手は、体勢を崩し、すぐに立て直すことはできません。その隙を打ち込むことで、有効打を決めることができます。</p>\n<h2 id=\"4-打ち落とし技\"><a href=\"#4-打ち落とし技\" class=\"headerlink\" title=\"4.打ち落とし技\"></a>4.打ち落とし技</h2><p>相手が打ってくる竹刀を斜め下に打ち落としながら、隙を狙って打っていく技です。<br>打ち落とした瞬間に打っていかないと、相手と間合いが詰まって有効打突となりませんので、力を抜いて手首の力で打ち落とすことで、間合いが詰まる前に打ち返すことができます。<br>また、こちらの姿勢が崩れていると、これも打ち返すのが遅くなる原因となりますので、姿勢を正したまま体をさばきながら打ち落とすことが重要です。</p>\n<hr>\n<h4 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h4><ul>\n<li><a href=\"https://sposhiru.com/86012#-6\" title=\"Title\">初心者必見！簡単にわかる剣道の技12選</a> </li>\n<li><a href=\"https://www.budosuki.com/category23/entry101.html\" title=\"Title\">剣道の技について</a> </li>\n<li><a href=\"https://budo-kendo.com/kendo-waza-list-general\" title=\"Title\">剣道の技種類一覧</a> </li>\n</ul>\n"},{"title":"富士登山","lang":"ja","date":"2016-11-29T16:13:52.000Z","_content":"\n<center>![Fuji](/image/Fuji/Fuji_1.jpg)</center>  \n  \n----------------------------------------  \n\n&#8195;&#8195;2016年8月に友達と一緒に富士山に登りました。通常、富士山は毎年の7、8月だけ開放されます。  \n&#8195;&#8195;2016年せっかくのチャンスを見逃したくないので、友達と一緒に五合目に行きました。私たちは夜8時に、わくわくしながら登り始めました。友人とチャットしながら登りました。途中、たくさんの国からの登山者がたくさんいまして、国籍のいかんによらず、皆は同じ目標に憧れていました。高く登れば登るほど寒くなっていきましたけど、準備不足で薄着だったせいで、寒いでした。  \n \n<center>![Fuji](/image/Fuji/Fuji_2.jpg)</center>  \n  \n----------------------------------------  \n\n&#8195;&#8195;2日目の朝3時に、私たちは九合目に着きました。その後、皆は富士山頂で日の出を待っていました。4時、太陽が昇って来たところの写真を撮りました。富士山頂の日の出はとても綺麗でした！「苦しい奮闘をめきにして、この美しい景色を見ることはできなかっただろう。」って感じました。その後、下山には6時間かかりました。疲れましたけど、面白かったです。 ","source":"_posts/ja/富士登山.md","raw":"---\ntitle: 富士登山\nlang: ja\ndate: 2016-11-30 01:13:52\n---\n\n<center>![Fuji](/image/Fuji/Fuji_1.jpg)</center>  \n  \n----------------------------------------  \n\n&#8195;&#8195;2016年8月に友達と一緒に富士山に登りました。通常、富士山は毎年の7、8月だけ開放されます。  \n&#8195;&#8195;2016年せっかくのチャンスを見逃したくないので、友達と一緒に五合目に行きました。私たちは夜8時に、わくわくしながら登り始めました。友人とチャットしながら登りました。途中、たくさんの国からの登山者がたくさんいまして、国籍のいかんによらず、皆は同じ目標に憧れていました。高く登れば登るほど寒くなっていきましたけど、準備不足で薄着だったせいで、寒いでした。  \n \n<center>![Fuji](/image/Fuji/Fuji_2.jpg)</center>  \n  \n----------------------------------------  \n\n&#8195;&#8195;2日目の朝3時に、私たちは九合目に着きました。その後、皆は富士山頂で日の出を待っていました。4時、太陽が昇って来たところの写真を撮りました。富士山頂の日の出はとても綺麗でした！「苦しい奮闘をめきにして、この美しい景色を見ることはできなかっただろう。」って感じました。その後、下山には6時間かかりました。疲れましたけど、面白かったです。 ","slug":"ja-富士登山","published":1,"updated":"2017-09-04T11:19:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yg000dog64gexpjatb","content":"<center><img src=\"/image/Fuji/Fuji_1.jpg\" alt=\"Fuji\"></center>  \n\n<hr>\n<p>&#8195;&#8195;2016年8月に友達と一緒に富士山に登りました。通常、富士山は毎年の7、8月だけ開放されます。<br>&#8195;&#8195;2016年せっかくのチャンスを見逃したくないので、友達と一緒に五合目に行きました。私たちは夜8時に、わくわくしながら登り始めました。友人とチャットしながら登りました。途中、たくさんの国からの登山者がたくさんいまして、国籍のいかんによらず、皆は同じ目標に憧れていました。高く登れば登るほど寒くなっていきましたけど、準備不足で薄着だったせいで、寒いでした。  </p>\n<center><img src=\"/image/Fuji/Fuji_2.jpg\" alt=\"Fuji\"></center>  \n\n<hr>\n<p>&#8195;&#8195;2日目の朝3時に、私たちは九合目に着きました。その後、皆は富士山頂で日の出を待っていました。4時、太陽が昇って来たところの写真を撮りました。富士山頂の日の出はとても綺麗でした！「苦しい奮闘をめきにして、この美しい景色を見ることはできなかっただろう。」って感じました。その後、下山には6時間かかりました。疲れましたけど、面白かったです。 </p>\n","excerpt":"","more":"<center><img src=\"/image/Fuji/Fuji_1.jpg\" alt=\"Fuji\"></center>  \n\n<hr>\n<p>&#8195;&#8195;2016年8月に友達と一緒に富士山に登りました。通常、富士山は毎年の7、8月だけ開放されます。<br>&#8195;&#8195;2016年せっかくのチャンスを見逃したくないので、友達と一緒に五合目に行きました。私たちは夜8時に、わくわくしながら登り始めました。友人とチャットしながら登りました。途中、たくさんの国からの登山者がたくさんいまして、国籍のいかんによらず、皆は同じ目標に憧れていました。高く登れば登るほど寒くなっていきましたけど、準備不足で薄着だったせいで、寒いでした。  </p>\n<center><img src=\"/image/Fuji/Fuji_2.jpg\" alt=\"Fuji\"></center>  \n\n<hr>\n<p>&#8195;&#8195;2日目の朝3時に、私たちは九合目に着きました。その後、皆は富士山頂で日の出を待っていました。4時、太陽が昇って来たところの写真を撮りました。富士山頂の日の出はとても綺麗でした！「苦しい奮闘をめきにして、この美しい景色を見ることはできなかっただろう。」って感じました。その後、下山には6時間かかりました。疲れましたけど、面白かったです。 </p>\n"},{"title":"紙幣の肖像人物","lang":"ja","date":"2016-11-30T02:14:00.000Z","_content":"10000円の日本紙幣を見た。1万円札の人はかっこいいので、日本紙幣の人の資料を調べた。\n1万円札の人の名前は福沢諭吉で、日本の立派な教育者だ。\n5千円札のは樋口一葉で、日本史上第一位女性の紙幣肖像人物として、才気あふれる小説家だ。\n1千円札のは野口英世で、日本の現代細菌学創始者だ。\n日本紙幣の肖像人物は民衆で、天皇ではない。中国紙幣肖像人物は、1毛と5毛ほかのはぜんぶ毛沢東だ。 ","source":"_posts/ja/紙幣の肖像人物.md","raw":"---\ntitle: 紙幣の肖像人物\nlang: ja\ndate: 2016-11-30 11:14:00\n---\n10000円の日本紙幣を見た。1万円札の人はかっこいいので、日本紙幣の人の資料を調べた。\n1万円札の人の名前は福沢諭吉で、日本の立派な教育者だ。\n5千円札のは樋口一葉で、日本史上第一位女性の紙幣肖像人物として、才気あふれる小説家だ。\n1千円札のは野口英世で、日本の現代細菌学創始者だ。\n日本紙幣の肖像人物は民衆で、天皇ではない。中国紙幣肖像人物は、1毛と5毛ほかのはぜんぶ毛沢東だ。 ","slug":"ja-紙幣の肖像人物","published":1,"updated":"2017-12-09T14:05:16.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yi000eog64mu3icbtv","content":"<p>10000円の日本紙幣を見た。1万円札の人はかっこいいので、日本紙幣の人の資料を調べた。<br>1万円札の人の名前は福沢諭吉で、日本の立派な教育者だ。<br>5千円札のは樋口一葉で、日本史上第一位女性の紙幣肖像人物として、才気あふれる小説家だ。<br>1千円札のは野口英世で、日本の現代細菌学創始者だ。<br>日本紙幣の肖像人物は民衆で、天皇ではない。中国紙幣肖像人物は、1毛と5毛ほかのはぜんぶ毛沢東だ。 </p>\n","excerpt":"","more":"<p>10000円の日本紙幣を見た。1万円札の人はかっこいいので、日本紙幣の人の資料を調べた。<br>1万円札の人の名前は福沢諭吉で、日本の立派な教育者だ。<br>5千円札のは樋口一葉で、日本史上第一位女性の紙幣肖像人物として、才気あふれる小説家だ。<br>1千円札のは野口英世で、日本の現代細菌学創始者だ。<br>日本紙幣の肖像人物は民衆で、天皇ではない。中国紙幣肖像人物は、1毛と5毛ほかのはぜんぶ毛沢東だ。 </p>\n"},{"title":"Deep learning笔记1-神经网络入门","lang":"zh","date":"2017-08-28T09:18:56.000Z","_content":"\n### 1. 简单的神经网络（Neural Network）\n\n<center>![simple-neuron](/image/DL/1/simple-neuron.png)</center>  \n\n- 神经网络示意图，圆圈代表单元，方块是运算\n\n- 这个架构使得神经网络可以实现，激活函数 f(h) 可以是任何函数，此例用\n\n$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    #TODO: Implement sigmoid function\n    return 1/(1 + np.exp(-x))\n\ninputs = np.array([0.7, -0.3])\nweights = np.array([0.1, 0.8])\nbias = -0.1\n\n#Calculate the output\noutput = sigmoid(np.dot(weights, inputs) + bias)\n\nprint('Output:')\nprint(output)\n```\n\n\n### 2. 重要的三个函数（Function）\n\n- h(x) 即模型，也就是从输入特征预测输入的那个函数\n\n$$h(x) = w_{1}x_{1} + w_{2}x_{2} + … + b , \\text{其中w为权值，b为偏置项}$$\n\n- E(w) 目标函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。eg:\n\n ① 最小二乘式（一般用于回归类问题）：\n$$E(w) = \\frac{1}{2}\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}})^{2}，其中，梯度 \\Delta E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i}$$\n\n ② 交叉熵式（一般用于分类问题）：\n$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$  \n\n- f(x) 激活函数，常用的有sigmoid／relu／softmax\n\n### 3. 随机梯度下降（Stochastic Gradient Descent,）\n\n- 将预测值与实际值的误差$|y^{0}-y^{t}|$，组成的一个抛物线函数$|{y^{0}-y^{t}}|^2$，在抛物线里一直找梯度向下的方向，乘以步长$\\eta$（也称作学习信率），不断地迭代，找出最小值。\n\n\n- 梯度下降算法：\n$$\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\qquad(式3.1)$$\n其中△为梯度算子，△f(x)是指f(x)的梯度，$\\eta$步长\n\n\n- 梯度下降算法可以写成：(△E(w)详细推导请看 [△E(w)的推导](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") )\n\n$$\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\qquad(式3.2)$$\n\n$$由于梯度：  △E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i} $$\n\n\n\n- 因此，线性单元的参数修改规则最后是:\n\n$$\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad(式3.3)$$\n\n\n### 4. 前向传播（Forward Propagation）\n\n- 在完成网络的每个层级时，计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。\n\n### 5. 反向传播（Back Propagation）\n\n- 在神经网络中使用权重将信号从输入层传播到输出层。使用权重将错误从输出层传播回网络，以便更新权重。  \n\n- 按照下面的方法计算出每个节点的误差项（error term） $\\delta_i$\n\n- 对于输出层节点i，\n$$\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式5.1)$$\n\n- 对于隐藏层节点，\n$$\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式5.2)$$\n\n- 梯度下降公式可表达为：\n$$w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5.3)$$\n\n- 反向传播算法的详细推导请看 （[反向传播算法的推导](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\"))\n\n- 关于反向传播的推荐阅读（[CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") ）\n\n- [动态图表示前向和后向传播全过程 - daniel-D](http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html \"Title\") \n<center>![simple-neuron](/image/DL/1/BP.gif)</center>   \n\n### 6. 超参数的确定（Adjust Parameters）\n\n#### 选择迭代次数\n\n也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n\n#### 选择学习速率$\\eta$\n\n速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n\n\n#### 选择隐藏节点数量\n\n隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。\n\n- 调整根据经验，下面有几个经验公式：\n\n\\begin{align}\n&m=\\sqrt{n+l}+\\alpha\\\\\n&m=log_2n\\\\\n&m=\\sqrt{nl}\\\\\n&m:隐藏层节点数\\\\\n&n:输入层节点数\\\\\n&l:输出层节点数\\\\\n&\\alpha:1到10之间的常数\n\\end{align}\n\n\n### 参考资料（Reference）\n\n\n- [梯度下降](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") \n\n\n- [反向传播以及超参数的确定](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\") \n  \n  \n- [CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") ","source":"_posts/zh/Deep learning笔记1-神经网络入门.md","raw":"\n---\ntitle: Deep learning笔记1-神经网络入门\nlang: zh\ndate: 2017-08-28 18:18:56\ntags: Deep Learning\n---\n\n### 1. 简单的神经网络（Neural Network）\n\n<center>![simple-neuron](/image/DL/1/simple-neuron.png)</center>  \n\n- 神经网络示意图，圆圈代表单元，方块是运算\n\n- 这个架构使得神经网络可以实现，激活函数 f(h) 可以是任何函数，此例用\n\n$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    #TODO: Implement sigmoid function\n    return 1/(1 + np.exp(-x))\n\ninputs = np.array([0.7, -0.3])\nweights = np.array([0.1, 0.8])\nbias = -0.1\n\n#Calculate the output\noutput = sigmoid(np.dot(weights, inputs) + bias)\n\nprint('Output:')\nprint(output)\n```\n\n\n### 2. 重要的三个函数（Function）\n\n- h(x) 即模型，也就是从输入特征预测输入的那个函数\n\n$$h(x) = w_{1}x_{1} + w_{2}x_{2} + … + b , \\text{其中w为权值，b为偏置项}$$\n\n- E(w) 目标函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。eg:\n\n ① 最小二乘式（一般用于回归类问题）：\n$$E(w) = \\frac{1}{2}\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}})^{2}，其中，梯度 \\Delta E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i}$$\n\n ② 交叉熵式（一般用于分类问题）：\n$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$  \n\n- f(x) 激活函数，常用的有sigmoid／relu／softmax\n\n### 3. 随机梯度下降（Stochastic Gradient Descent,）\n\n- 将预测值与实际值的误差$|y^{0}-y^{t}|$，组成的一个抛物线函数$|{y^{0}-y^{t}}|^2$，在抛物线里一直找梯度向下的方向，乘以步长$\\eta$（也称作学习信率），不断地迭代，找出最小值。\n\n\n- 梯度下降算法：\n$$\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\qquad(式3.1)$$\n其中△为梯度算子，△f(x)是指f(x)的梯度，$\\eta$步长\n\n\n- 梯度下降算法可以写成：(△E(w)详细推导请看 [△E(w)的推导](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") )\n\n$$\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\qquad(式3.2)$$\n\n$$由于梯度：  △E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i} $$\n\n\n\n- 因此，线性单元的参数修改规则最后是:\n\n$$\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad(式3.3)$$\n\n\n### 4. 前向传播（Forward Propagation）\n\n- 在完成网络的每个层级时，计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。\n\n### 5. 反向传播（Back Propagation）\n\n- 在神经网络中使用权重将信号从输入层传播到输出层。使用权重将错误从输出层传播回网络，以便更新权重。  \n\n- 按照下面的方法计算出每个节点的误差项（error term） $\\delta_i$\n\n- 对于输出层节点i，\n$$\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式5.1)$$\n\n- 对于隐藏层节点，\n$$\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式5.2)$$\n\n- 梯度下降公式可表达为：\n$$w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5.3)$$\n\n- 反向传播算法的详细推导请看 （[反向传播算法的推导](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\"))\n\n- 关于反向传播的推荐阅读（[CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") ）\n\n- [动态图表示前向和后向传播全过程 - daniel-D](http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html \"Title\") \n<center>![simple-neuron](/image/DL/1/BP.gif)</center>   \n\n### 6. 超参数的确定（Adjust Parameters）\n\n#### 选择迭代次数\n\n也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n\n#### 选择学习速率$\\eta$\n\n速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n\n\n#### 选择隐藏节点数量\n\n隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。\n\n- 调整根据经验，下面有几个经验公式：\n\n\\begin{align}\n&m=\\sqrt{n+l}+\\alpha\\\\\n&m=log_2n\\\\\n&m=\\sqrt{nl}\\\\\n&m:隐藏层节点数\\\\\n&n:输入层节点数\\\\\n&l:输出层节点数\\\\\n&\\alpha:1到10之间的常数\n\\end{align}\n\n\n### 参考资料（Reference）\n\n\n- [梯度下降](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") \n\n\n- [反向传播以及超参数的确定](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\") \n  \n  \n- [CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") ","slug":"zh-Deep-learning笔记1-神经网络入门","published":1,"updated":"2018-01-01T13:12:23.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yj000gog6444l2ogdu","content":"<h3 id=\"1-简单的神经网络（Neural-Network）\"><a href=\"#1-简单的神经网络（Neural-Network）\" class=\"headerlink\" title=\"1. 简单的神经网络（Neural Network）\"></a>1. 简单的神经网络（Neural Network）</h3><center><img src=\"/image/DL/1/simple-neuron.png\" alt=\"simple-neuron\"></center>  \n\n<ul>\n<li><p>神经网络示意图，圆圈代表单元，方块是运算</p>\n</li>\n<li><p>这个架构使得神经网络可以实现，激活函数 f(h) 可以是任何函数，此例用</p>\n</li>\n</ul>\n<p>$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(x)</span>:</span></div><div class=\"line\">    <span class=\"comment\">#<span class=\"doctag\">TODO:</span> Implement sigmoid function</span></div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span> + np.exp(-x))</div><div class=\"line\"></div><div class=\"line\">inputs = np.array([<span class=\"number\">0.7</span>, <span class=\"number\">-0.3</span>])</div><div class=\"line\">weights = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>])</div><div class=\"line\">bias = <span class=\"number\">-0.1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">#Calculate the output</span></div><div class=\"line\">output = sigmoid(np.dot(weights, inputs) + bias)</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Output:'</span>)</div><div class=\"line\">print(output)</div></pre></td></tr></table></figure>\n<h3 id=\"2-重要的三个函数（Function）\"><a href=\"#2-重要的三个函数（Function）\" class=\"headerlink\" title=\"2. 重要的三个函数（Function）\"></a>2. 重要的三个函数（Function）</h3><ul>\n<li>h(x) 即模型，也就是从输入特征预测输入的那个函数</li>\n</ul>\n<p>$$h(x) = w_{1}x_{1} + w_{2}x_{2} + … + b , \\text{其中w为权值，b为偏置项}$$</p>\n<ul>\n<li><p>E(w) 目标函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。eg:</p>\n<p>① 最小二乘式（一般用于回归类问题）：<br>$$E(w) = \\frac{1}{2}\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}})^{2}，其中，梯度 \\Delta E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i}$$</p>\n<p>② 交叉熵式（一般用于分类问题）：<br>$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$  </p>\n</li>\n<li><p>f(x) 激活函数，常用的有sigmoid／relu／softmax</p>\n</li>\n</ul>\n<h3 id=\"3-随机梯度下降（Stochastic-Gradient-Descent-）\"><a href=\"#3-随机梯度下降（Stochastic-Gradient-Descent-）\" class=\"headerlink\" title=\"3. 随机梯度下降（Stochastic Gradient Descent,）\"></a>3. 随机梯度下降（Stochastic Gradient Descent,）</h3><ul>\n<li>将预测值与实际值的误差$|y^{0}-y^{t}|$，组成的一个抛物线函数$|{y^{0}-y^{t}}|^2$，在抛物线里一直找梯度向下的方向，乘以步长$\\eta$（也称作学习信率），不断地迭代，找出最小值。</li>\n</ul>\n<ul>\n<li>梯度下降算法：<br>$$\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\qquad(式3.1)$$<br>其中△为梯度算子，△f(x)是指f(x)的梯度，$\\eta$步长</li>\n</ul>\n<ul>\n<li>梯度下降算法可以写成：(△E(w)详细推导请看 <a href=\"https://www.zybuluo.com/hanbingtao/note/448086/\" title=\"Title\" target=\"_blank\" rel=\"external\">△E(w)的推导</a> )</li>\n</ul>\n<p>$$\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\qquad(式3.2)$$</p>\n<p>$$由于梯度：  △E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i} $$</p>\n<ul>\n<li>因此，线性单元的参数修改规则最后是:</li>\n</ul>\n<p>$$\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad(式3.3)$$</p>\n<h3 id=\"4-前向传播（Forward-Propagation）\"><a href=\"#4-前向传播（Forward-Propagation）\" class=\"headerlink\" title=\"4. 前向传播（Forward Propagation）\"></a>4. 前向传播（Forward Propagation）</h3><ul>\n<li>在完成网络的每个层级时，计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。</li>\n</ul>\n<h3 id=\"5-反向传播（Back-Propagation）\"><a href=\"#5-反向传播（Back-Propagation）\" class=\"headerlink\" title=\"5. 反向传播（Back Propagation）\"></a>5. 反向传播（Back Propagation）</h3><ul>\n<li><p>在神经网络中使用权重将信号从输入层传播到输出层。使用权重将错误从输出层传播回网络，以便更新权重。  </p>\n</li>\n<li><p>按照下面的方法计算出每个节点的误差项（error term） $\\delta_i$</p>\n</li>\n<li><p>对于输出层节点i，<br>$$\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式5.1)$$</p>\n</li>\n<li><p>对于隐藏层节点，<br>$$\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式5.2)$$</p>\n</li>\n<li><p>梯度下降公式可表达为：<br>$$w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5.3)$$</p>\n</li>\n<li><p>反向传播算法的详细推导请看 （<a href=\"https://www.zybuluo.com/hanbingtao/note/476663/\" title=\"Title\" target=\"_blank\" rel=\"external\">反向传播算法的推导</a>)</p>\n</li>\n<li><p>关于反向传播的推荐阅读（<a href=\"http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html\" title=\"Title\" target=\"_blank\" rel=\"external\">CS224n笔记5 反向传播与项目指导 - Hankcs</a> ）</p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html\" title=\"Title\" target=\"_blank\" rel=\"external\">动态图表示前向和后向传播全过程 - daniel-D</a> </p>\n<center><img src=\"/image/DL/1/BP.gif\" alt=\"simple-neuron\"></center>   \n\n</li>\n</ul>\n<h3 id=\"6-超参数的确定（Adjust-Parameters）\"><a href=\"#6-超参数的确定（Adjust-Parameters）\" class=\"headerlink\" title=\"6. 超参数的确定（Adjust Parameters）\"></a>6. 超参数的确定（Adjust Parameters）</h3><h4 id=\"选择迭代次数\"><a href=\"#选择迭代次数\" class=\"headerlink\" title=\"选择迭代次数\"></a>选择迭代次数</h4><p>也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。</p>\n<h4 id=\"选择学习速率-eta\"><a href=\"#选择学习速率-eta\" class=\"headerlink\" title=\"选择学习速率$\\eta$\"></a>选择学习速率$\\eta$</h4><p>速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。</p>\n<h4 id=\"选择隐藏节点数量\"><a href=\"#选择隐藏节点数量\" class=\"headerlink\" title=\"选择隐藏节点数量\"></a>选择隐藏节点数量</h4><p>隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。</p>\n<ul>\n<li>调整根据经验，下面有几个经验公式：</li>\n</ul>\n<p>\\begin{align}<br>&amp;m=\\sqrt{n+l}+\\alpha\\\\<br>&amp;m=log_2n\\\\<br>&amp;m=\\sqrt{nl}\\\\<br>&amp;m:隐藏层节点数\\\\<br>&amp;n:输入层节点数\\\\<br>&amp;l:输出层节点数\\\\<br>&amp;\\alpha:1到10之间的常数<br>\\end{align}</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><a href=\"https://www.zybuluo.com/hanbingtao/note/448086/\" title=\"Title\" target=\"_blank\" rel=\"external\">梯度下降</a> </li>\n</ul>\n<ul>\n<li><a href=\"https://www.zybuluo.com/hanbingtao/note/476663/\" title=\"Title\" target=\"_blank\" rel=\"external\">反向传播以及超参数的确定</a> </li>\n</ul>\n<ul>\n<li><a href=\"http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html\" title=\"Title\" target=\"_blank\" rel=\"external\">CS224n笔记5 反向传播与项目指导 - Hankcs</a> </li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-简单的神经网络（Neural-Network）\"><a href=\"#1-简单的神经网络（Neural-Network）\" class=\"headerlink\" title=\"1. 简单的神经网络（Neural Network）\"></a>1. 简单的神经网络（Neural Network）</h3><center><img src=\"/image/DL/1/simple-neuron.png\" alt=\"simple-neuron\"></center>  \n\n<ul>\n<li><p>神经网络示意图，圆圈代表单元，方块是运算</p>\n</li>\n<li><p>这个架构使得神经网络可以实现，激活函数 f(h) 可以是任何函数，此例用</p>\n</li>\n</ul>\n<p>$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(x)</span>:</span></div><div class=\"line\">    <span class=\"comment\">#<span class=\"doctag\">TODO:</span> Implement sigmoid function</span></div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span>/(<span class=\"number\">1</span> + np.exp(-x))</div><div class=\"line\"></div><div class=\"line\">inputs = np.array([<span class=\"number\">0.7</span>, <span class=\"number\">-0.3</span>])</div><div class=\"line\">weights = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.8</span>])</div><div class=\"line\">bias = <span class=\"number\">-0.1</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">#Calculate the output</span></div><div class=\"line\">output = sigmoid(np.dot(weights, inputs) + bias)</div><div class=\"line\"></div><div class=\"line\">print(<span class=\"string\">'Output:'</span>)</div><div class=\"line\">print(output)</div></pre></td></tr></table></figure>\n<h3 id=\"2-重要的三个函数（Function）\"><a href=\"#2-重要的三个函数（Function）\" class=\"headerlink\" title=\"2. 重要的三个函数（Function）\"></a>2. 重要的三个函数（Function）</h3><ul>\n<li>h(x) 即模型，也就是从输入特征预测输入的那个函数</li>\n</ul>\n<p>$$h(x) = w_{1}x_{1} + w_{2}x_{2} + … + b , \\text{其中w为权值，b为偏置项}$$</p>\n<ul>\n<li><p>E(w) 目标函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。eg:</p>\n<p>① 最小二乘式（一般用于回归类问题）：<br>$$E(w) = \\frac{1}{2}\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}})^{2}，其中，梯度 \\Delta E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i}$$</p>\n<p>② 交叉熵式（一般用于分类问题）：<br>$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$  </p>\n</li>\n<li><p>f(x) 激活函数，常用的有sigmoid／relu／softmax</p>\n</li>\n</ul>\n<h3 id=\"3-随机梯度下降（Stochastic-Gradient-Descent-）\"><a href=\"#3-随机梯度下降（Stochastic-Gradient-Descent-）\" class=\"headerlink\" title=\"3. 随机梯度下降（Stochastic Gradient Descent,）\"></a>3. 随机梯度下降（Stochastic Gradient Descent,）</h3><ul>\n<li>将预测值与实际值的误差$|y^{0}-y^{t}|$，组成的一个抛物线函数$|{y^{0}-y^{t}}|^2$，在抛物线里一直找梯度向下的方向，乘以步长$\\eta$（也称作学习信率），不断地迭代，找出最小值。</li>\n</ul>\n<ul>\n<li>梯度下降算法：<br>$$\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\qquad(式3.1)$$<br>其中△为梯度算子，△f(x)是指f(x)的梯度，$\\eta$步长</li>\n</ul>\n<ul>\n<li>梯度下降算法可以写成：(△E(w)详细推导请看 <a href=\"https://www.zybuluo.com/hanbingtao/note/448086/\" title=\"Title\">△E(w)的推导</a> )</li>\n</ul>\n<p>$$\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\qquad(式3.2)$$</p>\n<p>$$由于梯度：  △E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i} $$</p>\n<ul>\n<li>因此，线性单元的参数修改规则最后是:</li>\n</ul>\n<p>$$\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad(式3.3)$$</p>\n<h3 id=\"4-前向传播（Forward-Propagation）\"><a href=\"#4-前向传播（Forward-Propagation）\" class=\"headerlink\" title=\"4. 前向传播（Forward Propagation）\"></a>4. 前向传播（Forward Propagation）</h3><ul>\n<li>在完成网络的每个层级时，计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。</li>\n</ul>\n<h3 id=\"5-反向传播（Back-Propagation）\"><a href=\"#5-反向传播（Back-Propagation）\" class=\"headerlink\" title=\"5. 反向传播（Back Propagation）\"></a>5. 反向传播（Back Propagation）</h3><ul>\n<li><p>在神经网络中使用权重将信号从输入层传播到输出层。使用权重将错误从输出层传播回网络，以便更新权重。  </p>\n</li>\n<li><p>按照下面的方法计算出每个节点的误差项（error term） $\\delta_i$</p>\n</li>\n<li><p>对于输出层节点i，<br>$$\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式5.1)$$</p>\n</li>\n<li><p>对于隐藏层节点，<br>$$\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式5.2)$$</p>\n</li>\n<li><p>梯度下降公式可表达为：<br>$$w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5.3)$$</p>\n</li>\n<li><p>反向传播算法的详细推导请看 （<a href=\"https://www.zybuluo.com/hanbingtao/note/476663/\" title=\"Title\">反向传播算法的推导</a>)</p>\n</li>\n<li><p>关于反向传播的推荐阅读（<a href=\"http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html\" title=\"Title\">CS224n笔记5 反向传播与项目指导 - Hankcs</a> ）</p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html\" title=\"Title\">动态图表示前向和后向传播全过程 - daniel-D</a> </p>\n<center><img src=\"/image/DL/1/BP.gif\" alt=\"simple-neuron\"></center>   \n\n</li>\n</ul>\n<h3 id=\"6-超参数的确定（Adjust-Parameters）\"><a href=\"#6-超参数的确定（Adjust-Parameters）\" class=\"headerlink\" title=\"6. 超参数的确定（Adjust Parameters）\"></a>6. 超参数的确定（Adjust Parameters）</h3><h4 id=\"选择迭代次数\"><a href=\"#选择迭代次数\" class=\"headerlink\" title=\"选择迭代次数\"></a>选择迭代次数</h4><p>也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。</p>\n<h4 id=\"选择学习速率-eta\"><a href=\"#选择学习速率-eta\" class=\"headerlink\" title=\"选择学习速率$\\eta$\"></a>选择学习速率$\\eta$</h4><p>速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。</p>\n<h4 id=\"选择隐藏节点数量\"><a href=\"#选择隐藏节点数量\" class=\"headerlink\" title=\"选择隐藏节点数量\"></a>选择隐藏节点数量</h4><p>隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。</p>\n<ul>\n<li>调整根据经验，下面有几个经验公式：</li>\n</ul>\n<p>\\begin{align}<br>&amp;m=\\sqrt{n+l}+\\alpha\\\\<br>&amp;m=log_2n\\\\<br>&amp;m=\\sqrt{nl}\\\\<br>&amp;m:隐藏层节点数\\\\<br>&amp;n:输入层节点数\\\\<br>&amp;l:输出层节点数\\\\<br>&amp;\\alpha:1到10之间的常数<br>\\end{align}</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><a href=\"https://www.zybuluo.com/hanbingtao/note/448086/\" title=\"Title\">梯度下降</a> </li>\n</ul>\n<ul>\n<li><a href=\"https://www.zybuluo.com/hanbingtao/note/476663/\" title=\"Title\">反向传播以及超参数的确定</a> </li>\n</ul>\n<ul>\n<li><a href=\"http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html\" title=\"Title\">CS224n笔记5 反向传播与项目指导 - Hankcs</a> </li>\n</ul>\n"},{"title":"Deep learning笔记4-TreeRNN递归神经网络","lang":"zh","date":"2017-10-19T09:18:56.000Z","_content":"\n### 1. 递归神经网络（TreeRNN）\n\n原图和公式以及说明来自：[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\nRNN循环神经网络处理词序列，但有时候把句子看做是词的序列是不够的，比如『两个外语学院的/学生』与『两个/外语学院的学生』意思不同，为了能够让模型区分出两个不同的意思，模型可借助树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。\n\nTreeRNN递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之就远。如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-1.png)</center> \n\n-------------------------------------\n\n递归神经网络可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-2.png)</center> \n\n-------------------------------------\n\n蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如\"intelligent humor\"是正面评价，而\"care about cleverness wit or any other kind of intelligent humor\"是中性评价。可以看到，模型能够正确的处理doesn't的含义，将正面评价转变为负面评价。\n\n尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。\n\n#### 1.1. 递归神经网络输出值的计算（正向）\n\n递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-3.png)</center> \n\n-------------------------------------\n\nC1和C2分别是表示两个子节点的向量，P是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。用矩阵W表示这些连接上的权重，它的维度将是d*2d，其中，d表示每个节点的维度。父节点的计算公式可以写成：\n\n\\begin{align}\n\\mathbf{p} = tanh(W\\begin{bmatrix}\\mathbf{c}_1\\\\\\mathbf{c}_2\\end{bmatrix}+\\mathbf{b})\n\\end{align}\n\n在上式中，tanh是激活函数（当然也可以用其它的激活函数），b是偏置项，它也是一个维度为d的向量。\n\n然后，把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，将得到根节点的向量，可以认为它是对整棵树的表示，这样就实现了把树映射为一个向量。在下图中使用递归神经网络处理一棵树，最终得到的向量（整棵树）的表示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-4.png)</center> \n\n-------------------------------------\n\n递归神经网络的权重w和偏置项b在所有的节点都是共享的。\n\n#### 1.2. 递归神经网络的训练（反向）\n\n递归神经网络的训练算法BPTS和循环神经网络类似，两者不同之处在于：\n\n● 前者需要将残差从根节点反向传播到各个子节点\n\n● 后者是将残差从当前时刻反向传播到初始时刻\n\n误差项从父节点传递到其子节点的公式：\n\n\\begin{align}\n\\delta_{c_j}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{c_j}}}\\\\\n&=\\frac{\\partial{E}}{\\partial{\\mathbf{c}_j}}\\frac{\\partial{\\mathbf{c}_j}}{\\partial{\\mathbf{net}_{c_j}}}\\\\\n&=W_j^T\\delta_p\\circ f'(\\mathbf{net}_{c_j})\n\\end{align}\n\n● 误差函数在第l层对权重的梯度为：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{w_{ji}^{(l)}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{w_{ji}^{(l)}}}\\\\\n&=\\delta_{p_j}^{(l)}c_i^{(l)}\\\\\n\\end{align}\n\n权重梯度是各个层权重梯度之和。即：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{W}}=\\sum_l\\frac{\\partial{E}}{\\partial{W^{(l)}}}\n\\end{align}\n\n如果使用梯度下降优化算法，那么权重更新公式为：\n\\begin{align}\nW\\gets W + \\eta\\frac{\\partial{E}}{\\partial{W}}\n\\end{align}\n\n● 误差函数对第l层偏置项的梯度为：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{b_j^{(l)}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{b_j^{(l)}}}\\\\\n&=\\delta_{p_j}^{(l)}\\\\\n\\end{align}\n\n偏置项梯度是各个层偏置项梯度之和，即：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}=\\sum_l\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}\n\\end{align}\n\n如果使用梯度下降优化算法，那么偏置项更新公式为：\n\\begin{align}\n\\mathbf{b}\\gets \\mathbf{b} + \\eta\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}\n\\end{align} \n\n### 2. 相关应用\n\n#### 2.1 在自然语言处理的应用（NLP）\n\n递归神经网络能够完成句子的语法分析，产生一颗语法解析树。\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/2-1.png)</center> \n\n-------------------------------------\n\n#### 2.1 在自然场景的应用\n\n除了自然语言之外，自然场景也具有可组合的性质。因此可以用类似的模型完成自然场景的解析，如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/2-2.png)</center> \n\n-------------------------------------\n\n### 参考资料（Reference）\n\n- [零基础入门深度学习(7) - 递归神经网络](https://zybuluo.com/hanbingtao/note/626300 \"Title\") \n\n- [Reccusive Neural Networkを用いた文章と句の類似度算出](https://qiita.com/hiroto0227/items/ea1c723903a3e20a32e2 \"Title\") \n\n- [Socher氏論文推論にNeural Tensor（テンソル）Networkモデルの提案](https://qiita.com/HirofumiYashima/items/8ced35dcb437ed6aab6c \"Title\") \n\n- [Recursive Neural Network](http://www.iro.umontreal.ca/~bengioy/talks/gss2012-YB6-NLP-recursive.pdf\"Title\") \n\n- [Recursive Deep Models](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf \"Title\") \n","source":"_posts/zh/Deep learning笔记4-TreeRNN递归神经网络.md","raw":"\n---\ntitle: Deep learning笔记4-TreeRNN递归神经网络\nlang: zh\ndate: 2017-10-19 18:18:56\ntags: Deep Learning\n---\n\n### 1. 递归神经网络（TreeRNN）\n\n原图和公式以及说明来自：[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\nRNN循环神经网络处理词序列，但有时候把句子看做是词的序列是不够的，比如『两个外语学院的/学生』与『两个/外语学院的学生』意思不同，为了能够让模型区分出两个不同的意思，模型可借助树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。\n\nTreeRNN递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之就远。如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-1.png)</center> \n\n-------------------------------------\n\n递归神经网络可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-2.png)</center> \n\n-------------------------------------\n\n蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如\"intelligent humor\"是正面评价，而\"care about cleverness wit or any other kind of intelligent humor\"是中性评价。可以看到，模型能够正确的处理doesn't的含义，将正面评价转变为负面评价。\n\n尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。\n\n#### 1.1. 递归神经网络输出值的计算（正向）\n\n递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-3.png)</center> \n\n-------------------------------------\n\nC1和C2分别是表示两个子节点的向量，P是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。用矩阵W表示这些连接上的权重，它的维度将是d*2d，其中，d表示每个节点的维度。父节点的计算公式可以写成：\n\n\\begin{align}\n\\mathbf{p} = tanh(W\\begin{bmatrix}\\mathbf{c}_1\\\\\\mathbf{c}_2\\end{bmatrix}+\\mathbf{b})\n\\end{align}\n\n在上式中，tanh是激活函数（当然也可以用其它的激活函数），b是偏置项，它也是一个维度为d的向量。\n\n然后，把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，将得到根节点的向量，可以认为它是对整棵树的表示，这样就实现了把树映射为一个向量。在下图中使用递归神经网络处理一棵树，最终得到的向量（整棵树）的表示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/1-4.png)</center> \n\n-------------------------------------\n\n递归神经网络的权重w和偏置项b在所有的节点都是共享的。\n\n#### 1.2. 递归神经网络的训练（反向）\n\n递归神经网络的训练算法BPTS和循环神经网络类似，两者不同之处在于：\n\n● 前者需要将残差从根节点反向传播到各个子节点\n\n● 后者是将残差从当前时刻反向传播到初始时刻\n\n误差项从父节点传递到其子节点的公式：\n\n\\begin{align}\n\\delta_{c_j}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{c_j}}}\\\\\n&=\\frac{\\partial{E}}{\\partial{\\mathbf{c}_j}}\\frac{\\partial{\\mathbf{c}_j}}{\\partial{\\mathbf{net}_{c_j}}}\\\\\n&=W_j^T\\delta_p\\circ f'(\\mathbf{net}_{c_j})\n\\end{align}\n\n● 误差函数在第l层对权重的梯度为：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{w_{ji}^{(l)}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{w_{ji}^{(l)}}}\\\\\n&=\\delta_{p_j}^{(l)}c_i^{(l)}\\\\\n\\end{align}\n\n权重梯度是各个层权重梯度之和。即：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{W}}=\\sum_l\\frac{\\partial{E}}{\\partial{W^{(l)}}}\n\\end{align}\n\n如果使用梯度下降优化算法，那么权重更新公式为：\n\\begin{align}\nW\\gets W + \\eta\\frac{\\partial{E}}{\\partial{W}}\n\\end{align}\n\n● 误差函数对第l层偏置项的梯度为：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{b_j^{(l)}}}&=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{b_j^{(l)}}}\\\\\n&=\\delta_{p_j}^{(l)}\\\\\n\\end{align}\n\n偏置项梯度是各个层偏置项梯度之和，即：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}=\\sum_l\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}\n\\end{align}\n\n如果使用梯度下降优化算法，那么偏置项更新公式为：\n\\begin{align}\n\\mathbf{b}\\gets \\mathbf{b} + \\eta\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}\n\\end{align} \n\n### 2. 相关应用\n\n#### 2.1 在自然语言处理的应用（NLP）\n\n递归神经网络能够完成句子的语法分析，产生一颗语法解析树。\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/2-1.png)</center> \n\n-------------------------------------\n\n#### 2.1 在自然场景的应用\n\n除了自然语言之外，自然场景也具有可组合的性质。因此可以用类似的模型完成自然场景的解析，如下图所示：\n\n-------------------------------------\n\n<center>![TreeRNN](/image/DL/4/2-2.png)</center> \n\n-------------------------------------\n\n### 参考资料（Reference）\n\n- [零基础入门深度学习(7) - 递归神经网络](https://zybuluo.com/hanbingtao/note/626300 \"Title\") \n\n- [Reccusive Neural Networkを用いた文章と句の類似度算出](https://qiita.com/hiroto0227/items/ea1c723903a3e20a32e2 \"Title\") \n\n- [Socher氏論文推論にNeural Tensor（テンソル）Networkモデルの提案](https://qiita.com/HirofumiYashima/items/8ced35dcb437ed6aab6c \"Title\") \n\n- [Recursive Neural Network](http://www.iro.umontreal.ca/~bengioy/talks/gss2012-YB6-NLP-recursive.pdf\"Title\") \n\n- [Recursive Deep Models](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf \"Title\") \n","slug":"zh-Deep-learning笔记4-TreeRNN递归神经网络","published":1,"updated":"2018-01-01T13:12:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yk000iog64j7zalswt","content":"<h3 id=\"1-递归神经网络（TreeRNN）\"><a href=\"#1-递归神经网络（TreeRNN）\" class=\"headerlink\" title=\"1. 递归神经网络（TreeRNN）\"></a>1. 递归神经网络（TreeRNN）</h3><p>原图和公式以及说明来自：<a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n<p>RNN循环神经网络处理词序列，但有时候把句子看做是词的序列是不够的，比如『两个外语学院的/学生』与『两个/外语学院的学生』意思不同，为了能够让模型区分出两个不同的意思，模型可借助树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。</p>\n<p>TreeRNN递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之就远。如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-1.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>递归神经网络可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。</p>\n<hr>\n<center><img src=\"/image/DL/4/1-2.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如”intelligent humor”是正面评价，而”care about cleverness wit or any other kind of intelligent humor”是中性评价。可以看到，模型能够正确的处理doesn’t的含义，将正面评价转变为负面评价。</p>\n<p>尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。</p>\n<h4 id=\"1-1-递归神经网络输出值的计算（正向）\"><a href=\"#1-1-递归神经网络输出值的计算（正向）\" class=\"headerlink\" title=\"1.1. 递归神经网络输出值的计算（正向）\"></a>1.1. 递归神经网络输出值的计算（正向）</h4><p>递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-3.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>C1和C2分别是表示两个子节点的向量，P是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。用矩阵W表示这些连接上的权重，它的维度将是d*2d，其中，d表示每个节点的维度。父节点的计算公式可以写成：</p>\n<p>\\begin{align}<br>\\mathbf{p} = tanh(W\\begin{bmatrix}\\mathbf{c}_1\\\\\\mathbf{c}_2\\end{bmatrix}+\\mathbf{b})<br>\\end{align}</p>\n<p>在上式中，tanh是激活函数（当然也可以用其它的激活函数），b是偏置项，它也是一个维度为d的向量。</p>\n<p>然后，把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，将得到根节点的向量，可以认为它是对整棵树的表示，这样就实现了把树映射为一个向量。在下图中使用递归神经网络处理一棵树，最终得到的向量（整棵树）的表示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-4.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>递归神经网络的权重w和偏置项b在所有的节点都是共享的。</p>\n<h4 id=\"1-2-递归神经网络的训练（反向）\"><a href=\"#1-2-递归神经网络的训练（反向）\" class=\"headerlink\" title=\"1.2. 递归神经网络的训练（反向）\"></a>1.2. 递归神经网络的训练（反向）</h4><p>递归神经网络的训练算法BPTS和循环神经网络类似，两者不同之处在于：</p>\n<p>● 前者需要将残差从根节点反向传播到各个子节点</p>\n<p>● 后者是将残差从当前时刻反向传播到初始时刻</p>\n<p>误差项从父节点传递到其子节点的公式：</p>\n<p>\\begin{align}<br>\\delta_{c_j}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{c_j}}}\\\\<br>&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{c}_j}}\\frac{\\partial{\\mathbf{c}_j}}{\\partial{\\mathbf{net}_{c_j}}}\\\\<br>&amp;=W_j^T\\delta_p\\circ f’(\\mathbf{net}_{c_j})<br>\\end{align}</p>\n<p>● 误差函数在第l层对权重的梯度为：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{w_{ji}^{(l)}}}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{w_{ji}^{(l)}}}\\\\<br>&amp;=\\delta_{p_j}^{(l)}c_i^{(l)}\\\\<br>\\end{align}</p>\n<p>权重梯度是各个层权重梯度之和。即：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{W}}=\\sum_l\\frac{\\partial{E}}{\\partial{W^{(l)}}}<br>\\end{align}</p>\n<p>如果使用梯度下降优化算法，那么权重更新公式为：<br>\\begin{align}<br>W\\gets W + \\eta\\frac{\\partial{E}}{\\partial{W}}<br>\\end{align}</p>\n<p>● 误差函数对第l层偏置项的梯度为：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{b_j^{(l)}}}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{b_j^{(l)}}}\\\\<br>&amp;=\\delta_{p_j}^{(l)}\\\\<br>\\end{align}</p>\n<p>偏置项梯度是各个层偏置项梯度之和，即：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}=\\sum_l\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}<br>\\end{align}</p>\n<p>如果使用梯度下降优化算法，那么偏置项更新公式为：<br>\\begin{align}<br>\\mathbf{b}\\gets \\mathbf{b} + \\eta\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}<br>\\end{align} </p>\n<h3 id=\"2-相关应用\"><a href=\"#2-相关应用\" class=\"headerlink\" title=\"2. 相关应用\"></a>2. 相关应用</h3><h4 id=\"2-1-在自然语言处理的应用（NLP）\"><a href=\"#2-1-在自然语言处理的应用（NLP）\" class=\"headerlink\" title=\"2.1 在自然语言处理的应用（NLP）\"></a>2.1 在自然语言处理的应用（NLP）</h4><p>递归神经网络能够完成句子的语法分析，产生一颗语法解析树。</p>\n<hr>\n<center><img src=\"/image/DL/4/2-1.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<h4 id=\"2-1-在自然场景的应用\"><a href=\"#2-1-在自然场景的应用\" class=\"headerlink\" title=\"2.1 在自然场景的应用\"></a>2.1 在自然场景的应用</h4><p>除了自然语言之外，自然场景也具有可组合的性质。因此可以用类似的模型完成自然场景的解析，如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/2-2.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/626300\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(7) - 递归神经网络</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/hiroto0227/items/ea1c723903a3e20a32e2\" title=\"Title\" target=\"_blank\" rel=\"external\">Reccusive Neural Networkを用いた文章と句の類似度算出</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/HirofumiYashima/items/8ced35dcb437ed6aab6c\" title=\"Title\" target=\"_blank\" rel=\"external\">Socher氏論文推論にNeural Tensor（テンソル）Networkモデルの提案</a> </p>\n</li>\n<li><p><a href=\"http://www.iro.umontreal.ca/~bengioy/talks/gss2012-YB6-NLP-recursive.pdf&quot;Title&quot;\" target=\"_blank\" rel=\"external\">Recursive Neural Network</a> </p>\n</li>\n<li><p><a href=\"https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf\" title=\"Title\" target=\"_blank\" rel=\"external\">Recursive Deep Models</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-递归神经网络（TreeRNN）\"><a href=\"#1-递归神经网络（TreeRNN）\" class=\"headerlink\" title=\"1. 递归神经网络（TreeRNN）\"></a>1. 递归神经网络（TreeRNN）</h3><p>原图和公式以及说明来自：<a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n<p>RNN循环神经网络处理词序列，但有时候把句子看做是词的序列是不够的，比如『两个外语学院的/学生』与『两个/外语学院的学生』意思不同，为了能够让模型区分出两个不同的意思，模型可借助树结构去处理信息，而不是序列，这就是递归神经网络的作用。当面对按照树/图结构处理信息更有效的任务时，递归神经网络通常都会获得不错的结果。</p>\n<p>TreeRNN递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之就远。如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-1.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>递归神经网络可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。</p>\n<hr>\n<center><img src=\"/image/DL/4/1-2.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>蓝色表示正面评价，红色表示负面评价。每个节点是一个向量，这个向量表达了以它为根的子树的情感评价。比如”intelligent humor”是正面评价，而”care about cleverness wit or any other kind of intelligent humor”是中性评价。可以看到，模型能够正确的处理doesn’t的含义，将正面评价转变为负面评价。</p>\n<p>尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。</p>\n<h4 id=\"1-1-递归神经网络输出值的计算（正向）\"><a href=\"#1-1-递归神经网络输出值的计算（正向）\" class=\"headerlink\" title=\"1.1. 递归神经网络输出值的计算（正向）\"></a>1.1. 递归神经网络输出值的计算（正向）</h4><p>递归神经网络的输入是两个子节点（也可以是多个），输出就是将这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-3.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>C1和C2分别是表示两个子节点的向量，P是表示父节点的向量。子节点和父节点组成一个全连接神经网络，也就是子节点的每个神经元都和父节点的每个神经元两两相连。用矩阵W表示这些连接上的权重，它的维度将是d*2d，其中，d表示每个节点的维度。父节点的计算公式可以写成：</p>\n<p>\\begin{align}<br>\\mathbf{p} = tanh(W\\begin{bmatrix}\\mathbf{c}_1\\\\\\mathbf{c}_2\\end{bmatrix}+\\mathbf{b})<br>\\end{align}</p>\n<p>在上式中，tanh是激活函数（当然也可以用其它的激活函数），b是偏置项，它也是一个维度为d的向量。</p>\n<p>然后，把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，将得到根节点的向量，可以认为它是对整棵树的表示，这样就实现了把树映射为一个向量。在下图中使用递归神经网络处理一棵树，最终得到的向量（整棵树）的表示：</p>\n<hr>\n<center><img src=\"/image/DL/4/1-4.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<p>递归神经网络的权重w和偏置项b在所有的节点都是共享的。</p>\n<h4 id=\"1-2-递归神经网络的训练（反向）\"><a href=\"#1-2-递归神经网络的训练（反向）\" class=\"headerlink\" title=\"1.2. 递归神经网络的训练（反向）\"></a>1.2. 递归神经网络的训练（反向）</h4><p>递归神经网络的训练算法BPTS和循环神经网络类似，两者不同之处在于：</p>\n<p>● 前者需要将残差从根节点反向传播到各个子节点</p>\n<p>● 后者是将残差从当前时刻反向传播到初始时刻</p>\n<p>误差项从父节点传递到其子节点的公式：</p>\n<p>\\begin{align}<br>\\delta_{c_j}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{c_j}}}\\\\<br>&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{c}_j}}\\frac{\\partial{\\mathbf{c}_j}}{\\partial{\\mathbf{net}_{c_j}}}\\\\<br>&amp;=W_j^T\\delta_p\\circ f’(\\mathbf{net}_{c_j})<br>\\end{align}</p>\n<p>● 误差函数在第l层对权重的梯度为：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{w_{ji}^{(l)}}}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{w_{ji}^{(l)}}}\\\\<br>&amp;=\\delta_{p_j}^{(l)}c_i^{(l)}\\\\<br>\\end{align}</p>\n<p>权重梯度是各个层权重梯度之和。即：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{W}}=\\sum_l\\frac{\\partial{E}}{\\partial{W^{(l)}}}<br>\\end{align}</p>\n<p>如果使用梯度下降优化算法，那么权重更新公式为：<br>\\begin{align}<br>W\\gets W + \\eta\\frac{\\partial{E}}{\\partial{W}}<br>\\end{align}</p>\n<p>● 误差函数对第l层偏置项的梯度为：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{b_j^{(l)}}}&amp;=\\frac{\\partial{E}}{\\partial{\\mathbf{net}_{p_j}^{(l)}}}\\frac{\\partial{\\mathbf{net}_{p_j}^{(l)}}}{\\partial{b_j^{(l)}}}\\\\<br>&amp;=\\delta_{p_j}^{(l)}\\\\<br>\\end{align}</p>\n<p>偏置项梯度是各个层偏置项梯度之和，即：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}=\\sum_l\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}<br>\\end{align}</p>\n<p>如果使用梯度下降优化算法，那么偏置项更新公式为：<br>\\begin{align}<br>\\mathbf{b}\\gets \\mathbf{b} + \\eta\\frac{\\partial{E}}{\\partial{\\mathbf{b}}}<br>\\end{align} </p>\n<h3 id=\"2-相关应用\"><a href=\"#2-相关应用\" class=\"headerlink\" title=\"2. 相关应用\"></a>2. 相关应用</h3><h4 id=\"2-1-在自然语言处理的应用（NLP）\"><a href=\"#2-1-在自然语言处理的应用（NLP）\" class=\"headerlink\" title=\"2.1 在自然语言处理的应用（NLP）\"></a>2.1 在自然语言处理的应用（NLP）</h4><p>递归神经网络能够完成句子的语法分析，产生一颗语法解析树。</p>\n<hr>\n<center><img src=\"/image/DL/4/2-1.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<h4 id=\"2-1-在自然场景的应用\"><a href=\"#2-1-在自然场景的应用\" class=\"headerlink\" title=\"2.1 在自然场景的应用\"></a>2.1 在自然场景的应用</h4><p>除了自然语言之外，自然场景也具有可组合的性质。因此可以用类似的模型完成自然场景的解析，如下图所示：</p>\n<hr>\n<center><img src=\"/image/DL/4/2-2.png\" alt=\"TreeRNN\"></center> \n\n<hr>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/626300\" title=\"Title\">零基础入门深度学习(7) - 递归神经网络</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/hiroto0227/items/ea1c723903a3e20a32e2\" title=\"Title\">Reccusive Neural Networkを用いた文章と句の類似度算出</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/HirofumiYashima/items/8ced35dcb437ed6aab6c\" title=\"Title\">Socher氏論文推論にNeural Tensor（テンソル）Networkモデルの提案</a> </p>\n</li>\n<li><p><a href=\"http://www.iro.umontreal.ca/~bengioy/talks/gss2012-YB6-NLP-recursive.pdf&quot;Title&quot;\">Recursive Neural Network</a> </p>\n</li>\n<li><p><a href=\"https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf\" title=\"Title\">Recursive Deep Models</a> </p>\n</li>\n</ul>\n"},{"title":"JUDO-ONO SHOHEI 柔道-大野将平","lang":"zh","date":"2017-08-24T09:18:50.000Z","_content":"\n<center>![W2](/image/Judo/Budokan.jpg)</center> \n\n---------------------  \n\n## 写在前面\n    \n&#8195;&#8195;今年的4月29日在日本的武道馆观看了全日本柔道锦标赛，这是一个无重量级限制的比赛。那时记忆犹新的是一位奥运的柔道冠军跟比他约重20公斤的选手的比赛，那是整场耗时最长的比赛，不过后来奥运冠军被一本击败。\n&#8195;&#8195;后来才知道那位被击败的奥运冠军叫大野将平。\n&#8195;&#8195;在Youtube上发现了一部关于他的柔道旅程日语视频，影片来自日本节目片《アスリートの魂》（运动员之魂）。觉得对于柔道爱好者练习柔道也很有帮助，翻译为中文视频。四十多分钟有点长，翻译实属不易，希望能为推广柔道作出贡献。  \n&#8195;&#8195;视频里有一段是大野将平跟相扑选手日馬富士讨论关于“心技体”的内容，心的境界往往比技能和身体更重要。\n&#8195;&#8195;这里想顺便感谢吾辈的柔道启蒙教练深圳聚道馆的陈跃腾教练，以及在东京一直在细心指导吾辈的日本讲道馆的竹内先生。\n&#8195;&#8195;译者日语翻译经验尚浅，不足之处欢迎联系修改，不胜感激。  \n\n<center>![W2](/image/Judo/ONO SHOHEI.jpg)</center>\n\n---------------------  \n\n## 人物简介  \n\n大野将平 Ōno Shōhei，出生于1992年2月3日。\n生于山口县山口市，是一名日本柔道选手。\n他在2016年夏季奥林匹克运动会中获得男子73公斤级柔道金牌。 \n\n身高：1.74米（5英尺8 1⁄2英寸）\n体重：73千克（161英磅）  \n\n---------------------  \n\n## 视频链接 \n  \n  <center><embed height=\"415\" width=\"544\" quality=\"high\" allowfullscreen=\"true\" type=\"application/x-shockwave-flash\" src=\"//static.hdslb.com/miniloader.swf\" flashvars=\"aid=14169956&page=1\" pluginspage=\"//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash\"></embed></center>  \n  \n---------------------  \n\n&#8195;&#8195;[Bilibili视频链接 - H.J.T.](https://www.bilibili.com/video/av14169956/ \"Title\")\n&#8195;&#8195;[Youtube视频链接 - H.J.T.](https://youtu.be/0f87-PeTgYs \"Title\")\n","source":"_posts/zh/JUDO-ONO SHOHEI 柔道-大野将平.md","raw":"---\ntitle: JUDO-ONO SHOHEI 柔道-大野将平\nlang: zh\ndate: 2017-08-24 18:18:50\ntags: Judo\n---\n\n<center>![W2](/image/Judo/Budokan.jpg)</center> \n\n---------------------  \n\n## 写在前面\n    \n&#8195;&#8195;今年的4月29日在日本的武道馆观看了全日本柔道锦标赛，这是一个无重量级限制的比赛。那时记忆犹新的是一位奥运的柔道冠军跟比他约重20公斤的选手的比赛，那是整场耗时最长的比赛，不过后来奥运冠军被一本击败。\n&#8195;&#8195;后来才知道那位被击败的奥运冠军叫大野将平。\n&#8195;&#8195;在Youtube上发现了一部关于他的柔道旅程日语视频，影片来自日本节目片《アスリートの魂》（运动员之魂）。觉得对于柔道爱好者练习柔道也很有帮助，翻译为中文视频。四十多分钟有点长，翻译实属不易，希望能为推广柔道作出贡献。  \n&#8195;&#8195;视频里有一段是大野将平跟相扑选手日馬富士讨论关于“心技体”的内容，心的境界往往比技能和身体更重要。\n&#8195;&#8195;这里想顺便感谢吾辈的柔道启蒙教练深圳聚道馆的陈跃腾教练，以及在东京一直在细心指导吾辈的日本讲道馆的竹内先生。\n&#8195;&#8195;译者日语翻译经验尚浅，不足之处欢迎联系修改，不胜感激。  \n\n<center>![W2](/image/Judo/ONO SHOHEI.jpg)</center>\n\n---------------------  \n\n## 人物简介  \n\n大野将平 Ōno Shōhei，出生于1992年2月3日。\n生于山口县山口市，是一名日本柔道选手。\n他在2016年夏季奥林匹克运动会中获得男子73公斤级柔道金牌。 \n\n身高：1.74米（5英尺8 1⁄2英寸）\n体重：73千克（161英磅）  \n\n---------------------  \n\n## 视频链接 \n  \n  <center><embed height=\"415\" width=\"544\" quality=\"high\" allowfullscreen=\"true\" type=\"application/x-shockwave-flash\" src=\"//static.hdslb.com/miniloader.swf\" flashvars=\"aid=14169956&page=1\" pluginspage=\"//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash\"></embed></center>  \n  \n---------------------  \n\n&#8195;&#8195;[Bilibili视频链接 - H.J.T.](https://www.bilibili.com/video/av14169956/ \"Title\")\n&#8195;&#8195;[Youtube视频链接 - H.J.T.](https://youtu.be/0f87-PeTgYs \"Title\")\n","slug":"zh-JUDO-ONO-SHOHEI-柔道-大野将平","published":1,"updated":"2018-01-01T13:13:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yl000kog64npxts221","content":"<center><img src=\"/image/Judo/Budokan.jpg\" alt=\"W2\"></center> \n\n<hr>\n<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>&#8195;&#8195;今年的4月29日在日本的武道馆观看了全日本柔道锦标赛，这是一个无重量级限制的比赛。那时记忆犹新的是一位奥运的柔道冠军跟比他约重20公斤的选手的比赛，那是整场耗时最长的比赛，不过后来奥运冠军被一本击败。<br>&#8195;&#8195;后来才知道那位被击败的奥运冠军叫大野将平。<br>&#8195;&#8195;在Youtube上发现了一部关于他的柔道旅程日语视频，影片来自日本节目片《アスリートの魂》（运动员之魂）。觉得对于柔道爱好者练习柔道也很有帮助，翻译为中文视频。四十多分钟有点长，翻译实属不易，希望能为推广柔道作出贡献。<br>&#8195;&#8195;视频里有一段是大野将平跟相扑选手日馬富士讨论关于“心技体”的内容，心的境界往往比技能和身体更重要。<br>&#8195;&#8195;这里想顺便感谢吾辈的柔道启蒙教练深圳聚道馆的陈跃腾教练，以及在东京一直在细心指导吾辈的日本讲道馆的竹内先生。<br>&#8195;&#8195;译者日语翻译经验尚浅，不足之处欢迎联系修改，不胜感激。  </p>\n<center><img src=\"/image/Judo/ONO SHOHEI.jpg\" alt=\"W2\"></center>\n\n<hr>\n<h2 id=\"人物简介\"><a href=\"#人物简介\" class=\"headerlink\" title=\"人物简介\"></a>人物简介</h2><p>大野将平 Ōno Shōhei，出生于1992年2月3日。<br>生于山口县山口市，是一名日本柔道选手。<br>他在2016年夏季奥林匹克运动会中获得男子73公斤级柔道金牌。 </p>\n<p>身高：1.74米（5英尺8 1⁄2英寸）<br>体重：73千克（161英磅）  </p>\n<hr>\n<h2 id=\"视频链接\"><a href=\"#视频链接\" class=\"headerlink\" title=\"视频链接\"></a>视频链接</h2>  <center><embed height=\"415\" width=\"544\" quality=\"high\" allowfullscreen=\"true\" type=\"application/x-shockwave-flash\" src=\"//static.hdslb.com/miniloader.swf\" flashvars=\"aid=14169956&page=1\" pluginspage=\"//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash\"></center>  \n\n<hr>\n<p>&#8195;&#8195;<a href=\"https://www.bilibili.com/video/av14169956/\" title=\"Title\" target=\"_blank\" rel=\"external\">Bilibili视频链接 - H.J.T.</a><br>&#8195;&#8195;<a href=\"https://youtu.be/0f87-PeTgYs\" title=\"Title\" target=\"_blank\" rel=\"external\">Youtube视频链接 - H.J.T.</a></p>\n","excerpt":"","more":"<center><img src=\"/image/Judo/Budokan.jpg\" alt=\"W2\"></center> \n\n<hr>\n<h2 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h2><p>&#8195;&#8195;今年的4月29日在日本的武道馆观看了全日本柔道锦标赛，这是一个无重量级限制的比赛。那时记忆犹新的是一位奥运的柔道冠军跟比他约重20公斤的选手的比赛，那是整场耗时最长的比赛，不过后来奥运冠军被一本击败。<br>&#8195;&#8195;后来才知道那位被击败的奥运冠军叫大野将平。<br>&#8195;&#8195;在Youtube上发现了一部关于他的柔道旅程日语视频，影片来自日本节目片《アスリートの魂》（运动员之魂）。觉得对于柔道爱好者练习柔道也很有帮助，翻译为中文视频。四十多分钟有点长，翻译实属不易，希望能为推广柔道作出贡献。<br>&#8195;&#8195;视频里有一段是大野将平跟相扑选手日馬富士讨论关于“心技体”的内容，心的境界往往比技能和身体更重要。<br>&#8195;&#8195;这里想顺便感谢吾辈的柔道启蒙教练深圳聚道馆的陈跃腾教练，以及在东京一直在细心指导吾辈的日本讲道馆的竹内先生。<br>&#8195;&#8195;译者日语翻译经验尚浅，不足之处欢迎联系修改，不胜感激。  </p>\n<center><img src=\"/image/Judo/ONO SHOHEI.jpg\" alt=\"W2\"></center>\n\n<hr>\n<h2 id=\"人物简介\"><a href=\"#人物简介\" class=\"headerlink\" title=\"人物简介\"></a>人物简介</h2><p>大野将平 Ōno Shōhei，出生于1992年2月3日。<br>生于山口县山口市，是一名日本柔道选手。<br>他在2016年夏季奥林匹克运动会中获得男子73公斤级柔道金牌。 </p>\n<p>身高：1.74米（5英尺8 1⁄2英寸）<br>体重：73千克（161英磅）  </p>\n<hr>\n<h2 id=\"视频链接\"><a href=\"#视频链接\" class=\"headerlink\" title=\"视频链接\"></a>视频链接</h2>  <center><embed height=\"415\" width=\"544\" quality=\"high\" allowfullscreen=\"true\" type=\"application/x-shockwave-flash\" src=\"//static.hdslb.com/miniloader.swf\" flashvars=\"aid=14169956&page=1\" pluginspage=\"//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash\"></embed></center>  \n\n<hr>\n<p>&#8195;&#8195;<a href=\"https://www.bilibili.com/video/av14169956/\" title=\"Title\">Bilibili视频链接 - H.J.T.</a><br>&#8195;&#8195;<a href=\"https://youtu.be/0f87-PeTgYs\" title=\"Title\">Youtube视频链接 - H.J.T.</a></p>\n"},{"title":"Mac OS X设置默认Python版本","date":"2017-01-04T03:59:30.000Z","_content":"想在Mac OS X设置默认Python版本，系统自带是2.7，自装的是3.5. 以下方法：\n\n### Method 1.修改链接\n先备份原来的python链接，再把3.5的解释器做一个链接到原目录下，适当修改相关路径：\n``` bash\nsudo ln -s /System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5 /usr/local/bin/python\n```\n但是建议不要用链接方法，因为可能会导致想调用自带python的软件出现问题。\n\n### Method 2.修改环境变量\n可以修改~/.bash_profile，修改path variable虽然比较安全。\n找不到.bash_profile文件的可以使用find查找：\n``` bash\nfind / -name .bash_profile \n```\n然后在bash_profile文件添加下行（具体path取决python3安装路径）：\n``` bash\nalias python=\"/System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5\"\n```\n然后重启一下Terminal。\n\n### Method 3.直接加版本号\n最便捷的方法是：\n如果想用系统自带python2.7，就在命令行输入“python”；\n如果想用python3.5，就在命令行输入“python3”。\n吾辈最终还是觉得这方法最合适。\n\n\n### Python路径相关：\n① Mac自带的python环境在(Python 2.7.10)：\n``` bash\n/System/Library/Frameworks/Python.framework/Versions/2.7\n```\n② 用户安装的python环境在(Python 3.5.2)：\n``` bash\n/Library/Frameworks/Python.framework/Versions/3.5\n```\n③ Mac自带的python解释器默认路径在(Python 2.7.10)：\n``` bash\n/usr/bin/python2.7\n```\n④ 用户安装的python解释器默认路径在(Python 3.5.2)：\n``` bash\n/usr/local/bin/python3.5\n```\n","source":"_posts/zh/Mac OS X设置默认Python版本.md","raw":"---\ntitle: Mac OS X设置默认Python版本\ndate: 2017-01-04 12:59:30\n---\n想在Mac OS X设置默认Python版本，系统自带是2.7，自装的是3.5. 以下方法：\n\n### Method 1.修改链接\n先备份原来的python链接，再把3.5的解释器做一个链接到原目录下，适当修改相关路径：\n``` bash\nsudo ln -s /System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5 /usr/local/bin/python\n```\n但是建议不要用链接方法，因为可能会导致想调用自带python的软件出现问题。\n\n### Method 2.修改环境变量\n可以修改~/.bash_profile，修改path variable虽然比较安全。\n找不到.bash_profile文件的可以使用find查找：\n``` bash\nfind / -name .bash_profile \n```\n然后在bash_profile文件添加下行（具体path取决python3安装路径）：\n``` bash\nalias python=\"/System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5\"\n```\n然后重启一下Terminal。\n\n### Method 3.直接加版本号\n最便捷的方法是：\n如果想用系统自带python2.7，就在命令行输入“python”；\n如果想用python3.5，就在命令行输入“python3”。\n吾辈最终还是觉得这方法最合适。\n\n\n### Python路径相关：\n① Mac自带的python环境在(Python 2.7.10)：\n``` bash\n/System/Library/Frameworks/Python.framework/Versions/2.7\n```\n② 用户安装的python环境在(Python 3.5.2)：\n``` bash\n/Library/Frameworks/Python.framework/Versions/3.5\n```\n③ Mac自带的python解释器默认路径在(Python 2.7.10)：\n``` bash\n/usr/bin/python2.7\n```\n④ 用户安装的python解释器默认路径在(Python 3.5.2)：\n``` bash\n/usr/local/bin/python3.5\n```\n","slug":"zh-Mac-OS-X设置默认Python版本","published":1,"updated":"2017-01-04T07:23:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8ym000mog64mkyp6ab4","content":"<p>想在Mac OS X设置默认Python版本，系统自带是2.7，自装的是3.5. 以下方法：</p>\n<h3 id=\"Method-1-修改链接\"><a href=\"#Method-1-修改链接\" class=\"headerlink\" title=\"Method 1.修改链接\"></a>Method 1.修改链接</h3><p>先备份原来的python链接，再把3.5的解释器做一个链接到原目录下，适当修改相关路径：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ln <span class=\"_\">-s</span> /System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5 /usr/<span class=\"built_in\">local</span>/bin/python</div></pre></td></tr></table></figure></p>\n<p>但是建议不要用链接方法，因为可能会导致想调用自带python的软件出现问题。</p>\n<h3 id=\"Method-2-修改环境变量\"><a href=\"#Method-2-修改环境变量\" class=\"headerlink\" title=\"Method 2.修改环境变量\"></a>Method 2.修改环境变量</h3><p>可以修改~/.bash_profile，修改path variable虽然比较安全。<br>找不到.bash_profile文件的可以使用find查找：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -name .bash_profile</div></pre></td></tr></table></figure></p>\n<p>然后在bash_profile文件添加下行（具体path取决python3安装路径）：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">alias</span> python=<span class=\"string\">\"/System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5\"</span></div></pre></td></tr></table></figure></p>\n<p>然后重启一下Terminal。</p>\n<h3 id=\"Method-3-直接加版本号\"><a href=\"#Method-3-直接加版本号\" class=\"headerlink\" title=\"Method 3.直接加版本号\"></a>Method 3.直接加版本号</h3><p>最便捷的方法是：<br>如果想用系统自带python2.7，就在命令行输入“python”；<br>如果想用python3.5，就在命令行输入“python3”。<br>吾辈最终还是觉得这方法最合适。</p>\n<h3 id=\"Python路径相关：\"><a href=\"#Python路径相关：\" class=\"headerlink\" title=\"Python路径相关：\"></a>Python路径相关：</h3><p>① Mac自带的python环境在(Python 2.7.10)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/System/Library/Frameworks/Python.framework/Versions/2.7</div></pre></td></tr></table></figure></p>\n<p>② 用户安装的python环境在(Python 3.5.2)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/Library/Frameworks/Python.framework/Versions/3.5</div></pre></td></tr></table></figure></p>\n<p>③ Mac自带的python解释器默认路径在(Python 2.7.10)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/bin/python2.7</div></pre></td></tr></table></figure></p>\n<p>④ 用户安装的python解释器默认路径在(Python 3.5.2)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/<span class=\"built_in\">local</span>/bin/python3.5</div></pre></td></tr></table></figure></p>\n","excerpt":"","more":"<p>想在Mac OS X设置默认Python版本，系统自带是2.7，自装的是3.5. 以下方法：</p>\n<h3 id=\"Method-1-修改链接\"><a href=\"#Method-1-修改链接\" class=\"headerlink\" title=\"Method 1.修改链接\"></a>Method 1.修改链接</h3><p>先备份原来的python链接，再把3.5的解释器做一个链接到原目录下，适当修改相关路径：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo ln <span class=\"_\">-s</span> /System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5 /usr/<span class=\"built_in\">local</span>/bin/python</div></pre></td></tr></table></figure></p>\n<p>但是建议不要用链接方法，因为可能会导致想调用自带python的软件出现问题。</p>\n<h3 id=\"Method-2-修改环境变量\"><a href=\"#Method-2-修改环境变量\" class=\"headerlink\" title=\"Method 2.修改环境变量\"></a>Method 2.修改环境变量</h3><p>可以修改~/.bash_profile，修改path variable虽然比较安全。<br>找不到.bash_profile文件的可以使用find查找：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -name .bash_profile</div></pre></td></tr></table></figure></p>\n<p>然后在bash_profile文件添加下行（具体path取决python3安装路径）：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">alias</span> python=<span class=\"string\">\"/System/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5\"</span></div></pre></td></tr></table></figure></p>\n<p>然后重启一下Terminal。</p>\n<h3 id=\"Method-3-直接加版本号\"><a href=\"#Method-3-直接加版本号\" class=\"headerlink\" title=\"Method 3.直接加版本号\"></a>Method 3.直接加版本号</h3><p>最便捷的方法是：<br>如果想用系统自带python2.7，就在命令行输入“python”；<br>如果想用python3.5，就在命令行输入“python3”。<br>吾辈最终还是觉得这方法最合适。</p>\n<h3 id=\"Python路径相关：\"><a href=\"#Python路径相关：\" class=\"headerlink\" title=\"Python路径相关：\"></a>Python路径相关：</h3><p>① Mac自带的python环境在(Python 2.7.10)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/System/Library/Frameworks/Python.framework/Versions/2.7</div></pre></td></tr></table></figure></p>\n<p>② 用户安装的python环境在(Python 3.5.2)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/Library/Frameworks/Python.framework/Versions/3.5</div></pre></td></tr></table></figure></p>\n<p>③ Mac自带的python解释器默认路径在(Python 2.7.10)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/bin/python2.7</div></pre></td></tr></table></figure></p>\n<p>④ 用户安装的python解释器默认路径在(Python 3.5.2)：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/usr/<span class=\"built_in\">local</span>/bin/python3.5</div></pre></td></tr></table></figure></p>\n"},{"title":"Machine learning笔记1-Python","lang":"zh","date":"2017-07-27T16:00:00.000Z","_content":"<center>![pic](/image/ML/1_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/1_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/1_3.jpg)</center>  \n","source":"_posts/zh/Machine learning笔记1-Python.md","raw":"\n---\ntitle: Machine learning笔记1-Python\nlang: zh\ndate: 2017-07-28 01:00:00\ntags: Machine Learning\n---\n<center>![pic](/image/ML/1_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/1_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/1_3.jpg)</center>  \n","slug":"zh-Machine-learning笔记1-Python","published":1,"updated":"2018-01-01T13:14:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yn000oog64sy873j2v","content":"<center><img src=\"/image/ML/1_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/1_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/1_3.jpg\" alt=\"pic\"></center>  \n","excerpt":"","more":"<center><img src=\"/image/ML/1_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/1_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/1_3.jpg\" alt=\"pic\"></center>  \n"},{"title":"Machine learning笔记2-Statistic","lang":"zh","date":"2017-07-27T17:00:00.000Z","_content":"<center>![pic](/image/ML/2_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_3.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_4.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_5.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_6.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_7.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_8.jpg)</center>  \n","source":"_posts/zh/Machine learning笔记2-Statistic.md","raw":"---\ntitle: Machine learning笔记2-Statistic\nlang: zh\ndate: 2017-07-28 02:00:00\ntags: Machine Learning\n---\n<center>![pic](/image/ML/2_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_3.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_4.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_5.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_6.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_7.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/2_8.jpg)</center>  \n","slug":"zh-Machine-learning笔记2-Statistic","published":1,"updated":"2018-01-01T13:14:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yn000qog64xtrg1j9j","content":"<center><img src=\"/image/ML/2_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_3.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_4.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_5.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_6.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_7.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_8.jpg\" alt=\"pic\"></center>  \n","excerpt":"","more":"<center><img src=\"/image/ML/2_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_3.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_4.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_5.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_6.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_7.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/2_8.jpg\" alt=\"pic\"></center>  \n"},{"title":"Machine learning笔记3-Linear algebra","lang":"zh","date":"2017-07-27T18:00:00.000Z","_content":"<center>![pic](/image/ML/3_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_3.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_4.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_5.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_6.jpg)</center>  \n","source":"_posts/zh/Machine learning笔记3-Linear algebra.md","raw":"\n---\ntitle: Machine learning笔记3-Linear algebra\nlang: zh\ndate: 2017-07-28 03:00:00\ntags: Machine Learning\n---\n<center>![pic](/image/ML/3_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_2.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_3.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_4.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_5.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/3_6.jpg)</center>  \n","slug":"zh-Machine-learning笔记3-Linear-algebra","published":1,"updated":"2018-01-01T13:14:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yo000sog64era89nlw","content":"<center><img src=\"/image/ML/3_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_3.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_4.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_5.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_6.jpg\" alt=\"pic\"></center>  \n","excerpt":"","more":"<center><img src=\"/image/ML/3_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_2.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_3.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_4.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_5.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/3_6.jpg\" alt=\"pic\"></center>  \n"},{"title":"Machine learning笔记4-Data Analysis","lang":"zh","date":"2017-07-27T19:00:00.000Z","_content":"<center>![pic](/image/ML/4_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/4_2.jpg)</center>  \n","source":"_posts/zh/Machine learning笔记4-Data Analysis.md","raw":"\n---\ntitle: Machine learning笔记4-Data Analysis\nlang: zh\ndate: 2017-07-28 04:00:00\ntags: Machine Learning\n---\n<center>![pic](/image/ML/4_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/4_2.jpg)</center>  \n","slug":"zh-Machine-learning笔记4-Data-Analysis","published":1,"updated":"2018-01-01T13:14:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yp000uog64e4iy38jx","content":"<center><img src=\"/image/ML/4_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/4_2.jpg\" alt=\"pic\"></center>  \n","excerpt":"","more":"<center><img src=\"/image/ML/4_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/4_2.jpg\" alt=\"pic\"></center>  \n"},{"title":"Machine learning笔记5-Model","lang":"zh","date":"2017-07-27T20:00:00.000Z","_content":"<center>![pic](/image/ML/5_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/5_2.jpg)</center>  \n\n\n","source":"_posts/zh/Machine learning笔记5-Model.md","raw":"\n---\ntitle: Machine learning笔记5-Model\nlang: zh\ndate: 2017-07-28 05:00:00\ntags: Machine Learning\n---\n<center>![pic](/image/ML/5_1.jpg)</center>  \n\n--------------------------------\n\n<center>![pic](/image/ML/5_2.jpg)</center>  \n\n\n","slug":"zh-Machine-learning笔记5-Model","published":1,"updated":"2018-01-01T13:14:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yr000wog640t5n4hyu","content":"<center><img src=\"/image/ML/5_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/5_2.jpg\" alt=\"pic\"></center>  \n\n\n","excerpt":"","more":"<center><img src=\"/image/ML/5_1.jpg\" alt=\"pic\"></center>  \n\n<hr>\n<center><img src=\"/image/ML/5_2.jpg\" alt=\"pic\"></center>  \n\n\n"},{"title":"Reinforcement Learning笔记1-MDP","lang":"zh","date":"2017-11-11T09:18:56.000Z","_content":"\n### 1. 马尔可夫模型介绍（Markov）\n\n马尔可夫模型的几类子模型:\n\n| 　             |      不考虑动作     |               考虑动作              |\n|----------------|:-------------------:|:-----------------------------------:|\n| 状态完全可见   | 马尔科夫链(MC)      | 马尔可夫决策过程(MDP)               |\n| 状态不完全可见 | 隐马尔可夫模型(HMM) | 不完全可观察马尔可夫决策过程(POMDP) |\n\n- Markdown table - [tablesgenerator](http://www.tablesgenerator.com/markdown_tables \"Title\") \n\n### 2. 马尔可夫决策过程（MDP）\n\n马尔可夫决策过程（Markov Decision Processes, MDP）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。\n\nMDP 的策略完全取决于当前状态（Only present matters），可以简单表示为： \n  \nM = ( S, A, P{s,a}, R )\n \n ① $s \\in S$: 有限状态 state 集合，s 表示某个特定状态\n \n ② $a \\in A$: 有限动作 action 集合，a 表示某个特定动作\n \n ③ Transition Model $T(S, a, S') \\sim P_r(s'|s, a)$: Transition Model, 根据当前状态 s 和动作 a 预测下一个状态 $s’$，这里的 $P_r$ 表示从 s 采取行动 a 转移到 $s’$ 的概率\n \n ④ Reward $R(s, a) = E[R_{t+1}|s, a]$:表示 agent 采取某个动作后的即时奖励，它还有 R(s, a, s’), R(s) 等表现形式，采用不同的形式，其意义略有不同\n \n ⑤ Policy $\\pi(s) \\to a$: 根据当前 state 来产生 action，可表现为 $a=\\pi(s)$ 或 $\\pi(a|s) = P[a|s]$，后者表示某种状态下执行某个动作的概率\n\n### 3. 回报\n\nU(s0,s1,s2…) 与 折扣率（discount）y: U 代表执行一组 action 后所有状态累计的 reward 之和，但由于直接的 reward 相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个 Utility 函数里引入 y 折扣率这一概念，令往后的状态所反馈回来的 reward 乘上这个 discount 系数，这样意味着当下的 reward 比未来反馈的 reward 更重要，这也比较符合直觉。定义：\n\n\n\\begin{align}\nU(s_0\\,s_1\\,s_2\\,\\cdots) &= \\sum_{t=0}^{\\infty}{\\gamma^tR(s_t)} \\quad 0\\le\\gamma<1 \\\\\n    &\\le \\sum_{t=0}^{\\infty }{\\gamma^tR_{max}} = \\frac{R_{max}}{1-\\gamma}\n\\end{align}\n    \n由于引入了 discount，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。\n\n强化学习的目的是最大化长期未来奖励，即寻找最大的 U。\n\n于回报（return），再引入两个函数：\n\n ① 状态价值函数：\n \n\\begin{align}\n v(s)=E[U_t|S_t=s]\n\\end{align}\n \n 意义为基于 t 时刻的状态 s 能获得的未来回报（return）的期望，加入动作选择策略后可表示为 \n \n\\begin{align}\n v_{\\pi}(s)=E_{\\pi}[U_t|S_t=s](U_t=R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1}R_T)\n\\end{align}\n\n ② 动作价值函数：\n \n\\begin{align}\n q_{\\pi}=E_{\\pi}[U_t|S_t=s,\\,A_t=a]\n\\end{align}\n  \n意义为基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望。\n\n- 价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作\n\n### 4. MDP 求解\n\n需要找到最优的策略使未来回报最大化，求解过程大致可分为两步：\n\n ① 预测：给定策略，评估相应的状态价值函数和状态-动作价值函数\n \n ② 行动：根据价值函数得到当前状态对应的最优动作\n\n- 推荐课程 - [CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/ \"Title\")\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n","source":"_posts/zh/Reinforcement Learning笔记1-MDP.md","raw":"\n---\ntitle: Reinforcement Learning笔记1-MDP\nlang: zh\ndate: 2017-11-11 18:18:56\ntags: Reinforcement Learning\n---\n\n### 1. 马尔可夫模型介绍（Markov）\n\n马尔可夫模型的几类子模型:\n\n| 　             |      不考虑动作     |               考虑动作              |\n|----------------|:-------------------:|:-----------------------------------:|\n| 状态完全可见   | 马尔科夫链(MC)      | 马尔可夫决策过程(MDP)               |\n| 状态不完全可见 | 隐马尔可夫模型(HMM) | 不完全可观察马尔可夫决策过程(POMDP) |\n\n- Markdown table - [tablesgenerator](http://www.tablesgenerator.com/markdown_tables \"Title\") \n\n### 2. 马尔可夫决策过程（MDP）\n\n马尔可夫决策过程（Markov Decision Processes, MDP）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。\n\nMDP 的策略完全取决于当前状态（Only present matters），可以简单表示为： \n  \nM = ( S, A, P{s,a}, R )\n \n ① $s \\in S$: 有限状态 state 集合，s 表示某个特定状态\n \n ② $a \\in A$: 有限动作 action 集合，a 表示某个特定动作\n \n ③ Transition Model $T(S, a, S') \\sim P_r(s'|s, a)$: Transition Model, 根据当前状态 s 和动作 a 预测下一个状态 $s’$，这里的 $P_r$ 表示从 s 采取行动 a 转移到 $s’$ 的概率\n \n ④ Reward $R(s, a) = E[R_{t+1}|s, a]$:表示 agent 采取某个动作后的即时奖励，它还有 R(s, a, s’), R(s) 等表现形式，采用不同的形式，其意义略有不同\n \n ⑤ Policy $\\pi(s) \\to a$: 根据当前 state 来产生 action，可表现为 $a=\\pi(s)$ 或 $\\pi(a|s) = P[a|s]$，后者表示某种状态下执行某个动作的概率\n\n### 3. 回报\n\nU(s0,s1,s2…) 与 折扣率（discount）y: U 代表执行一组 action 后所有状态累计的 reward 之和，但由于直接的 reward 相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个 Utility 函数里引入 y 折扣率这一概念，令往后的状态所反馈回来的 reward 乘上这个 discount 系数，这样意味着当下的 reward 比未来反馈的 reward 更重要，这也比较符合直觉。定义：\n\n\n\\begin{align}\nU(s_0\\,s_1\\,s_2\\,\\cdots) &= \\sum_{t=0}^{\\infty}{\\gamma^tR(s_t)} \\quad 0\\le\\gamma<1 \\\\\n    &\\le \\sum_{t=0}^{\\infty }{\\gamma^tR_{max}} = \\frac{R_{max}}{1-\\gamma}\n\\end{align}\n    \n由于引入了 discount，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。\n\n强化学习的目的是最大化长期未来奖励，即寻找最大的 U。\n\n于回报（return），再引入两个函数：\n\n ① 状态价值函数：\n \n\\begin{align}\n v(s)=E[U_t|S_t=s]\n\\end{align}\n \n 意义为基于 t 时刻的状态 s 能获得的未来回报（return）的期望，加入动作选择策略后可表示为 \n \n\\begin{align}\n v_{\\pi}(s)=E_{\\pi}[U_t|S_t=s](U_t=R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1}R_T)\n\\end{align}\n\n ② 动作价值函数：\n \n\\begin{align}\n q_{\\pi}=E_{\\pi}[U_t|S_t=s,\\,A_t=a]\n\\end{align}\n  \n意义为基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望。\n\n- 价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作\n\n### 4. MDP 求解\n\n需要找到最优的策略使未来回报最大化，求解过程大致可分为两步：\n\n ① 预测：给定策略，评估相应的状态价值函数和状态-动作价值函数\n \n ② 行动：根据价值函数得到当前状态对应的最优动作\n\n- 推荐课程 - [CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/ \"Title\")\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n","slug":"zh-Reinforcement-Learning笔记1-MDP","published":1,"updated":"2018-01-01T13:15:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yr000yog64nbnppcs9","content":"<h3 id=\"1-马尔可夫模型介绍（Markov）\"><a href=\"#1-马尔可夫模型介绍（Markov）\" class=\"headerlink\" title=\"1. 马尔可夫模型介绍（Markov）\"></a>1. 马尔可夫模型介绍（Markov）</h3><p>马尔可夫模型的几类子模型:</p>\n<table>\n<thead>\n<tr>\n<th>　</th>\n<th style=\"text-align:center\">不考虑动作</th>\n<th style=\"text-align:center\">考虑动作</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>状态完全可见</td>\n<td style=\"text-align:center\">马尔科夫链(MC)</td>\n<td style=\"text-align:center\">马尔可夫决策过程(MDP)</td>\n</tr>\n<tr>\n<td>状态不完全可见</td>\n<td style=\"text-align:center\">隐马尔可夫模型(HMM)</td>\n<td style=\"text-align:center\">不完全可观察马尔可夫决策过程(POMDP)</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Markdown table - <a href=\"http://www.tablesgenerator.com/markdown_tables\" title=\"Title\" target=\"_blank\" rel=\"external\">tablesgenerator</a> </li>\n</ul>\n<h3 id=\"2-马尔可夫决策过程（MDP）\"><a href=\"#2-马尔可夫决策过程（MDP）\" class=\"headerlink\" title=\"2. 马尔可夫决策过程（MDP）\"></a>2. 马尔可夫决策过程（MDP）</h3><p>马尔可夫决策过程（Markov Decision Processes, MDP）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。</p>\n<p>MDP 的策略完全取决于当前状态（Only present matters），可以简单表示为： </p>\n<p>M = ( S, A, P{s,a}, R )</p>\n<p> ① $s \\in S$: 有限状态 state 集合，s 表示某个特定状态</p>\n<p> ② $a \\in A$: 有限动作 action 集合，a 表示某个特定动作</p>\n<p> ③ Transition Model $T(S, a, S’) \\sim P_r(s’|s, a)$: Transition Model, 根据当前状态 s 和动作 a 预测下一个状态 $s’$，这里的 $P_r$ 表示从 s 采取行动 a 转移到 $s’$ 的概率</p>\n<p> ④ Reward $R(s, a) = E[R_{t+1}|s, a]$:表示 agent 采取某个动作后的即时奖励，它还有 R(s, a, s’), R(s) 等表现形式，采用不同的形式，其意义略有不同</p>\n<p> ⑤ Policy $\\pi(s) \\to a$: 根据当前 state 来产生 action，可表现为 $a=\\pi(s)$ 或 $\\pi(a|s) = P[a|s]$，后者表示某种状态下执行某个动作的概率</p>\n<h3 id=\"3-回报\"><a href=\"#3-回报\" class=\"headerlink\" title=\"3. 回报\"></a>3. 回报</h3><p>U(s0,s1,s2…) 与 折扣率（discount）y: U 代表执行一组 action 后所有状态累计的 reward 之和，但由于直接的 reward 相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个 Utility 函数里引入 y 折扣率这一概念，令往后的状态所反馈回来的 reward 乘上这个 discount 系数，这样意味着当下的 reward 比未来反馈的 reward 更重要，这也比较符合直觉。定义：</p>\n<p>\\begin{align}<br>U(s_0\\,s_1\\,s_2\\,\\cdots) &amp;= \\sum_{t=0}^{\\infty}{\\gamma^tR(s_t)} \\quad 0\\le\\gamma&lt;1 \\\\<br>    &amp;\\le \\sum_{t=0}^{\\infty }{\\gamma^tR_{max}} = \\frac{R_{max}}{1-\\gamma}<br>\\end{align}</p>\n<p>由于引入了 discount，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。</p>\n<p>强化学习的目的是最大化长期未来奖励，即寻找最大的 U。</p>\n<p>于回报（return），再引入两个函数：</p>\n<p> ① 状态价值函数：</p>\n<p>\\begin{align}<br> v(s)=E[U_t|S_t=s]<br>\\end{align}</p>\n<p> 意义为基于 t 时刻的状态 s 能获得的未来回报（return）的期望，加入动作选择策略后可表示为 </p>\n<p>\\begin{align}<br> v_{\\pi}(s)=E_{\\pi}<a href=\"U_t=R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1}R_T\">U_t|S_t=s</a><br>\\end{align}</p>\n<p> ② 动作价值函数：</p>\n<p>\\begin{align}<br> q_{\\pi}=E_{\\pi}[U_t|S_t=s,\\,A_t=a]<br>\\end{align}</p>\n<p>意义为基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望。</p>\n<ul>\n<li>价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作</li>\n</ul>\n<h3 id=\"4-MDP-求解\"><a href=\"#4-MDP-求解\" class=\"headerlink\" title=\"4. MDP 求解\"></a>4. MDP 求解</h3><p>需要找到最优的策略使未来回报最大化，求解过程大致可分为两步：</p>\n<p> ① 预测：给定策略，评估相应的状态价值函数和状态-动作价值函数</p>\n<p> ② 行动：根据价值函数得到当前状态对应的最优动作</p>\n<ul>\n<li>推荐课程 - <a href=\"http://rll.berkeley.edu/deeprlcourse/\" title=\"Title\" target=\"_blank\" rel=\"external\">CS 294: Deep Reinforcement Learning</a></li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\" target=\"_blank\" rel=\"external\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\" target=\"_blank\" rel=\"external\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\" target=\"_blank\" rel=\"external\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习知识整理</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-马尔可夫模型介绍（Markov）\"><a href=\"#1-马尔可夫模型介绍（Markov）\" class=\"headerlink\" title=\"1. 马尔可夫模型介绍（Markov）\"></a>1. 马尔可夫模型介绍（Markov）</h3><p>马尔可夫模型的几类子模型:</p>\n<table>\n<thead>\n<tr>\n<th>　</th>\n<th style=\"text-align:center\">不考虑动作</th>\n<th style=\"text-align:center\">考虑动作</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>状态完全可见</td>\n<td style=\"text-align:center\">马尔科夫链(MC)</td>\n<td style=\"text-align:center\">马尔可夫决策过程(MDP)</td>\n</tr>\n<tr>\n<td>状态不完全可见</td>\n<td style=\"text-align:center\">隐马尔可夫模型(HMM)</td>\n<td style=\"text-align:center\">不完全可观察马尔可夫决策过程(POMDP)</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Markdown table - <a href=\"http://www.tablesgenerator.com/markdown_tables\" title=\"Title\">tablesgenerator</a> </li>\n</ul>\n<h3 id=\"2-马尔可夫决策过程（MDP）\"><a href=\"#2-马尔可夫决策过程（MDP）\" class=\"headerlink\" title=\"2. 马尔可夫决策过程（MDP）\"></a>2. 马尔可夫决策过程（MDP）</h3><p>马尔可夫决策过程（Markov Decision Processes, MDP）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。</p>\n<p>MDP 的策略完全取决于当前状态（Only present matters），可以简单表示为： </p>\n<p>M = ( S, A, P{s,a}, R )</p>\n<p> ① $s \\in S$: 有限状态 state 集合，s 表示某个特定状态</p>\n<p> ② $a \\in A$: 有限动作 action 集合，a 表示某个特定动作</p>\n<p> ③ Transition Model $T(S, a, S’) \\sim P_r(s’|s, a)$: Transition Model, 根据当前状态 s 和动作 a 预测下一个状态 $s’$，这里的 $P_r$ 表示从 s 采取行动 a 转移到 $s’$ 的概率</p>\n<p> ④ Reward $R(s, a) = E[R_{t+1}|s, a]$:表示 agent 采取某个动作后的即时奖励，它还有 R(s, a, s’), R(s) 等表现形式，采用不同的形式，其意义略有不同</p>\n<p> ⑤ Policy $\\pi(s) \\to a$: 根据当前 state 来产生 action，可表现为 $a=\\pi(s)$ 或 $\\pi(a|s) = P[a|s]$，后者表示某种状态下执行某个动作的概率</p>\n<h3 id=\"3-回报\"><a href=\"#3-回报\" class=\"headerlink\" title=\"3. 回报\"></a>3. 回报</h3><p>U(s0,s1,s2…) 与 折扣率（discount）y: U 代表执行一组 action 后所有状态累计的 reward 之和，但由于直接的 reward 相加在无限时间序列中会导致无偏向，而且会产生状态的无限循环。因此在这个 Utility 函数里引入 y 折扣率这一概念，令往后的状态所反馈回来的 reward 乘上这个 discount 系数，这样意味着当下的 reward 比未来反馈的 reward 更重要，这也比较符合直觉。定义：</p>\n<p>\\begin{align}<br>U(s_0\\,s_1\\,s_2\\,\\cdots) &amp;= \\sum_{t=0}^{\\infty}{\\gamma^tR(s_t)} \\quad 0\\le\\gamma&lt;1 \\\\<br>    &amp;\\le \\sum_{t=0}^{\\infty }{\\gamma^tR_{max}} = \\frac{R_{max}}{1-\\gamma}<br>\\end{align}</p>\n<p>由于引入了 discount，可以看到我们把一个无限长度的问题转换成了一个拥有最大值上限的问题。</p>\n<p>强化学习的目的是最大化长期未来奖励，即寻找最大的 U。</p>\n<p>于回报（return），再引入两个函数：</p>\n<p> ① 状态价值函数：</p>\n<p>\\begin{align}<br> v(s)=E[U_t|S_t=s]<br>\\end{align}</p>\n<p> 意义为基于 t 时刻的状态 s 能获得的未来回报（return）的期望，加入动作选择策略后可表示为 </p>\n<p>\\begin{align}<br> v_{\\pi}(s)=E_{\\pi}<a href=\"U_t=R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^{T-t-1}R_T\">U_t|S_t=s</a><br>\\end{align}</p>\n<p> ② 动作价值函数：</p>\n<p>\\begin{align}<br> q_{\\pi}=E_{\\pi}[U_t|S_t=s,\\,A_t=a]<br>\\end{align}</p>\n<p>意义为基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望。</p>\n<ul>\n<li>价值函数用来衡量某一状态或动作-状态的优劣，即对智能体来说是否值得选择某一状态或在某一状态下执行某一动作</li>\n</ul>\n<h3 id=\"4-MDP-求解\"><a href=\"#4-MDP-求解\" class=\"headerlink\" title=\"4. MDP 求解\"></a>4. MDP 求解</h3><p>需要找到最优的策略使未来回报最大化，求解过程大致可分为两步：</p>\n<p> ① 预测：给定策略，评估相应的状态价值函数和状态-动作价值函数</p>\n<p> ② 行动：根据价值函数得到当前状态对应的最优动作</p>\n<ul>\n<li>推荐课程 - <a href=\"http://rll.berkeley.edu/deeprlcourse/\" title=\"Title\">CS 294: Deep Reinforcement Learning</a></li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\">强化学习知识整理</a> </p>\n</li>\n</ul>\n"},{"title":"Reinforcement Learning笔记2-Bellman","lang":"zh","date":"2017-11-12T09:18:56.000Z","_content":"\n### 1. Bellman方程（Bellman Equation）\n\n贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。\n\n贝尔曼方程表明当前状态的值函数与下个状态的值函数的关系。\n\n见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/1.jpg)</center> \n\n-------------------------------------\n\n根据上图得出状态价值函数公式：\n\n\\begin{align}    \nv_{\\pi}(s)=E[U_t|S_t=s]=\\sum_{a\\in A}\\pi(a|s)\\,\\left( R_s^a+\\gamma \\sum_{s'\\in S}P^a_{ss'}v_{\\pi}(s')\\right)\n\\end{align}\n\n其中：\n\n\\begin{align}    \nP_{ss’}^a = P(S_{t+1}=s'|S_t=s, A_t=a)\n\\end{align}\n\n上式中策略π是指给定状态 s 的情况下，动作 a 的概率分布，即：\n\n\\begin{align}    \n\\pi(a|s)=P(a|s)\n\\end{align}\n\n将概率和转换为期望，上式等价于：\n\n\\begin{align}    \nv_{\\pi}(s) = E_{\\pi}[R_s^a + \\gamma v_{\\pi}(S_{t+1}|S_t=s]\n\\end{align}\n\n同理，我们可以得到动作价值函数的公式如下：\n\n\\begin{align}    \nq_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]\n\\end{align}\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/2.jpg)</center> \n\n-------------------------------------\n如上图，Bellman 方程也可以表达成矩阵形式：\n\n\\begin{align}    \nv=R+\\gamma Pv\n\\end{align}\n\n可直接求出：\n\n\\begin{align}    \nv=(I-\\gamma P)^{-1}R\n\\end{align}\n\n其复杂度为：\n\n\\begin{align}    \nO(n^3)\n\\end{align}\n\n一般可通过动态规划、时间差分法与蒙特卡洛估计求解。\n\n● 通过解 Bellman 最优性方程找一个最优策略需要以下条件:\n\n- 动态模型已知\n\n- 拥有足够的计算空间和时间\n\n- 系统满足 Markov 特性\n\n### 2. 状态价值函数和动作价值函数的关系\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/3.jpg)</center> \n\n-------------------------------------\n\n\\begin{align}    \nv_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) = E[q_{\\pi}(s,a)|S_t=s] \n\\end{align}\n\n\\begin{align}    \nq_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a \\sum_{a' \\in A}\\pi(a'|s')q_{\\pi}(s',a') = R_s^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')\n\\end{align}\n\n### 3. 最优方程\n\n\\begin{align}    \nv_*(s)= \\max_aq_*(s,a)= \\max_a\\left( R_s^a + \\gamma\\sum_{s'\\in S}P_{ss'}^a v_*(s') \\right) \\\\\nq_*(s,a)= R_s^a+\\gamma \\sum_{s'\\in S}P_{ss'}^av_*(s') = R_s^a + \\gamma \\sum_{s’ \\in S}P_{ss'}^a\\max_{a’}q_*(s',a')\n\\end{align}\n\n● v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值\n\n● q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值\n\n### 4. 最优策略\n\n关于收敛性：（对策略定义一个偏序）\n\n$\\pi \\ge \\pi' \\,\\mbox{if}\\; v_{\\pi}(s)\\ge v_{\\pi'}(s),\\forall s$\n\n定理：\n\n对于任意 MDP：\n\n- 总是存在一个最优策略π*，它比其它任何策略都要好，或者至少一样好\n\n- 所有最优决策都达到最优值函数， $v_{\\pi_*}(s)=v_*(s)$\n\n- 所有最优决策都达到最优行动值函数，$q_{\\pi_*}(s,a)=q_*(s,a)$\n\n最优策略可从最优状态价值函数或者最优动作价值函数得出：\n\n\\begin{align} \n\\pi_*(a|s) = \n\\begin{cases}\n1, & \\mbox{if } a = \\arg\\max_{a \\in A} q_*(s,a)\\\\\n0, & otherwise\n\\end{cases}\n\\end{align}\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n","source":"_posts/zh/Reinforcement Learning笔记2-Bellman.md","raw":"\n---\ntitle: Reinforcement Learning笔记2-Bellman\nlang: zh\ndate: 2017-11-12 18:18:56\ntags: Reinforcement Learning\n---\n\n### 1. Bellman方程（Bellman Equation）\n\n贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。\n\n贝尔曼方程表明当前状态的值函数与下个状态的值函数的关系。\n\n见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/1.jpg)</center> \n\n-------------------------------------\n\n根据上图得出状态价值函数公式：\n\n\\begin{align}    \nv_{\\pi}(s)=E[U_t|S_t=s]=\\sum_{a\\in A}\\pi(a|s)\\,\\left( R_s^a+\\gamma \\sum_{s'\\in S}P^a_{ss'}v_{\\pi}(s')\\right)\n\\end{align}\n\n其中：\n\n\\begin{align}    \nP_{ss’}^a = P(S_{t+1}=s'|S_t=s, A_t=a)\n\\end{align}\n\n上式中策略π是指给定状态 s 的情况下，动作 a 的概率分布，即：\n\n\\begin{align}    \n\\pi(a|s)=P(a|s)\n\\end{align}\n\n将概率和转换为期望，上式等价于：\n\n\\begin{align}    \nv_{\\pi}(s) = E_{\\pi}[R_s^a + \\gamma v_{\\pi}(S_{t+1}|S_t=s]\n\\end{align}\n\n同理，我们可以得到动作价值函数的公式如下：\n\n\\begin{align}    \nq_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]\n\\end{align}\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/2.jpg)</center> \n\n-------------------------------------\n如上图，Bellman 方程也可以表达成矩阵形式：\n\n\\begin{align}    \nv=R+\\gamma Pv\n\\end{align}\n\n可直接求出：\n\n\\begin{align}    \nv=(I-\\gamma P)^{-1}R\n\\end{align}\n\n其复杂度为：\n\n\\begin{align}    \nO(n^3)\n\\end{align}\n\n一般可通过动态规划、时间差分法与蒙特卡洛估计求解。\n\n● 通过解 Bellman 最优性方程找一个最优策略需要以下条件:\n\n- 动态模型已知\n\n- 拥有足够的计算空间和时间\n\n- 系统满足 Markov 特性\n\n### 2. 状态价值函数和动作价值函数的关系\n\n-------------------------------------\n\n<center>![RL](/image/RL/2/3.jpg)</center> \n\n-------------------------------------\n\n\\begin{align}    \nv_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) = E[q_{\\pi}(s,a)|S_t=s] \n\\end{align}\n\n\\begin{align}    \nq_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a \\sum_{a' \\in A}\\pi(a'|s')q_{\\pi}(s',a') = R_s^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')\n\\end{align}\n\n### 3. 最优方程\n\n\\begin{align}    \nv_*(s)= \\max_aq_*(s,a)= \\max_a\\left( R_s^a + \\gamma\\sum_{s'\\in S}P_{ss'}^a v_*(s') \\right) \\\\\nq_*(s,a)= R_s^a+\\gamma \\sum_{s'\\in S}P_{ss'}^av_*(s') = R_s^a + \\gamma \\sum_{s’ \\in S}P_{ss'}^a\\max_{a’}q_*(s',a')\n\\end{align}\n\n● v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值\n\n● q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值\n\n### 4. 最优策略\n\n关于收敛性：（对策略定义一个偏序）\n\n$\\pi \\ge \\pi' \\,\\mbox{if}\\; v_{\\pi}(s)\\ge v_{\\pi'}(s),\\forall s$\n\n定理：\n\n对于任意 MDP：\n\n- 总是存在一个最优策略π*，它比其它任何策略都要好，或者至少一样好\n\n- 所有最优决策都达到最优值函数， $v_{\\pi_*}(s)=v_*(s)$\n\n- 所有最优决策都达到最优行动值函数，$q_{\\pi_*}(s,a)=q_*(s,a)$\n\n最优策略可从最优状态价值函数或者最优动作价值函数得出：\n\n\\begin{align} \n\\pi_*(a|s) = \n\\begin{cases}\n1, & \\mbox{if } a = \\arg\\max_{a \\in A} q_*(s,a)\\\\\n0, & otherwise\n\\end{cases}\n\\end{align}\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n","slug":"zh-Reinforcement-Learning笔记2-Bellman","published":1,"updated":"2018-01-01T13:15:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8ys000zog64dgnj0l5v","content":"<h3 id=\"1-Bellman方程（Bellman-Equation）\"><a href=\"#1-Bellman方程（Bellman-Equation）\" class=\"headerlink\" title=\"1. Bellman方程（Bellman Equation）\"></a>1. Bellman方程（Bellman Equation）</h3><p>贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。</p>\n<p>贝尔曼方程表明当前状态的值函数与下个状态的值函数的关系。</p>\n<p>见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。</p>\n<hr>\n<center><img src=\"/image/RL/2/1.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>根据上图得出状态价值函数公式：</p>\n<p>\\begin{align}<br>v_{\\pi}(s)=E[U_t|S_t=s]=\\sum_{a\\in A}\\pi(a|s)\\,\\left( R_s^a+\\gamma \\sum_{s’\\in S}P^a_{ss’}v_{\\pi}(s’)\\right)<br>\\end{align}</p>\n<p>其中：</p>\n<p>\\begin{align}<br>P_{ss’}^a = P(S_{t+1}=s’|S_t=s, A_t=a)<br>\\end{align}</p>\n<p>上式中策略π是指给定状态 s 的情况下，动作 a 的概率分布，即：</p>\n<p>\\begin{align}<br>\\pi(a|s)=P(a|s)<br>\\end{align}</p>\n<p>将概率和转换为期望，上式等价于：</p>\n<p>\\begin{align}<br>v_{\\pi}(s) = E_{\\pi}[R_s^a + \\gamma v_{\\pi}(S_{t+1}|S_t=s]<br>\\end{align}</p>\n<p>同理，我们可以得到动作价值函数的公式如下：</p>\n<p>\\begin{align}<br>q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]<br>\\end{align}</p>\n<hr>\n<center><img src=\"/image/RL/2/2.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>如上图，Bellman 方程也可以表达成矩阵形式：</p>\n<p>\\begin{align}<br>v=R+\\gamma Pv<br>\\end{align}</p>\n<p>可直接求出：</p>\n<p>\\begin{align}<br>v=(I-\\gamma P)^{-1}R<br>\\end{align}</p>\n<p>其复杂度为：</p>\n<p>\\begin{align}<br>O(n^3)<br>\\end{align}</p>\n<p>一般可通过动态规划、时间差分法与蒙特卡洛估计求解。</p>\n<p>● 通过解 Bellman 最优性方程找一个最优策略需要以下条件:</p>\n<ul>\n<li><p>动态模型已知</p>\n</li>\n<li><p>拥有足够的计算空间和时间</p>\n</li>\n<li><p>系统满足 Markov 特性</p>\n</li>\n</ul>\n<h3 id=\"2-状态价值函数和动作价值函数的关系\"><a href=\"#2-状态价值函数和动作价值函数的关系\" class=\"headerlink\" title=\"2. 状态价值函数和动作价值函数的关系\"></a>2. 状态价值函数和动作价值函数的关系</h3><hr>\n<center><img src=\"/image/RL/2/3.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>\\begin{align}<br>v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) = E[q_{\\pi}(s,a)|S_t=s]<br>\\end{align}</p>\n<p>\\begin{align}<br>q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^a \\sum_{a’ \\in A}\\pi(a’|s’)q_{\\pi}(s’,a’) = R_s^a+\\gamma \\sum_{s’ \\in S} P_{ss’}^a v_{\\pi}(s’)<br>\\end{align}</p>\n<h3 id=\"3-最优方程\"><a href=\"#3-最优方程\" class=\"headerlink\" title=\"3. 最优方程\"></a>3. 最优方程</h3><p>\\begin{align}<br>v_<em>(s)= \\max_aq_</em>(s,a)= \\max_a\\left( R_s^a + \\gamma\\sum_{s’\\in S}P_{ss’}^a v_<em>(s’) \\right) \\\\<br>q_</em>(s,a)= R_s^a+\\gamma \\sum_{s’\\in S}P_{ss’}^av_<em>(s’) = R_s^a + \\gamma \\sum_{s’ \\in S}P_{ss’}^a\\max_{a’}q_</em>(s’,a’)<br>\\end{align}</p>\n<p>● v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值</p>\n<p>● q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值</p>\n<h3 id=\"4-最优策略\"><a href=\"#4-最优策略\" class=\"headerlink\" title=\"4. 最优策略\"></a>4. 最优策略</h3><p>关于收敛性：（对策略定义一个偏序）</p>\n<p>$\\pi \\ge \\pi’ \\,\\mbox{if}\\; v_{\\pi}(s)\\ge v_{\\pi’}(s),\\forall s$</p>\n<p>定理：</p>\n<p>对于任意 MDP：</p>\n<ul>\n<li><p>总是存在一个最优策略π*，它比其它任何策略都要好，或者至少一样好</p>\n</li>\n<li><p>所有最优决策都达到最优值函数， $v_{\\pi_<em>}(s)=v_</em>(s)$</p>\n</li>\n<li><p>所有最优决策都达到最优行动值函数，$q_{\\pi_<em>}(s,a)=q_</em>(s,a)$</p>\n</li>\n</ul>\n<p>最优策略可从最优状态价值函数或者最优动作价值函数得出：</p>\n<p>\\begin{align}<br>\\pi_<em>(a|s) =<br>\\begin{cases}<br>1, &amp; \\mbox{if } a = \\arg\\max_{a \\in A} q_</em>(s,a)\\\\<br>0, &amp; otherwise<br>\\end{cases}<br>\\end{align}</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\" target=\"_blank\" rel=\"external\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\" target=\"_blank\" rel=\"external\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\" target=\"_blank\" rel=\"external\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习知识整理</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-Bellman方程（Bellman-Equation）\"><a href=\"#1-Bellman方程（Bellman-Equation）\" class=\"headerlink\" title=\"1. Bellman方程（Bellman Equation）\"></a>1. Bellman方程（Bellman Equation）</h3><p>贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。</p>\n<p>贝尔曼方程表明当前状态的值函数与下个状态的值函数的关系。</p>\n<p>见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。</p>\n<hr>\n<center><img src=\"/image/RL/2/1.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>根据上图得出状态价值函数公式：</p>\n<p>\\begin{align}<br>v_{\\pi}(s)=E[U_t|S_t=s]=\\sum_{a\\in A}\\pi(a|s)\\,\\left( R_s^a+\\gamma \\sum_{s’\\in S}P^a_{ss’}v_{\\pi}(s’)\\right)<br>\\end{align}</p>\n<p>其中：</p>\n<p>\\begin{align}<br>P_{ss’}^a = P(S_{t+1}=s’|S_t=s, A_t=a)<br>\\end{align}</p>\n<p>上式中策略π是指给定状态 s 的情况下，动作 a 的概率分布，即：</p>\n<p>\\begin{align}<br>\\pi(a|s)=P(a|s)<br>\\end{align}</p>\n<p>将概率和转换为期望，上式等价于：</p>\n<p>\\begin{align}<br>v_{\\pi}(s) = E_{\\pi}[R_s^a + \\gamma v_{\\pi}(S_{t+1}|S_t=s]<br>\\end{align}</p>\n<p>同理，我们可以得到动作价值函数的公式如下：</p>\n<p>\\begin{align}<br>q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]<br>\\end{align}</p>\n<hr>\n<center><img src=\"/image/RL/2/2.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>如上图，Bellman 方程也可以表达成矩阵形式：</p>\n<p>\\begin{align}<br>v=R+\\gamma Pv<br>\\end{align}</p>\n<p>可直接求出：</p>\n<p>\\begin{align}<br>v=(I-\\gamma P)^{-1}R<br>\\end{align}</p>\n<p>其复杂度为：</p>\n<p>\\begin{align}<br>O(n^3)<br>\\end{align}</p>\n<p>一般可通过动态规划、时间差分法与蒙特卡洛估计求解。</p>\n<p>● 通过解 Bellman 最优性方程找一个最优策略需要以下条件:</p>\n<ul>\n<li><p>动态模型已知</p>\n</li>\n<li><p>拥有足够的计算空间和时间</p>\n</li>\n<li><p>系统满足 Markov 特性</p>\n</li>\n</ul>\n<h3 id=\"2-状态价值函数和动作价值函数的关系\"><a href=\"#2-状态价值函数和动作价值函数的关系\" class=\"headerlink\" title=\"2. 状态价值函数和动作价值函数的关系\"></a>2. 状态价值函数和动作价值函数的关系</h3><hr>\n<center><img src=\"/image/RL/2/3.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>\\begin{align}<br>v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)q_{\\pi}(s,a) = E[q_{\\pi}(s,a)|S_t=s]<br>\\end{align}</p>\n<p>\\begin{align}<br>q_{\\pi}(s,a) = R_s^a + \\gamma \\sum_{s’\\in S}P_{ss’}^a \\sum_{a’ \\in A}\\pi(a’|s’)q_{\\pi}(s’,a’) = R_s^a+\\gamma \\sum_{s’ \\in S} P_{ss’}^a v_{\\pi}(s’)<br>\\end{align}</p>\n<h3 id=\"3-最优方程\"><a href=\"#3-最优方程\" class=\"headerlink\" title=\"3. 最优方程\"></a>3. 最优方程</h3><p>\\begin{align}<br>v_<em>(s)= \\max_aq_</em>(s,a)= \\max_a\\left( R_s^a + \\gamma\\sum_{s’\\in S}P_{ss’}^a v_<em>(s’) \\right) \\\\<br>q_</em>(s,a)= R_s^a+\\gamma \\sum_{s’\\in S}P_{ss’}^av_<em>(s’) = R_s^a + \\gamma \\sum_{s’ \\in S}P_{ss’}^a\\max_{a’}q_</em>(s’,a’)<br>\\end{align}</p>\n<p>● v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值</p>\n<p>● q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值</p>\n<h3 id=\"4-最优策略\"><a href=\"#4-最优策略\" class=\"headerlink\" title=\"4. 最优策略\"></a>4. 最优策略</h3><p>关于收敛性：（对策略定义一个偏序）</p>\n<p>$\\pi \\ge \\pi’ \\,\\mbox{if}\\; v_{\\pi}(s)\\ge v_{\\pi’}(s),\\forall s$</p>\n<p>定理：</p>\n<p>对于任意 MDP：</p>\n<ul>\n<li><p>总是存在一个最优策略π*，它比其它任何策略都要好，或者至少一样好</p>\n</li>\n<li><p>所有最优决策都达到最优值函数， $v_{\\pi_<em>}(s)=v_</em>(s)$</p>\n</li>\n<li><p>所有最优决策都达到最优行动值函数，$q_{\\pi_<em>}(s,a)=q_</em>(s,a)$</p>\n</li>\n</ul>\n<p>最优策略可从最优状态价值函数或者最优动作价值函数得出：</p>\n<p>\\begin{align}<br>\\pi_<em>(a|s) =<br>\\begin{cases}<br>1, &amp; \\mbox{if } a = \\arg\\max_{a \\in A} q_</em>(s,a)\\\\<br>0, &amp; otherwise<br>\\end{cases}<br>\\end{align}</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\">强化学习知识整理</a> </p>\n</li>\n</ul>\n"},{"title":"Reinforcement Learning笔记3-Dynamic Program","lang":"zh","date":"2017-11-13T09:18:56.000Z","_content":"\n#### Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）\n\n动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。\n\n「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。\n\n「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 Bellman 方程可以递归地切分子问题。\n\n### 1. 策略估计(Policy Evaluation)\n\n对于任意的策略π，我们如何计算其状态值函数Vπ(s)？这个问题被称作策略估计。\n\n● 基于当前的 Policy 计算出每个状态的 Value function\n\n● 同步更新：每次迭代更新所有的状态的 v\n\\begin{align} \nV_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)\\left( R_s^a+\\gamma\\sum_{s'\\in S}P_{ss'}^av_k(s')\\right)\n\\end{align}\n\n● 矩阵形式：\n\\begin{align} \n\\mathbf{v^{k+1}=R^{\\pi}+\\gamma P^{\\pi}v^k}\n\\end{align}\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/1.jpg)</center> \n<center>![RL](/image/RL/3/2.jpg)</center> \n\n-------------------------------------\n\n● 左边是第 k 次迭代每个 state 上状态价值函数的值，右边是通过贪心（greedy）算法找到策略\n\n● 计算实例：\n\\begin{align}\nk=2, -1.7  \\approx -1.75 = 0.25*(-1+0) + 0.25*(-1-1) + 0.25*(-1-1) + 0.25*(-1-1)\n\\end{align}\n\\begin{align}\nk=3, -2.9  \\approx -2.925 = -0.25*(-1-2) + 0.25*(-1-2) + 0.25*(-1-2) + 0.25*(-1-1.7)\n\\end{align}\n\n### 2. 策略改进(Policy Improvement)\n\n● 基于当前的状态价值函数（value function），用贪心算法找到最优策略\n\n$\\pi'(s)=\\arg\\max_{a\\in A} q_{\\pi}(s,a)$\n\n● Vπ会一直迭代到收敛，具体证明如图:\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/3.jpg)</center> \n\n-------------------------------------\n\n事实上在大多数情况下 Policy evaluation 不必要非常逼近最优值，这时我们通常引入 \\epsilon-convergence 函数来控制迭代停止。\n\n很多情况下价值函数还未完全收敛，Policy 就已经最优，所以在每次迭代之后都可以更新策略（Policy），当策略无变化时停止迭代。\n\n### 3. 策略迭代(Policy Iteration)\n\n策略迭代算法就是策略估计和策略改进两节内容的组合。假设有一个策略π，那么可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π'，接着再计算Vπ'(s),再获得更好的策略π''。\n\n### 4. 价值迭代(Value Iteration)\n\n最优化原理：当且仅当状态 s 达到任意能到达的状态 s‘ 时，价值函数 v 能在当前策略（policy）下达到最优，即$v_{\\pi}(s') = v_*(s')$，与此同时，状态 s 也能基于当前策略达到最优，即$v_{\\pi}(s) = v_*(s)$\n\n状态转移公式为：\n$v_{k+1}(s) = \\max_{a\\in A}(R^a_s+\\gamma\\sum_{s' \\in S}P^a_{ss'}v_k(s'))$\n\n矩阵形式为：$\\mathbf{v_{k+1}} =\\max_{a \\in A} \\mathbf{R^a_s} +\\gamma\\mathbf{P^av_k})$\n\n下面是一个实例，求每个格子到终点的最短距离，走一步的 reward 是 -1:\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/4.png)</center> \n\n-------------------------------------\n\n#### 同步动态规划算法小结\n\nMDP 的问题主要分两类：\n\n① Prediction 问题\n- 输入：MDP (S,A,P,R,y)和策略（policy）π\n\n- 输出：状态价值函数 Vπ\n\n② Control 问题\n- 输入：MDP (S,A,P,R.y)\n\n- 输出：最优状态价值函数V*和最优策略π\n\n\n● 策略估计(Policy Evaluation)解决的是 Prediction 问题，使用了贝尔曼期望方程（Bellman Expectation Equation）\n\n● 策略改进(Policy Improvement)解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法\n\n● 策略迭代（Policy Iteration）解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法\n\n● 价值迭代（Value Iteration） 解决的是 Control 问题，它并没有直接计算策略（Policy），而是在得到最优的基于策略的价值函数之后推导出最优的 Policy，使用的是贝尔曼最优化方程（Bellman Optimality Equation）\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n\n- [动态规划法(Dynamic Programming Methods)](http://www.cnblogs.com/jinxulin/p/3526542.html \"Title\") \n","source":"_posts/zh/Reinforcement Learning笔记3-Dynamic Program.md","raw":"\n---\ntitle: Reinforcement Learning笔记3-Dynamic Program\nlang: zh\ndate: 2017-11-13 18:18:56\ntags: Reinforcement Learning\n---\n\n#### Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）\n\n动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。\n\n「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。\n\n「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 Bellman 方程可以递归地切分子问题。\n\n### 1. 策略估计(Policy Evaluation)\n\n对于任意的策略π，我们如何计算其状态值函数Vπ(s)？这个问题被称作策略估计。\n\n● 基于当前的 Policy 计算出每个状态的 Value function\n\n● 同步更新：每次迭代更新所有的状态的 v\n\\begin{align} \nV_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)\\left( R_s^a+\\gamma\\sum_{s'\\in S}P_{ss'}^av_k(s')\\right)\n\\end{align}\n\n● 矩阵形式：\n\\begin{align} \n\\mathbf{v^{k+1}=R^{\\pi}+\\gamma P^{\\pi}v^k}\n\\end{align}\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/1.jpg)</center> \n<center>![RL](/image/RL/3/2.jpg)</center> \n\n-------------------------------------\n\n● 左边是第 k 次迭代每个 state 上状态价值函数的值，右边是通过贪心（greedy）算法找到策略\n\n● 计算实例：\n\\begin{align}\nk=2, -1.7  \\approx -1.75 = 0.25*(-1+0) + 0.25*(-1-1) + 0.25*(-1-1) + 0.25*(-1-1)\n\\end{align}\n\\begin{align}\nk=3, -2.9  \\approx -2.925 = -0.25*(-1-2) + 0.25*(-1-2) + 0.25*(-1-2) + 0.25*(-1-1.7)\n\\end{align}\n\n### 2. 策略改进(Policy Improvement)\n\n● 基于当前的状态价值函数（value function），用贪心算法找到最优策略\n\n$\\pi'(s)=\\arg\\max_{a\\in A} q_{\\pi}(s,a)$\n\n● Vπ会一直迭代到收敛，具体证明如图:\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/3.jpg)</center> \n\n-------------------------------------\n\n事实上在大多数情况下 Policy evaluation 不必要非常逼近最优值，这时我们通常引入 \\epsilon-convergence 函数来控制迭代停止。\n\n很多情况下价值函数还未完全收敛，Policy 就已经最优，所以在每次迭代之后都可以更新策略（Policy），当策略无变化时停止迭代。\n\n### 3. 策略迭代(Policy Iteration)\n\n策略迭代算法就是策略估计和策略改进两节内容的组合。假设有一个策略π，那么可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π'，接着再计算Vπ'(s),再获得更好的策略π''。\n\n### 4. 价值迭代(Value Iteration)\n\n最优化原理：当且仅当状态 s 达到任意能到达的状态 s‘ 时，价值函数 v 能在当前策略（policy）下达到最优，即$v_{\\pi}(s') = v_*(s')$，与此同时，状态 s 也能基于当前策略达到最优，即$v_{\\pi}(s) = v_*(s)$\n\n状态转移公式为：\n$v_{k+1}(s) = \\max_{a\\in A}(R^a_s+\\gamma\\sum_{s' \\in S}P^a_{ss'}v_k(s'))$\n\n矩阵形式为：$\\mathbf{v_{k+1}} =\\max_{a \\in A} \\mathbf{R^a_s} +\\gamma\\mathbf{P^av_k})$\n\n下面是一个实例，求每个格子到终点的最短距离，走一步的 reward 是 -1:\n\n-------------------------------------\n\n<center>![RL](/image/RL/3/4.png)</center> \n\n-------------------------------------\n\n#### 同步动态规划算法小结\n\nMDP 的问题主要分两类：\n\n① Prediction 问题\n- 输入：MDP (S,A,P,R,y)和策略（policy）π\n\n- 输出：状态价值函数 Vπ\n\n② Control 问题\n- 输入：MDP (S,A,P,R.y)\n\n- 输出：最优状态价值函数V*和最优策略π\n\n\n● 策略估计(Policy Evaluation)解决的是 Prediction 问题，使用了贝尔曼期望方程（Bellman Expectation Equation）\n\n● 策略改进(Policy Improvement)解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法\n\n● 策略迭代（Policy Iteration）解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法\n\n● 价值迭代（Value Iteration） 解决的是 Control 问题，它并没有直接计算策略（Policy），而是在得到最优的基于策略的价值函数之后推导出最优的 Policy，使用的是贝尔曼最优化方程（Bellman Optimality Equation）\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n\n- [动态规划法(Dynamic Programming Methods)](http://www.cnblogs.com/jinxulin/p/3526542.html \"Title\") \n","slug":"zh-Reinforcement-Learning笔记3-Dynamic-Program","published":1,"updated":"2018-01-01T13:15:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yu0012og64fv3azzcs","content":"<h4 id=\"Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）\"><a href=\"#Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）\" class=\"headerlink\" title=\"Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）\"></a>Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）</h4><p>动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。</p>\n<p>「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。</p>\n<p>「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 Bellman 方程可以递归地切分子问题。</p>\n<h3 id=\"1-策略估计-Policy-Evaluation\"><a href=\"#1-策略估计-Policy-Evaluation\" class=\"headerlink\" title=\"1. 策略估计(Policy Evaluation)\"></a>1. 策略估计(Policy Evaluation)</h3><p>对于任意的策略π，我们如何计算其状态值函数Vπ(s)？这个问题被称作策略估计。</p>\n<p>● 基于当前的 Policy 计算出每个状态的 Value function</p>\n<p>● 同步更新：每次迭代更新所有的状态的 v<br>\\begin{align}<br>V_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)\\left( R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^av_k(s’)\\right)<br>\\end{align}</p>\n<p>● 矩阵形式：<br>\\begin{align}<br>\\mathbf{v^{k+1}=R^{\\pi}+\\gamma P^{\\pi}v^k}<br>\\end{align}</p>\n<hr>\n<center><img src=\"/image/RL/3/1.jpg\" alt=\"RL\"></center><br><center><img src=\"/image/RL/3/2.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>● 左边是第 k 次迭代每个 state 上状态价值函数的值，右边是通过贪心（greedy）算法找到策略</p>\n<p>● 计算实例：<br>\\begin{align}<br>k=2, -1.7  \\approx -1.75 = 0.25<em>(-1+0) + 0.25</em>(-1-1) + 0.25<em>(-1-1) + 0.25</em>(-1-1)<br>\\end{align}<br>\\begin{align}<br>k=3, -2.9  \\approx -2.925 = -0.25<em>(-1-2) + 0.25</em>(-1-2) + 0.25<em>(-1-2) + 0.25</em>(-1-1.7)<br>\\end{align}</p>\n<h3 id=\"2-策略改进-Policy-Improvement\"><a href=\"#2-策略改进-Policy-Improvement\" class=\"headerlink\" title=\"2. 策略改进(Policy Improvement)\"></a>2. 策略改进(Policy Improvement)</h3><p>● 基于当前的状态价值函数（value function），用贪心算法找到最优策略</p>\n<p>$\\pi’(s)=\\arg\\max_{a\\in A} q_{\\pi}(s,a)$</p>\n<p>● Vπ会一直迭代到收敛，具体证明如图:</p>\n<hr>\n<center><img src=\"/image/RL/3/3.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>事实上在大多数情况下 Policy evaluation 不必要非常逼近最优值，这时我们通常引入 \\epsilon-convergence 函数来控制迭代停止。</p>\n<p>很多情况下价值函数还未完全收敛，Policy 就已经最优，所以在每次迭代之后都可以更新策略（Policy），当策略无变化时停止迭代。</p>\n<h3 id=\"3-策略迭代-Policy-Iteration\"><a href=\"#3-策略迭代-Policy-Iteration\" class=\"headerlink\" title=\"3. 策略迭代(Policy Iteration)\"></a>3. 策略迭代(Policy Iteration)</h3><p>策略迭代算法就是策略估计和策略改进两节内容的组合。假设有一个策略π，那么可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π’，接着再计算Vπ’(s),再获得更好的策略π’’。</p>\n<h3 id=\"4-价值迭代-Value-Iteration\"><a href=\"#4-价值迭代-Value-Iteration\" class=\"headerlink\" title=\"4. 价值迭代(Value Iteration)\"></a>4. 价值迭代(Value Iteration)</h3><p>最优化原理：当且仅当状态 s 达到任意能到达的状态 s‘ 时，价值函数 v 能在当前策略（policy）下达到最优，即$v_{\\pi}(s’) = v_<em>(s’)$，与此同时，状态 s 也能基于当前策略达到最优，即$v_{\\pi}(s) = v_</em>(s)$</p>\n<p>状态转移公式为：<br>$v_{k+1}(s) = \\max_{a\\in A}(R^a_s+\\gamma\\sum_{s’ \\in S}P^a_{ss’}v_k(s’))$</p>\n<p>矩阵形式为：$\\mathbf{v_{k+1}} =\\max_{a \\in A} \\mathbf{R^a_s} +\\gamma\\mathbf{P^av_k})$</p>\n<p>下面是一个实例，求每个格子到终点的最短距离，走一步的 reward 是 -1:</p>\n<hr>\n<center><img src=\"/image/RL/3/4.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"同步动态规划算法小结\"><a href=\"#同步动态规划算法小结\" class=\"headerlink\" title=\"同步动态规划算法小结\"></a>同步动态规划算法小结</h4><p>MDP 的问题主要分两类：</p>\n<p>① Prediction 问题</p>\n<ul>\n<li><p>输入：MDP (S,A,P,R,y)和策略（policy）π</p>\n</li>\n<li><p>输出：状态价值函数 Vπ</p>\n</li>\n</ul>\n<p>② Control 问题</p>\n<ul>\n<li><p>输入：MDP (S,A,P,R.y)</p>\n</li>\n<li><p>输出：最优状态价值函数V*和最优策略π</p>\n</li>\n</ul>\n<p>● 策略估计(Policy Evaluation)解决的是 Prediction 问题，使用了贝尔曼期望方程（Bellman Expectation Equation）</p>\n<p>● 策略改进(Policy Improvement)解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>\n<p>● 策略迭代（Policy Iteration）解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>\n<p>● 价值迭代（Value Iteration） 解决的是 Control 问题，它并没有直接计算策略（Policy），而是在得到最优的基于策略的价值函数之后推导出最优的 Policy，使用的是贝尔曼最优化方程（Bellman Optimality Equation）</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\" target=\"_blank\" rel=\"external\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\" target=\"_blank\" rel=\"external\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\" target=\"_blank\" rel=\"external\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习知识整理</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/3526542.html\" title=\"Title\" target=\"_blank\" rel=\"external\">动态规划法(Dynamic Programming Methods)</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h4 id=\"Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）\"><a href=\"#Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）\" class=\"headerlink\" title=\"Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）\"></a>Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）</h4><p>动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。</p>\n<p>「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。</p>\n<p>「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 Bellman 方程可以递归地切分子问题。</p>\n<h3 id=\"1-策略估计-Policy-Evaluation\"><a href=\"#1-策略估计-Policy-Evaluation\" class=\"headerlink\" title=\"1. 策略估计(Policy Evaluation)\"></a>1. 策略估计(Policy Evaluation)</h3><p>对于任意的策略π，我们如何计算其状态值函数Vπ(s)？这个问题被称作策略估计。</p>\n<p>● 基于当前的 Policy 计算出每个状态的 Value function</p>\n<p>● 同步更新：每次迭代更新所有的状态的 v<br>\\begin{align}<br>V_{k+1}(s)=\\sum_{a\\in A}\\pi(a|s)\\left( R_s^a+\\gamma\\sum_{s’\\in S}P_{ss’}^av_k(s’)\\right)<br>\\end{align}</p>\n<p>● 矩阵形式：<br>\\begin{align}<br>\\mathbf{v^{k+1}=R^{\\pi}+\\gamma P^{\\pi}v^k}<br>\\end{align}</p>\n<hr>\n<center><img src=\"/image/RL/3/1.jpg\" alt=\"RL\"></center><br><center><img src=\"/image/RL/3/2.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>● 左边是第 k 次迭代每个 state 上状态价值函数的值，右边是通过贪心（greedy）算法找到策略</p>\n<p>● 计算实例：<br>\\begin{align}<br>k=2, -1.7  \\approx -1.75 = 0.25<em>(-1+0) + 0.25</em>(-1-1) + 0.25<em>(-1-1) + 0.25</em>(-1-1)<br>\\end{align}<br>\\begin{align}<br>k=3, -2.9  \\approx -2.925 = -0.25<em>(-1-2) + 0.25</em>(-1-2) + 0.25<em>(-1-2) + 0.25</em>(-1-1.7)<br>\\end{align}</p>\n<h3 id=\"2-策略改进-Policy-Improvement\"><a href=\"#2-策略改进-Policy-Improvement\" class=\"headerlink\" title=\"2. 策略改进(Policy Improvement)\"></a>2. 策略改进(Policy Improvement)</h3><p>● 基于当前的状态价值函数（value function），用贪心算法找到最优策略</p>\n<p>$\\pi’(s)=\\arg\\max_{a\\in A} q_{\\pi}(s,a)$</p>\n<p>● Vπ会一直迭代到收敛，具体证明如图:</p>\n<hr>\n<center><img src=\"/image/RL/3/3.jpg\" alt=\"RL\"></center> \n\n<hr>\n<p>事实上在大多数情况下 Policy evaluation 不必要非常逼近最优值，这时我们通常引入 \\epsilon-convergence 函数来控制迭代停止。</p>\n<p>很多情况下价值函数还未完全收敛，Policy 就已经最优，所以在每次迭代之后都可以更新策略（Policy），当策略无变化时停止迭代。</p>\n<h3 id=\"3-策略迭代-Policy-Iteration\"><a href=\"#3-策略迭代-Policy-Iteration\" class=\"headerlink\" title=\"3. 策略迭代(Policy Iteration)\"></a>3. 策略迭代(Policy Iteration)</h3><p>策略迭代算法就是策略估计和策略改进两节内容的组合。假设有一个策略π，那么可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π’，接着再计算Vπ’(s),再获得更好的策略π’’。</p>\n<h3 id=\"4-价值迭代-Value-Iteration\"><a href=\"#4-价值迭代-Value-Iteration\" class=\"headerlink\" title=\"4. 价值迭代(Value Iteration)\"></a>4. 价值迭代(Value Iteration)</h3><p>最优化原理：当且仅当状态 s 达到任意能到达的状态 s‘ 时，价值函数 v 能在当前策略（policy）下达到最优，即$v_{\\pi}(s’) = v_<em>(s’)$，与此同时，状态 s 也能基于当前策略达到最优，即$v_{\\pi}(s) = v_</em>(s)$</p>\n<p>状态转移公式为：<br>$v_{k+1}(s) = \\max_{a\\in A}(R^a_s+\\gamma\\sum_{s’ \\in S}P^a_{ss’}v_k(s’))$</p>\n<p>矩阵形式为：$\\mathbf{v_{k+1}} =\\max_{a \\in A} \\mathbf{R^a_s} +\\gamma\\mathbf{P^av_k})$</p>\n<p>下面是一个实例，求每个格子到终点的最短距离，走一步的 reward 是 -1:</p>\n<hr>\n<center><img src=\"/image/RL/3/4.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"同步动态规划算法小结\"><a href=\"#同步动态规划算法小结\" class=\"headerlink\" title=\"同步动态规划算法小结\"></a>同步动态规划算法小结</h4><p>MDP 的问题主要分两类：</p>\n<p>① Prediction 问题</p>\n<ul>\n<li><p>输入：MDP (S,A,P,R,y)和策略（policy）π</p>\n</li>\n<li><p>输出：状态价值函数 Vπ</p>\n</li>\n</ul>\n<p>② Control 问题</p>\n<ul>\n<li><p>输入：MDP (S,A,P,R.y)</p>\n</li>\n<li><p>输出：最优状态价值函数V*和最优策略π</p>\n</li>\n</ul>\n<p>● 策略估计(Policy Evaluation)解决的是 Prediction 问题，使用了贝尔曼期望方程（Bellman Expectation Equation）</p>\n<p>● 策略改进(Policy Improvement)解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>\n<p>● 策略迭代（Policy Iteration）解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>\n<p>● 价值迭代（Value Iteration） 解决的是 Control 问题，它并没有直接计算策略（Policy），而是在得到最优的基于策略的价值函数之后推导出最优的 Policy，使用的是贝尔曼最优化方程（Bellman Optimality Equation）</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\">强化学习知识整理</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/3526542.html\" title=\"Title\">动态规划法(Dynamic Programming Methods)</a> </p>\n</li>\n</ul>\n"},{"title":"Reinforcement Learning笔记5-Temporal Diff","lang":"zh","date":"2017-11-15T09:18:56.000Z","_content":"\n#### Bellman最优解策略 - 时间差分法（Temporal Difference）\n\n蒙特卡洛算法需要采样完成一个轨迹之后，才能进行值估计（value estimation），这样蒙特卡洛速度很慢，主要原因在于蒙特卡洛没有充分的利用强化学习任务的 MDP 结构。但是，TD 充分利用了 “MC”和 动态规划的思想，做到了更加高效率的免模型学习。\n\n强化学习算法可以分为在同策略(on-policy)和异策略(off-policy)两类：\n\n若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。\n\n若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。\n\n● Q-learning属于离策略(off-policy)算法。\n\n● Sarsa属于在策略(on-policy)算法。\n\n本文内容来自[知乎 - 莫烦](https://zhuanlan.zhihu.com/morvan \"Title\") ，以小时候写作业为例子，看看TD是如何来决策：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/0-0.png)</center> \n\n-------------------------------------\n\n### 1. Q-learning\n\n● 感性认识：内容来自 - [什么是 Q-Learning - 莫烦](https://zhuanlan.zhihu.com/p/24808797 \"Title\") \n\n#### 1.1. Q-Learning 决策\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-1.png)</center> \n\n-------------------------------------\n\n#### 1.2. Q-Learning 更新\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-2.png)</center> \n\n-------------------------------------\n\n#### 1.3. Q-Learning 整体算法\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-3.png)</center> \n\n-------------------------------------\n\n#### 1.4. Q-Learning 中的 Lambda\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-4.png)</center> \n\n-------------------------------------\n\n#### DQN（Deep Q-learning） \n\nQ-learning中因为需要下一状态的所有动作的最大Q，loss的也有个特点:网络要多前向计算一次，才能得到下一状态的所有动作的Q。也是因为这个loss的原因，这里的DQN等于说是把CNN用作生成Q的一个函数。原来的质量表达Q是个矩阵，参数少，现在的用深度网络，就叫深度质量网络（DQN）。\n\n### 2. Sarsa\n\n● 感性认识：内容来自 - [什么是 Sarsa - 莫烦](https://zhuanlan.zhihu.com/p/24860793 \"Title\") \n\nsarsa算法估计的是动作值函数(Q函数)而非状态值函数。\n\n#### 2.1. Sarsa 决策\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/2-1.png)</center> \n\n-------------------------------------\n\nSarsa 的决策部分和 Q-learning 一模一样, 因为使用的是 Q 表的形式决策, 所以在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩. 但是不同的地方在于 Sarsa 的更新方式是不一样的。\n\n#### 2.2. Sarsa 更新行为准则\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/2-2.png)</center> \n\n-------------------------------------\n\n### 3. Sarsa VS Q-learning\n\n内容来自：[什么是 Sarsa - 莫烦](https://zhuanlan.zhihu.com/p/24860793 \"Title\") \n\n-------------------------------------\n\n<center>![RL](/image/RL/5/3-1.png)</center> \n\n-------------------------------------\n\n从算法来看, 这就是他们两最大的不同之处了. 因为 Sarsa 是说到做到型, 所以我们也叫他 on-policy在线学习, 学着自己在做的事情. 而 Q learning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法。\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/3-2.png)</center> \n\n-------------------------------------\n\n为什么说他勇敢呢, 因为 Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用 Sarsa 方法的不同之处。\n\n● 开源推荐：[OpenAI Gym](https://github.com/openai/gym \"Title\") \n\n### 4. 时间差分法（TD）VS 蒙特卡罗（ MC）\n\n内容来自：[知乎：强化学习中时间差分(TD)和蒙特卡洛(MC)方法各自的优劣？- 远方的梦回答](https://www.zhihu.com/question/62388365/answer/218012513 \"Title\") \n\nTD对应的sarsa/qlearning和MC对应的control算法进行比较:\n\n● 1. MC计算量更大，更新缓慢。\n\n可以这样理解：对MC而言，其return如下：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-1.png)</center> \n\n-------------------------------------\n\n所以MC必须在整轮episode迭代结束后进行更新(也就是一盘围棋要下完才更新一次)，而TD(0)在下一个状态s(t+1)后就可以进行更新,其return如下：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-2.png)</center> \n\n-------------------------------------\n\n● 2.MC不能用于continuous task（比如倒立摆）， 而TD(0)可以：理由同上。\n\n● 3.相反于第二点，MC能用于一些围棋之类的规划较深的任务。\n\n这个“深”字体现在：先阶段某个action对以后未来的action决策都有较大的影响（象棋，围棋之类的，想必深有体会）。而对于倒立摆这种问题，我现在让它向左的这个决策对于其1分钟后而言，其实基本没什么关系。参考下面两张图：\n\n\n图片来自：[Model-Free Prediction](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf \"Title\") \n\n **蒙特卡罗（ MC）：**\n \n-------------------------------------\n\n<center>![RL](/image/RL/5/4-3.png)</center> \n\n-------------------------------------\n\n **时间差分法（TD）：**\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-4.png)</center> \n\n-------------------------------------\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [Dissecting Reinforcement Learning-Part.2](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [时间差分学习(Q learning,Sarsa learning)](http://www.cnblogs.com/jinxulin/p/5116332.html \"Title\") \n\n- [A Painless Q-learning Tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm \"Title\")\n\n- [一个 Q-learning 算法的简明教程](http://blog.csdn.net/itplus/article/details/9361915 \"Title\")\n","source":"_posts/zh/Reinforcement Learning笔记5-Temporal Diff.md","raw":"\n---\ntitle: Reinforcement Learning笔记5-Temporal Diff\nlang: zh\ndate: 2017-11-15 18:18:56\ntags: Reinforcement Learning\n---\n\n#### Bellman最优解策略 - 时间差分法（Temporal Difference）\n\n蒙特卡洛算法需要采样完成一个轨迹之后，才能进行值估计（value estimation），这样蒙特卡洛速度很慢，主要原因在于蒙特卡洛没有充分的利用强化学习任务的 MDP 结构。但是，TD 充分利用了 “MC”和 动态规划的思想，做到了更加高效率的免模型学习。\n\n强化学习算法可以分为在同策略(on-policy)和异策略(off-policy)两类：\n\n若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。\n\n若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。\n\n● Q-learning属于离策略(off-policy)算法。\n\n● Sarsa属于在策略(on-policy)算法。\n\n本文内容来自[知乎 - 莫烦](https://zhuanlan.zhihu.com/morvan \"Title\") ，以小时候写作业为例子，看看TD是如何来决策：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/0-0.png)</center> \n\n-------------------------------------\n\n### 1. Q-learning\n\n● 感性认识：内容来自 - [什么是 Q-Learning - 莫烦](https://zhuanlan.zhihu.com/p/24808797 \"Title\") \n\n#### 1.1. Q-Learning 决策\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-1.png)</center> \n\n-------------------------------------\n\n#### 1.2. Q-Learning 更新\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-2.png)</center> \n\n-------------------------------------\n\n#### 1.3. Q-Learning 整体算法\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-3.png)</center> \n\n-------------------------------------\n\n#### 1.4. Q-Learning 中的 Lambda\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/1-4.png)</center> \n\n-------------------------------------\n\n#### DQN（Deep Q-learning） \n\nQ-learning中因为需要下一状态的所有动作的最大Q，loss的也有个特点:网络要多前向计算一次，才能得到下一状态的所有动作的Q。也是因为这个loss的原因，这里的DQN等于说是把CNN用作生成Q的一个函数。原来的质量表达Q是个矩阵，参数少，现在的用深度网络，就叫深度质量网络（DQN）。\n\n### 2. Sarsa\n\n● 感性认识：内容来自 - [什么是 Sarsa - 莫烦](https://zhuanlan.zhihu.com/p/24860793 \"Title\") \n\nsarsa算法估计的是动作值函数(Q函数)而非状态值函数。\n\n#### 2.1. Sarsa 决策\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/2-1.png)</center> \n\n-------------------------------------\n\nSarsa 的决策部分和 Q-learning 一模一样, 因为使用的是 Q 表的形式决策, 所以在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩. 但是不同的地方在于 Sarsa 的更新方式是不一样的。\n\n#### 2.2. Sarsa 更新行为准则\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/2-2.png)</center> \n\n-------------------------------------\n\n### 3. Sarsa VS Q-learning\n\n内容来自：[什么是 Sarsa - 莫烦](https://zhuanlan.zhihu.com/p/24860793 \"Title\") \n\n-------------------------------------\n\n<center>![RL](/image/RL/5/3-1.png)</center> \n\n-------------------------------------\n\n从算法来看, 这就是他们两最大的不同之处了. 因为 Sarsa 是说到做到型, 所以我们也叫他 on-policy在线学习, 学着自己在做的事情. 而 Q learning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法。\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/3-2.png)</center> \n\n-------------------------------------\n\n为什么说他勇敢呢, 因为 Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用 Sarsa 方法的不同之处。\n\n● 开源推荐：[OpenAI Gym](https://github.com/openai/gym \"Title\") \n\n### 4. 时间差分法（TD）VS 蒙特卡罗（ MC）\n\n内容来自：[知乎：强化学习中时间差分(TD)和蒙特卡洛(MC)方法各自的优劣？- 远方的梦回答](https://www.zhihu.com/question/62388365/answer/218012513 \"Title\") \n\nTD对应的sarsa/qlearning和MC对应的control算法进行比较:\n\n● 1. MC计算量更大，更新缓慢。\n\n可以这样理解：对MC而言，其return如下：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-1.png)</center> \n\n-------------------------------------\n\n所以MC必须在整轮episode迭代结束后进行更新(也就是一盘围棋要下完才更新一次)，而TD(0)在下一个状态s(t+1)后就可以进行更新,其return如下：\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-2.png)</center> \n\n-------------------------------------\n\n● 2.MC不能用于continuous task（比如倒立摆）， 而TD(0)可以：理由同上。\n\n● 3.相反于第二点，MC能用于一些围棋之类的规划较深的任务。\n\n这个“深”字体现在：先阶段某个action对以后未来的action决策都有较大的影响（象棋，围棋之类的，想必深有体会）。而对于倒立摆这种问题，我现在让它向左的这个决策对于其1分钟后而言，其实基本没什么关系。参考下面两张图：\n\n\n图片来自：[Model-Free Prediction](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf \"Title\") \n\n **蒙特卡罗（ MC）：**\n \n-------------------------------------\n\n<center>![RL](/image/RL/5/4-3.png)</center> \n\n-------------------------------------\n\n **时间差分法（TD）：**\n\n-------------------------------------\n\n<center>![RL](/image/RL/5/4-4.png)</center> \n\n-------------------------------------\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [Dissecting Reinforcement Learning-Part.2](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html \"Title\") \n\n- [REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/ \"Title\") \n\n- [Pythonではじめる強化学習](https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906 \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [时间差分学习(Q learning,Sarsa learning)](http://www.cnblogs.com/jinxulin/p/5116332.html \"Title\") \n\n- [A Painless Q-learning Tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm \"Title\")\n\n- [一个 Q-learning 算法的简明教程](http://blog.csdn.net/itplus/article/details/9361915 \"Title\")\n","slug":"zh-Reinforcement-Learning笔记5-Temporal-Diff","published":1,"updated":"2018-01-01T13:15:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yu0013og648k3tyjxm","content":"<h4 id=\"Bellman最优解策略-时间差分法（Temporal-Difference）\"><a href=\"#Bellman最优解策略-时间差分法（Temporal-Difference）\" class=\"headerlink\" title=\"Bellman最优解策略 - 时间差分法（Temporal Difference）\"></a>Bellman最优解策略 - 时间差分法（Temporal Difference）</h4><p>蒙特卡洛算法需要采样完成一个轨迹之后，才能进行值估计（value estimation），这样蒙特卡洛速度很慢，主要原因在于蒙特卡洛没有充分的利用强化学习任务的 MDP 结构。但是，TD 充分利用了 “MC”和 动态规划的思想，做到了更加高效率的免模型学习。</p>\n<p>强化学习算法可以分为在同策略(on-policy)和异策略(off-policy)两类：</p>\n<p>若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。</p>\n<p>若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。</p>\n<p>● Q-learning属于离策略(off-policy)算法。</p>\n<p>● Sarsa属于在策略(on-policy)算法。</p>\n<p>本文内容来自<a href=\"https://zhuanlan.zhihu.com/morvan\" title=\"Title\" target=\"_blank\" rel=\"external\">知乎 - 莫烦</a> ，以小时候写作业为例子，看看TD是如何来决策：</p>\n<hr>\n<center><img src=\"/image/RL/5/0-0.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"1-Q-learning\"><a href=\"#1-Q-learning\" class=\"headerlink\" title=\"1. Q-learning\"></a>1. Q-learning</h3><p>● 感性认识：内容来自 - <a href=\"https://zhuanlan.zhihu.com/p/24808797\" title=\"Title\" target=\"_blank\" rel=\"external\">什么是 Q-Learning - 莫烦</a> </p>\n<h4 id=\"1-1-Q-Learning-决策\"><a href=\"#1-1-Q-Learning-决策\" class=\"headerlink\" title=\"1.1. Q-Learning 决策\"></a>1.1. Q-Learning 决策</h4><hr>\n<center><img src=\"/image/RL/5/1-1.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-2-Q-Learning-更新\"><a href=\"#1-2-Q-Learning-更新\" class=\"headerlink\" title=\"1.2. Q-Learning 更新\"></a>1.2. Q-Learning 更新</h4><hr>\n<center><img src=\"/image/RL/5/1-2.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-3-Q-Learning-整体算法\"><a href=\"#1-3-Q-Learning-整体算法\" class=\"headerlink\" title=\"1.3. Q-Learning 整体算法\"></a>1.3. Q-Learning 整体算法</h4><hr>\n<center><img src=\"/image/RL/5/1-3.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-4-Q-Learning-中的-Lambda\"><a href=\"#1-4-Q-Learning-中的-Lambda\" class=\"headerlink\" title=\"1.4. Q-Learning 中的 Lambda\"></a>1.4. Q-Learning 中的 Lambda</h4><hr>\n<center><img src=\"/image/RL/5/1-4.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"DQN（Deep-Q-learning）\"><a href=\"#DQN（Deep-Q-learning）\" class=\"headerlink\" title=\"DQN（Deep Q-learning）\"></a>DQN（Deep Q-learning）</h4><p>Q-learning中因为需要下一状态的所有动作的最大Q，loss的也有个特点:网络要多前向计算一次，才能得到下一状态的所有动作的Q。也是因为这个loss的原因，这里的DQN等于说是把CNN用作生成Q的一个函数。原来的质量表达Q是个矩阵，参数少，现在的用深度网络，就叫深度质量网络（DQN）。</p>\n<h3 id=\"2-Sarsa\"><a href=\"#2-Sarsa\" class=\"headerlink\" title=\"2. Sarsa\"></a>2. Sarsa</h3><p>● 感性认识：内容来自 - <a href=\"https://zhuanlan.zhihu.com/p/24860793\" title=\"Title\" target=\"_blank\" rel=\"external\">什么是 Sarsa - 莫烦</a> </p>\n<p>sarsa算法估计的是动作值函数(Q函数)而非状态值函数。</p>\n<h4 id=\"2-1-Sarsa-决策\"><a href=\"#2-1-Sarsa-决策\" class=\"headerlink\" title=\"2.1. Sarsa 决策\"></a>2.1. Sarsa 决策</h4><hr>\n<center><img src=\"/image/RL/5/2-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>Sarsa 的决策部分和 Q-learning 一模一样, 因为使用的是 Q 表的形式决策, 所以在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩. 但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>\n<h4 id=\"2-2-Sarsa-更新行为准则\"><a href=\"#2-2-Sarsa-更新行为准则\" class=\"headerlink\" title=\"2.2. Sarsa 更新行为准则\"></a>2.2. Sarsa 更新行为准则</h4><hr>\n<center><img src=\"/image/RL/5/2-2.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"3-Sarsa-VS-Q-learning\"><a href=\"#3-Sarsa-VS-Q-learning\" class=\"headerlink\" title=\"3. Sarsa VS Q-learning\"></a>3. Sarsa VS Q-learning</h3><p>内容来自：<a href=\"https://zhuanlan.zhihu.com/p/24860793\" title=\"Title\" target=\"_blank\" rel=\"external\">什么是 Sarsa - 莫烦</a> </p>\n<hr>\n<center><img src=\"/image/RL/5/3-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>从算法来看, 这就是他们两最大的不同之处了. 因为 Sarsa 是说到做到型, 所以我们也叫他 on-policy在线学习, 学着自己在做的事情. 而 Q learning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法。</p>\n<hr>\n<center><img src=\"/image/RL/5/3-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>为什么说他勇敢呢, 因为 Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用 Sarsa 方法的不同之处。</p>\n<p>● 开源推荐：<a href=\"https://github.com/openai/gym\" title=\"Title\" target=\"_blank\" rel=\"external\">OpenAI Gym</a> </p>\n<h3 id=\"4-时间差分法（TD）VS-蒙特卡罗（-MC）\"><a href=\"#4-时间差分法（TD）VS-蒙特卡罗（-MC）\" class=\"headerlink\" title=\"4. 时间差分法（TD）VS 蒙特卡罗（ MC）\"></a>4. 时间差分法（TD）VS 蒙特卡罗（ MC）</h3><p>内容来自：<a href=\"https://www.zhihu.com/question/62388365/answer/218012513\" title=\"Title\" target=\"_blank\" rel=\"external\">知乎：强化学习中时间差分(TD)和蒙特卡洛(MC)方法各自的优劣？- 远方的梦回答</a> </p>\n<p>TD对应的sarsa/qlearning和MC对应的control算法进行比较:</p>\n<p>● 1. MC计算量更大，更新缓慢。</p>\n<p>可以这样理解：对MC而言，其return如下：</p>\n<hr>\n<center><img src=\"/image/RL/5/4-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>所以MC必须在整轮episode迭代结束后进行更新(也就是一盘围棋要下完才更新一次)，而TD(0)在下一个状态s(t+1)后就可以进行更新,其return如下：</p>\n<hr>\n<center><img src=\"/image/RL/5/4-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>● 2.MC不能用于continuous task（比如倒立摆）， 而TD(0)可以：理由同上。</p>\n<p>● 3.相反于第二点，MC能用于一些围棋之类的规划较深的任务。</p>\n<p>这个“深”字体现在：先阶段某个action对以后未来的action决策都有较大的影响（象棋，围棋之类的，想必深有体会）。而对于倒立摆这种问题，我现在让它向左的这个决策对于其1分钟后而言，其实基本没什么关系。参考下面两张图：</p>\n<p>图片来自：<a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf\" title=\"Title\" target=\"_blank\" rel=\"external\">Model-Free Prediction</a> </p>\n<p> <strong>蒙特卡罗（ MC）：</strong></p>\n<hr>\n<center><img src=\"/image/RL/5/4-3.png\" alt=\"RL\"></center> \n\n<hr>\n<p> <strong>时间差分法（TD）：</strong></p>\n<hr>\n<center><img src=\"/image/RL/5/4-4.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.2</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\" target=\"_blank\" rel=\"external\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\" target=\"_blank\" rel=\"external\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\" target=\"_blank\" rel=\"external\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/5116332.html\" title=\"Title\" target=\"_blank\" rel=\"external\">时间差分学习(Q learning,Sarsa learning)</a> </p>\n</li>\n<li><p><a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\" title=\"Title\" target=\"_blank\" rel=\"external\">A Painless Q-learning Tutorial</a></p>\n</li>\n<li><p><a href=\"http://blog.csdn.net/itplus/article/details/9361915\" title=\"Title\" target=\"_blank\" rel=\"external\">一个 Q-learning 算法的简明教程</a></p>\n</li>\n</ul>\n","excerpt":"","more":"<h4 id=\"Bellman最优解策略-时间差分法（Temporal-Difference）\"><a href=\"#Bellman最优解策略-时间差分法（Temporal-Difference）\" class=\"headerlink\" title=\"Bellman最优解策略 - 时间差分法（Temporal Difference）\"></a>Bellman最优解策略 - 时间差分法（Temporal Difference）</h4><p>蒙特卡洛算法需要采样完成一个轨迹之后，才能进行值估计（value estimation），这样蒙特卡洛速度很慢，主要原因在于蒙特卡洛没有充分的利用强化学习任务的 MDP 结构。但是，TD 充分利用了 “MC”和 动态规划的思想，做到了更加高效率的免模型学习。</p>\n<p>强化学习算法可以分为在同策略(on-policy)和异策略(off-policy)两类：</p>\n<p>若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。</p>\n<p>若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。</p>\n<p>● Q-learning属于离策略(off-policy)算法。</p>\n<p>● Sarsa属于在策略(on-policy)算法。</p>\n<p>本文内容来自<a href=\"https://zhuanlan.zhihu.com/morvan\" title=\"Title\">知乎 - 莫烦</a> ，以小时候写作业为例子，看看TD是如何来决策：</p>\n<hr>\n<center><img src=\"/image/RL/5/0-0.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"1-Q-learning\"><a href=\"#1-Q-learning\" class=\"headerlink\" title=\"1. Q-learning\"></a>1. Q-learning</h3><p>● 感性认识：内容来自 - <a href=\"https://zhuanlan.zhihu.com/p/24808797\" title=\"Title\">什么是 Q-Learning - 莫烦</a> </p>\n<h4 id=\"1-1-Q-Learning-决策\"><a href=\"#1-1-Q-Learning-决策\" class=\"headerlink\" title=\"1.1. Q-Learning 决策\"></a>1.1. Q-Learning 决策</h4><hr>\n<center><img src=\"/image/RL/5/1-1.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-2-Q-Learning-更新\"><a href=\"#1-2-Q-Learning-更新\" class=\"headerlink\" title=\"1.2. Q-Learning 更新\"></a>1.2. Q-Learning 更新</h4><hr>\n<center><img src=\"/image/RL/5/1-2.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-3-Q-Learning-整体算法\"><a href=\"#1-3-Q-Learning-整体算法\" class=\"headerlink\" title=\"1.3. Q-Learning 整体算法\"></a>1.3. Q-Learning 整体算法</h4><hr>\n<center><img src=\"/image/RL/5/1-3.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"1-4-Q-Learning-中的-Lambda\"><a href=\"#1-4-Q-Learning-中的-Lambda\" class=\"headerlink\" title=\"1.4. Q-Learning 中的 Lambda\"></a>1.4. Q-Learning 中的 Lambda</h4><hr>\n<center><img src=\"/image/RL/5/1-4.png\" alt=\"RL\"></center> \n\n<hr>\n<h4 id=\"DQN（Deep-Q-learning）\"><a href=\"#DQN（Deep-Q-learning）\" class=\"headerlink\" title=\"DQN（Deep Q-learning）\"></a>DQN（Deep Q-learning）</h4><p>Q-learning中因为需要下一状态的所有动作的最大Q，loss的也有个特点:网络要多前向计算一次，才能得到下一状态的所有动作的Q。也是因为这个loss的原因，这里的DQN等于说是把CNN用作生成Q的一个函数。原来的质量表达Q是个矩阵，参数少，现在的用深度网络，就叫深度质量网络（DQN）。</p>\n<h3 id=\"2-Sarsa\"><a href=\"#2-Sarsa\" class=\"headerlink\" title=\"2. Sarsa\"></a>2. Sarsa</h3><p>● 感性认识：内容来自 - <a href=\"https://zhuanlan.zhihu.com/p/24860793\" title=\"Title\">什么是 Sarsa - 莫烦</a> </p>\n<p>sarsa算法估计的是动作值函数(Q函数)而非状态值函数。</p>\n<h4 id=\"2-1-Sarsa-决策\"><a href=\"#2-1-Sarsa-决策\" class=\"headerlink\" title=\"2.1. Sarsa 决策\"></a>2.1. Sarsa 决策</h4><hr>\n<center><img src=\"/image/RL/5/2-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>Sarsa 的决策部分和 Q-learning 一模一样, 因为使用的是 Q 表的形式决策, 所以在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩. 但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>\n<h4 id=\"2-2-Sarsa-更新行为准则\"><a href=\"#2-2-Sarsa-更新行为准则\" class=\"headerlink\" title=\"2.2. Sarsa 更新行为准则\"></a>2.2. Sarsa 更新行为准则</h4><hr>\n<center><img src=\"/image/RL/5/2-2.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"3-Sarsa-VS-Q-learning\"><a href=\"#3-Sarsa-VS-Q-learning\" class=\"headerlink\" title=\"3. Sarsa VS Q-learning\"></a>3. Sarsa VS Q-learning</h3><p>内容来自：<a href=\"https://zhuanlan.zhihu.com/p/24860793\" title=\"Title\">什么是 Sarsa - 莫烦</a> </p>\n<hr>\n<center><img src=\"/image/RL/5/3-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>从算法来看, 这就是他们两最大的不同之处了. 因为 Sarsa 是说到做到型, 所以我们也叫他 on-policy在线学习, 学着自己在做的事情. 而 Q learning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法。</p>\n<hr>\n<center><img src=\"/image/RL/5/3-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>为什么说他勇敢呢, 因为 Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用 Sarsa 方法的不同之处。</p>\n<p>● 开源推荐：<a href=\"https://github.com/openai/gym\" title=\"Title\">OpenAI Gym</a> </p>\n<h3 id=\"4-时间差分法（TD）VS-蒙特卡罗（-MC）\"><a href=\"#4-时间差分法（TD）VS-蒙特卡罗（-MC）\" class=\"headerlink\" title=\"4. 时间差分法（TD）VS 蒙特卡罗（ MC）\"></a>4. 时间差分法（TD）VS 蒙特卡罗（ MC）</h3><p>内容来自：<a href=\"https://www.zhihu.com/question/62388365/answer/218012513\" title=\"Title\">知乎：强化学习中时间差分(TD)和蒙特卡洛(MC)方法各自的优劣？- 远方的梦回答</a> </p>\n<p>TD对应的sarsa/qlearning和MC对应的control算法进行比较:</p>\n<p>● 1. MC计算量更大，更新缓慢。</p>\n<p>可以这样理解：对MC而言，其return如下：</p>\n<hr>\n<center><img src=\"/image/RL/5/4-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>所以MC必须在整轮episode迭代结束后进行更新(也就是一盘围棋要下完才更新一次)，而TD(0)在下一个状态s(t+1)后就可以进行更新,其return如下：</p>\n<hr>\n<center><img src=\"/image/RL/5/4-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>● 2.MC不能用于continuous task（比如倒立摆）， 而TD(0)可以：理由同上。</p>\n<p>● 3.相反于第二点，MC能用于一些围棋之类的规划较深的任务。</p>\n<p>这个“深”字体现在：先阶段某个action对以后未来的action决策都有较大的影响（象棋，围棋之类的，想必深有体会）。而对于倒立摆这种问题，我现在让它向左的这个决策对于其1分钟后而言，其实基本没什么关系。参考下面两张图：</p>\n<p>图片来自：<a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf\" title=\"Title\">Model-Free Prediction</a> </p>\n<p> <strong>蒙特卡罗（ MC）：</strong></p>\n<hr>\n<center><img src=\"/image/RL/5/4-3.png\" alt=\"RL\"></center> \n\n<hr>\n<p> <strong>时间差分法（TD）：</strong></p>\n<hr>\n<center><img src=\"/image/RL/5/4-4.png\" alt=\"RL\"></center> \n\n<hr>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.2</a> </p>\n</li>\n<li><p><a href=\"https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/\" title=\"Title\">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\" title=\"Title\">Pythonではじめる強化学習</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/5116332.html\" title=\"Title\">时间差分学习(Q learning,Sarsa learning)</a> </p>\n</li>\n<li><p><a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\" title=\"Title\">A Painless Q-learning Tutorial</a></p>\n</li>\n<li><p><a href=\"http://blog.csdn.net/itplus/article/details/9361915\" title=\"Title\">一个 Q-learning 算法的简明教程</a></p>\n</li>\n</ul>\n"},{"title":"“御茶ノ水” 乐器街之行","lang":"zh","date":"2016-11-30T02:14:15.000Z","_content":"<center>![sumiao_2](/image/Yamaki/Yamaki-1.jpg)</center>.   \n\n&#8195;&#8195;在东京没有自己的一把吉他，本想让人从家把吉他寄过来，无奈邮费贵了点，特别去了位于东京千代田区的吉他圣地\"御茶ノ水\" 乐器街物色练手吉他。  \n\n&#8195;&#8195;在御茶之水街上，入驻了约十几二十家各式的琴行，每家的规模都非常大，商品种类繁多，分类很细，每层主营不同种类品牌的吉他及周边产品。御茶水音乐文化氛围也很好，御茶ノ水店员的服务很優しい，而且有些琴艺非常不错，每家店基本都有中古的产品出售。  \n在一家卖中古的吉他店里，物色到了两把吉他，这家的店员帮我调音，看到他没有用调音器，挨拶后得知他练了20多年的吉他。他说像他这样的人在御茶水很常见。  \n\n&#8195;&#8195;两把吉他中，一把是Yamaki（日本一个老牌子,在70年代多以手工做琴为主）于1980年代制造，虽然琴身表面有点磨碎，但是琴声弹起来非常不错。  \n\n<center>![sumiao_2](/image/Yamaki/Yamaki-2.jpg)</center>.   \n\n&#8195;&#8195;另外一把是Yamaha（相信大家对这个牌子并不陌生），39寸，但是小吉他的声音共鸣方面感觉不是很好。  \n\n&#8195;&#8195;最后选了Yamaki，两万日元不到，希望在接下来的日子跟它一起好好陪伴。  \n","source":"_posts/zh/“御茶ノ水” 乐器街之行.md","raw":"---\ntitle: “御茶ノ水” 乐器街之行\nlang: zh\ndate: 2016-11-30 11:14:15\n---\n<center>![sumiao_2](/image/Yamaki/Yamaki-1.jpg)</center>.   \n\n&#8195;&#8195;在东京没有自己的一把吉他，本想让人从家把吉他寄过来，无奈邮费贵了点，特别去了位于东京千代田区的吉他圣地\"御茶ノ水\" 乐器街物色练手吉他。  \n\n&#8195;&#8195;在御茶之水街上，入驻了约十几二十家各式的琴行，每家的规模都非常大，商品种类繁多，分类很细，每层主营不同种类品牌的吉他及周边产品。御茶水音乐文化氛围也很好，御茶ノ水店员的服务很優しい，而且有些琴艺非常不错，每家店基本都有中古的产品出售。  \n在一家卖中古的吉他店里，物色到了两把吉他，这家的店员帮我调音，看到他没有用调音器，挨拶后得知他练了20多年的吉他。他说像他这样的人在御茶水很常见。  \n\n&#8195;&#8195;两把吉他中，一把是Yamaki（日本一个老牌子,在70年代多以手工做琴为主）于1980年代制造，虽然琴身表面有点磨碎，但是琴声弹起来非常不错。  \n\n<center>![sumiao_2](/image/Yamaki/Yamaki-2.jpg)</center>.   \n\n&#8195;&#8195;另外一把是Yamaha（相信大家对这个牌子并不陌生），39寸，但是小吉他的声音共鸣方面感觉不是很好。  \n\n&#8195;&#8195;最后选了Yamaki，两万日元不到，希望在接下来的日子跟它一起好好陪伴。  \n","slug":"zh-“御茶ノ水”-乐器街之行","published":1,"updated":"2017-12-09T14:05:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yw0015og645izl78ij","content":"<p><center><img src=\"/image/Yamaki/Yamaki-1.jpg\" alt=\"sumiao_2\"></center>.   </p>\n<p>&#8195;&#8195;在东京没有自己的一把吉他，本想让人从家把吉他寄过来，无奈邮费贵了点，特别去了位于东京千代田区的吉他圣地”御茶ノ水” 乐器街物色练手吉他。  </p>\n<p>&#8195;&#8195;在御茶之水街上，入驻了约十几二十家各式的琴行，每家的规模都非常大，商品种类繁多，分类很细，每层主营不同种类品牌的吉他及周边产品。御茶水音乐文化氛围也很好，御茶ノ水店员的服务很優しい，而且有些琴艺非常不错，每家店基本都有中古的产品出售。<br>在一家卖中古的吉他店里，物色到了两把吉他，这家的店员帮我调音，看到他没有用调音器，挨拶后得知他练了20多年的吉他。他说像他这样的人在御茶水很常见。  </p>\n<p>&#8195;&#8195;两把吉他中，一把是Yamaki（日本一个老牌子,在70年代多以手工做琴为主）于1980年代制造，虽然琴身表面有点磨碎，但是琴声弹起来非常不错。  </p>\n<p><center><img src=\"/image/Yamaki/Yamaki-2.jpg\" alt=\"sumiao_2\"></center>.   </p>\n<p>&#8195;&#8195;另外一把是Yamaha（相信大家对这个牌子并不陌生），39寸，但是小吉他的声音共鸣方面感觉不是很好。  </p>\n<p>&#8195;&#8195;最后选了Yamaki，两万日元不到，希望在接下来的日子跟它一起好好陪伴。  </p>\n","excerpt":"","more":"<p><center><img src=\"/image/Yamaki/Yamaki-1.jpg\" alt=\"sumiao_2\"></center>.   </p>\n<p>&#8195;&#8195;在东京没有自己的一把吉他，本想让人从家把吉他寄过来，无奈邮费贵了点，特别去了位于东京千代田区的吉他圣地”御茶ノ水” 乐器街物色练手吉他。  </p>\n<p>&#8195;&#8195;在御茶之水街上，入驻了约十几二十家各式的琴行，每家的规模都非常大，商品种类繁多，分类很细，每层主营不同种类品牌的吉他及周边产品。御茶水音乐文化氛围也很好，御茶ノ水店员的服务很優しい，而且有些琴艺非常不错，每家店基本都有中古的产品出售。<br>在一家卖中古的吉他店里，物色到了两把吉他，这家的店员帮我调音，看到他没有用调音器，挨拶后得知他练了20多年的吉他。他说像他这样的人在御茶水很常见。  </p>\n<p>&#8195;&#8195;两把吉他中，一把是Yamaki（日本一个老牌子,在70年代多以手工做琴为主）于1980年代制造，虽然琴身表面有点磨碎，但是琴声弹起来非常不错。  </p>\n<p><center><img src=\"/image/Yamaki/Yamaki-2.jpg\" alt=\"sumiao_2\"></center>.   </p>\n<p>&#8195;&#8195;另外一把是Yamaha（相信大家对这个牌子并不陌生），39寸，但是小吉他的声音共鸣方面感觉不是很好。  </p>\n<p>&#8195;&#8195;最后选了Yamaki，两万日元不到，希望在接下来的日子跟它一起好好陪伴。  </p>\n"},{"title":"人物素描1.0","lang":"zh","date":"2016-11-29T16:13:58.000Z","_content":"\n这次的素描对象是一位新疆女生，原图：\n<center>![sumiao_2](/image/sumiao/sumiao_2.jpg)</center>\n\n本次作图训练右手画结构,左手调明暗，首先是整体结构绘图：\n<center>![sumiao_2](/image/sumiao/sumiao_3.jpg)</center>\n\n人物的眼睛是全图的最重要部分之一，画法步骤参考徐怀芳的《素描人物头像结构训练法》之眼睛结构图解：\n<center>![sumiao_2](/image/sumiao/sumiao_4.jpg)</center>\n\n其次是明暗修改：\n<center>![sumiao_2](/image/sumiao/sumiao_5.jpg)</center>\n\n人物的左眼有稍皱眉的表情，估计是天气寒冷飘雪缘故，这里可以看到左眼对比右眼稍稍闭目的迹象，这里处理也花了一些时间：\n<center>![sumiao_2](/image/sumiao/sumiao_6.jpg)</center>\n\n最后是用软性炭笔将整体画的颜色调深。这是博主一个习惯，一般用xB铅笔画完结构后再将炭笔调明暗。完成图：\n<center>![sumiao_2](/image/sumiao/sumiao_7.jpg)</center>\n\n还有一点没有把雪花表现出来。因为暂时还不知道如何画才能把雪花画好……试画了几次，最后都要重画。画好雪花这也是课后要努力的地方。博主喜欢冷军的素描，希望绘画朝着他的方向进步。\n\n\n更多素描分享：\n\n- [人物素描_(北)](https://www.hjt.so/?p=324&lang=zh \"Title\") \n\n- [人物素描_(南)](https://www.hjt.so/?p=2752&lang=zh \"Title\") \n\n- [人物素描_(日)](https://www.hjt.so/?p=3111&lang=zh \"Title\") ","source":"_posts/zh/人物素描1.0.md","raw":"---\ntitle: 人物素描1.0\nlang: zh\ndate: 2016-11-30 01:13:58\ntags: Sketches\n---\n\n这次的素描对象是一位新疆女生，原图：\n<center>![sumiao_2](/image/sumiao/sumiao_2.jpg)</center>\n\n本次作图训练右手画结构,左手调明暗，首先是整体结构绘图：\n<center>![sumiao_2](/image/sumiao/sumiao_3.jpg)</center>\n\n人物的眼睛是全图的最重要部分之一，画法步骤参考徐怀芳的《素描人物头像结构训练法》之眼睛结构图解：\n<center>![sumiao_2](/image/sumiao/sumiao_4.jpg)</center>\n\n其次是明暗修改：\n<center>![sumiao_2](/image/sumiao/sumiao_5.jpg)</center>\n\n人物的左眼有稍皱眉的表情，估计是天气寒冷飘雪缘故，这里可以看到左眼对比右眼稍稍闭目的迹象，这里处理也花了一些时间：\n<center>![sumiao_2](/image/sumiao/sumiao_6.jpg)</center>\n\n最后是用软性炭笔将整体画的颜色调深。这是博主一个习惯，一般用xB铅笔画完结构后再将炭笔调明暗。完成图：\n<center>![sumiao_2](/image/sumiao/sumiao_7.jpg)</center>\n\n还有一点没有把雪花表现出来。因为暂时还不知道如何画才能把雪花画好……试画了几次，最后都要重画。画好雪花这也是课后要努力的地方。博主喜欢冷军的素描，希望绘画朝着他的方向进步。\n\n\n更多素描分享：\n\n- [人物素描_(北)](https://www.hjt.so/?p=324&lang=zh \"Title\") \n\n- [人物素描_(南)](https://www.hjt.so/?p=2752&lang=zh \"Title\") \n\n- [人物素描_(日)](https://www.hjt.so/?p=3111&lang=zh \"Title\") ","slug":"zh-人物素描1-0","published":1,"updated":"2019-03-25T13:15:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yw0017og64paeuagu3","content":"<p>这次的素描对象是一位新疆女生，原图：</p>\n<center><img src=\"/image/sumiao/sumiao_2.jpg\" alt=\"sumiao_2\"></center>\n\n<p>本次作图训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao/sumiao_3.jpg\" alt=\"sumiao_2\"></center>\n\n<p>人物的眼睛是全图的最重要部分之一，画法步骤参考徐怀芳的《素描人物头像结构训练法》之眼睛结构图解：</p>\n<center><img src=\"/image/sumiao/sumiao_4.jpg\" alt=\"sumiao_2\"></center>\n\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao/sumiao_5.jpg\" alt=\"sumiao_2\"></center>\n\n<p>人物的左眼有稍皱眉的表情，估计是天气寒冷飘雪缘故，这里可以看到左眼对比右眼稍稍闭目的迹象，这里处理也花了一些时间：</p>\n<center><img src=\"/image/sumiao/sumiao_6.jpg\" alt=\"sumiao_2\"></center>\n\n<p>最后是用软性炭笔将整体画的颜色调深。这是博主一个习惯，一般用xB铅笔画完结构后再将炭笔调明暗。完成图：</p>\n<center><img src=\"/image/sumiao/sumiao_7.jpg\" alt=\"sumiao_2\"></center>\n\n<p>还有一点没有把雪花表现出来。因为暂时还不知道如何画才能把雪花画好……试画了几次，最后都要重画。画好雪花这也是课后要努力的地方。博主喜欢冷军的素描，希望绘画朝着他的方向进步。</p>\n<p>更多素描分享：</p>\n<ul>\n<li><p><a href=\"https://www.hjt.so/?p=324&amp;lang=zh\" title=\"Title\" target=\"_blank\" rel=\"external\">人物素描_(北)</a> </p>\n</li>\n<li><p><a href=\"https://www.hjt.so/?p=2752&amp;lang=zh\" title=\"Title\" target=\"_blank\" rel=\"external\">人物素描_(南)</a> </p>\n</li>\n<li><p><a href=\"https://www.hjt.so/?p=3111&amp;lang=zh\" title=\"Title\" target=\"_blank\" rel=\"external\">人物素描_(日)</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<p>这次的素描对象是一位新疆女生，原图：</p>\n<center><img src=\"/image/sumiao/sumiao_2.jpg\" alt=\"sumiao_2\"></center>\n\n<p>本次作图训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao/sumiao_3.jpg\" alt=\"sumiao_2\"></center>\n\n<p>人物的眼睛是全图的最重要部分之一，画法步骤参考徐怀芳的《素描人物头像结构训练法》之眼睛结构图解：</p>\n<center><img src=\"/image/sumiao/sumiao_4.jpg\" alt=\"sumiao_2\"></center>\n\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao/sumiao_5.jpg\" alt=\"sumiao_2\"></center>\n\n<p>人物的左眼有稍皱眉的表情，估计是天气寒冷飘雪缘故，这里可以看到左眼对比右眼稍稍闭目的迹象，这里处理也花了一些时间：</p>\n<center><img src=\"/image/sumiao/sumiao_6.jpg\" alt=\"sumiao_2\"></center>\n\n<p>最后是用软性炭笔将整体画的颜色调深。这是博主一个习惯，一般用xB铅笔画完结构后再将炭笔调明暗。完成图：</p>\n<center><img src=\"/image/sumiao/sumiao_7.jpg\" alt=\"sumiao_2\"></center>\n\n<p>还有一点没有把雪花表现出来。因为暂时还不知道如何画才能把雪花画好……试画了几次，最后都要重画。画好雪花这也是课后要努力的地方。博主喜欢冷军的素描，希望绘画朝着他的方向进步。</p>\n<p>更多素描分享：</p>\n<ul>\n<li><p><a href=\"https://www.hjt.so/?p=324&amp;lang=zh\" title=\"Title\">人物素描_(北)</a> </p>\n</li>\n<li><p><a href=\"https://www.hjt.so/?p=2752&amp;lang=zh\" title=\"Title\">人物素描_(南)</a> </p>\n</li>\n<li><p><a href=\"https://www.hjt.so/?p=3111&amp;lang=zh\" title=\"Title\">人物素描_(日)</a> </p>\n</li>\n</ul>\n"},{"title":"人物素描2.0","lang":"zh","date":"2016-11-29T16:13:59.000Z","_content":"\n这次的素描对象是一位哈尔滨女生，原图：\n\n<center>![W](/image/sumiao2/W.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![W1](/image/sumiao2/W1.jpg)</center>\n\n----------------------------------------  \n\n其次是明暗修改：\n\n<center>![W2](/image/sumiao2/W2.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![W3](/image/sumiao2/W3.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![W4](/image/sumiao2/W4.jpg)</center>\n\n----------------------------------------  ","source":"_posts/zh/人物素描2.0.md","raw":"---\ntitle: 人物素描2.0\nlang: zh\ndate: 2016-11-30 01:13:59\ntags: Sketches\n---\n\n这次的素描对象是一位哈尔滨女生，原图：\n\n<center>![W](/image/sumiao2/W.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![W1](/image/sumiao2/W1.jpg)</center>\n\n----------------------------------------  \n\n其次是明暗修改：\n\n<center>![W2](/image/sumiao2/W2.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![W3](/image/sumiao2/W3.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![W4](/image/sumiao2/W4.jpg)</center>\n\n----------------------------------------  ","slug":"zh-人物素描2-0","published":1,"updated":"2019-03-25T13:16:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8yz0019og64ce0l2d8n","content":"<p>这次的素描对象是一位哈尔滨女生，原图：</p>\n<center><img src=\"/image/sumiao2/W.jpg\" alt=\"W\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao2/W1.jpg\" alt=\"W1\"></center>\n\n<hr>\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao2/W2.jpg\" alt=\"W2\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao2/W3.jpg\" alt=\"W3\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao2/W4.jpg\" alt=\"W4\"></center>\n\n<hr>\n","excerpt":"","more":"<p>这次的素描对象是一位哈尔滨女生，原图：</p>\n<center><img src=\"/image/sumiao2/W.jpg\" alt=\"W\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao2/W1.jpg\" alt=\"W1\"></center>\n\n<hr>\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao2/W2.jpg\" alt=\"W2\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao2/W3.jpg\" alt=\"W3\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao2/W4.jpg\" alt=\"W4\"></center>\n\n<hr>\n"},{"title":"人物素描3.0","lang":"zh","date":"2017-12-07T16:18:01.000Z","_content":"\n这次的素描对象是一位温州女生，原图：\n\n<center>![ZYY](/image/sumiao3/ZYY.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![ZYY](/image/sumiao3/ZYY_1.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao3/ZYY_2.jpg)</center>\n\n----------------------------------------  \n\n其次是明暗修改：\n\n<center>![ZYY](/image/sumiao3/ZYY_3.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![ZYY](/image/sumiao3/ZYY_4.jpg)</center>\n\n----------------------------------------  ","source":"_posts/zh/人物素描3.0.md","raw":"---\ntitle: 人物素描3.0\nlang: zh\ndate: 2017-12-08 01:18:01\ntags: Sketches\n---\n\n这次的素描对象是一位温州女生，原图：\n\n<center>![ZYY](/image/sumiao3/ZYY.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![ZYY](/image/sumiao3/ZYY_1.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao3/ZYY_2.jpg)</center>\n\n----------------------------------------  \n\n其次是明暗修改：\n\n<center>![ZYY](/image/sumiao3/ZYY_3.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![ZYY](/image/sumiao3/ZYY_4.jpg)</center>\n\n----------------------------------------  ","slug":"zh-人物素描3-0","published":1,"updated":"2019-03-25T13:16:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z0001bog64pmqubld1","content":"<p>这次的素描对象是一位温州女生，原图：</p>\n<center><img src=\"/image/sumiao3/ZYY.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao3/ZYY_1.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao3/ZYY_2.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao3/ZYY_3.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao3/ZYY_4.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n","excerpt":"","more":"<p>这次的素描对象是一位温州女生，原图：</p>\n<center><img src=\"/image/sumiao3/ZYY.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao3/ZYY_1.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao3/ZYY_2.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>其次是明暗修改：</p>\n<center><img src=\"/image/sumiao3/ZYY_3.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao3/ZYY_4.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n"},{"title":"人物素描4.0","lang":"zh","date":"2017-12-11T16:18:01.000Z","_content":"\n这次的素描对象是一位济南女生，原图：\n\n<center>![ZYY](/image/sumiao4/MZ.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![ZYY](/image/sumiao4/MZ_1.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao4/MZ_2.jpg)</center>\n\n----------------------------------------  \n\n其次用软性炭笔明暗修改：\n\n<center>![ZYY](/image/sumiao4/MZ_3.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao4/MZ_4.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![ZYY](/image/sumiao4/MZ_5.jpg)</center>\n\n----------------------------------------  ","source":"_posts/zh/人物素描4.0.md","raw":"---\ntitle: 人物素描4.0\nlang: zh\ndate: 2017-12-12 01:18:01\ntags: Sketches\n---\n\n这次的素描对象是一位济南女生，原图：\n\n<center>![ZYY](/image/sumiao4/MZ.jpg)</center>\n\n----------------------------------------  \n\n本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：\n\n<center>![ZYY](/image/sumiao4/MZ_1.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao4/MZ_2.jpg)</center>\n\n----------------------------------------  \n\n其次用软性炭笔明暗修改：\n\n<center>![ZYY](/image/sumiao4/MZ_3.jpg)</center>\n\n----------------------------------------  \n\n进一步修改：\n\n<center>![ZYY](/image/sumiao4/MZ_4.jpg)</center>\n\n----------------------------------------  \n\n完成图：\n\n<center>![ZYY](/image/sumiao4/MZ_5.jpg)</center>\n\n----------------------------------------  ","slug":"zh-人物素描4-0","published":1,"updated":"2019-03-25T13:16:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z1001dog64g012l81d","content":"<p>这次的素描对象是一位济南女生，原图：</p>\n<center><img src=\"/image/sumiao4/MZ.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao4/MZ_1.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao4/MZ_2.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>其次用软性炭笔明暗修改：</p>\n<center><img src=\"/image/sumiao4/MZ_3.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao4/MZ_4.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao4/MZ_5.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n","excerpt":"","more":"<p>这次的素描对象是一位济南女生，原图：</p>\n<center><img src=\"/image/sumiao4/MZ.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>本次作图依旧训练右手画结构,左手调明暗，首先是整体结构绘图：</p>\n<center><img src=\"/image/sumiao4/MZ_1.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao4/MZ_2.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>其次用软性炭笔明暗修改：</p>\n<center><img src=\"/image/sumiao4/MZ_3.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>进一步修改：</p>\n<center><img src=\"/image/sumiao4/MZ_4.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n<p>完成图：</p>\n<center><img src=\"/image/sumiao4/MZ_5.jpg\" alt=\"ZYY\"></center>\n\n<hr>\n"},{"title":"游泳动图","lang":"zh","date":"2016-11-29T16:13:55.000Z","_content":"\n（本文来自网上，方便矫正泳姿）游泳一般有以下四种最常见的姿势：自由泳、蛙泳、蝶泳、仰泳。下面来具体细说这四种动作以及用动态的图片分解姿势。\n\n--------------\n## 蛙泳 Breaststroke\n蛙泳是最平常的一种泳姿，就如青蛙游水。\n动作要领：双手先划、胳膊伸后蹬腿，手划的时候腿不要动，手先收腿再收，手腿伸直并拢再滑行一会儿（大约1~2秒左右）。\n配合动作：双手前伸，手掌掌心向外，倾斜45°左右，小拇指向上，双手向外划时候冒头换气（吸气），双手内划时腿收憋气，双手伸过头的同时蹬腿吐气。  \n\n<center>![swim](/image/swim/breaststroke_1.gif)</center>. \n\n<center>![swim](/image/swim/breaststroke_2.gif)</center>. \n\n<center>![swim](/image/swim/breaststroke_3.gif)</center>. \n\n- 关键点                                                                                                \n1、手臂：先向外划相对于抱水姿势，再向下、向内、伸展保持原来姿势。  \n2、二腿：收腿、翻脚、蹬夹水，再滑行。(蹬腿动作就如青蛙游水)             \n3、呼吸：手臂有力外划时、抬头吸气，双手内划腿收头在水里憋气，双手伸过头，同时蹬腿呼气。                               \n4、节奏：双臂伸展开来时，蹬腿动作已经完成。\n\n--------------\n## 自由泳 Free style\n自由泳：依靠手臂划水，腿部打水产生推动力。\n\n<center>![swim](/image/swim/freestyle_1.gif)</center>. \n\n<center>![swim](/image/swim/freestyle_2.gif)</center>. \n\n<center>![swim](/image/swim/freestyle_3.gif)</center>. \n\n- 关键点 \n1、身体要平直，腿面要刚刚好露出水面时打水\n2、身体入水后，手、肘、前伸伸展、抱水，手臂向内，向上划。出手后，手臂要放松并经空中前移，继续下一个动作，循环反复。\n3、双腿并拢，膝部稍微弯曲，向下打水。（鞭状打水）\n4、头朝左、朝右肩膀出水面转动时呼吸。\n5、一般的节奏：左右腿打水六次，两臂各划水一次、呼吸一次。\n\n--------------\n## 蝶泳 Butterfly\n蝶泳顾名思义就如蝴蝶状飞翔游水。   \n\n<center>![swim](/image/swim/butterfly_1.gif)</center>. \n\n<center>![swim](/image/swim/butterfly_2.gif)</center>. \n\n<center>![swim](/image/swim/butterfly_3.gif)</center>.     \n                                                    \n- 关键点 \n1、肩部与水面齐平，头部比手臂先入水，抬头的时候比较低，入水后手臂沿曲线向外和抱水，再下划抓水，两手分开到肩宽，弯肘划水。   \n2、打腿要点：开始时候双腿并拢，脚后跟一露出水面，双脚弯曲向下打水。整体节奏双手划一次，二腿同时打水二次。\n\n--------------\n## 仰泳 Backstroke\n仰泳又称背泳，是一种仰卧在水中的游泳姿势。\n\n<center>![swim](/image/swim/backstroke_1.gif)</center>. \n\n<center>![swim](/image/swim/backstroke_2.gif)</center>. \n\n<center>![swim](/image/swim/backstroke_3.gif)</center>.     \n          \n- 关键点 \n1、就如躺在水面上，两耳刚刚进入水中，臀部恰好就在水面下，面部向上，双脚刚好露出水面。\n2、手臂入水向后抱水向下划。\n3、双腿并拢上下轮流打水，膝部弯曲不露出水面。一般节奏是头部保持稳定不能左右摇摆，二手臂各划一次，二腿一共打水六次，呼吸一次。","source":"_posts/zh/游泳动图.md","raw":"---\ntitle: 游泳动图\nlang: zh\ndate: 2016-11-30 01:13:55\n---\n\n（本文来自网上，方便矫正泳姿）游泳一般有以下四种最常见的姿势：自由泳、蛙泳、蝶泳、仰泳。下面来具体细说这四种动作以及用动态的图片分解姿势。\n\n--------------\n## 蛙泳 Breaststroke\n蛙泳是最平常的一种泳姿，就如青蛙游水。\n动作要领：双手先划、胳膊伸后蹬腿，手划的时候腿不要动，手先收腿再收，手腿伸直并拢再滑行一会儿（大约1~2秒左右）。\n配合动作：双手前伸，手掌掌心向外，倾斜45°左右，小拇指向上，双手向外划时候冒头换气（吸气），双手内划时腿收憋气，双手伸过头的同时蹬腿吐气。  \n\n<center>![swim](/image/swim/breaststroke_1.gif)</center>. \n\n<center>![swim](/image/swim/breaststroke_2.gif)</center>. \n\n<center>![swim](/image/swim/breaststroke_3.gif)</center>. \n\n- 关键点                                                                                                \n1、手臂：先向外划相对于抱水姿势，再向下、向内、伸展保持原来姿势。  \n2、二腿：收腿、翻脚、蹬夹水，再滑行。(蹬腿动作就如青蛙游水)             \n3、呼吸：手臂有力外划时、抬头吸气，双手内划腿收头在水里憋气，双手伸过头，同时蹬腿呼气。                               \n4、节奏：双臂伸展开来时，蹬腿动作已经完成。\n\n--------------\n## 自由泳 Free style\n自由泳：依靠手臂划水，腿部打水产生推动力。\n\n<center>![swim](/image/swim/freestyle_1.gif)</center>. \n\n<center>![swim](/image/swim/freestyle_2.gif)</center>. \n\n<center>![swim](/image/swim/freestyle_3.gif)</center>. \n\n- 关键点 \n1、身体要平直，腿面要刚刚好露出水面时打水\n2、身体入水后，手、肘、前伸伸展、抱水，手臂向内，向上划。出手后，手臂要放松并经空中前移，继续下一个动作，循环反复。\n3、双腿并拢，膝部稍微弯曲，向下打水。（鞭状打水）\n4、头朝左、朝右肩膀出水面转动时呼吸。\n5、一般的节奏：左右腿打水六次，两臂各划水一次、呼吸一次。\n\n--------------\n## 蝶泳 Butterfly\n蝶泳顾名思义就如蝴蝶状飞翔游水。   \n\n<center>![swim](/image/swim/butterfly_1.gif)</center>. \n\n<center>![swim](/image/swim/butterfly_2.gif)</center>. \n\n<center>![swim](/image/swim/butterfly_3.gif)</center>.     \n                                                    \n- 关键点 \n1、肩部与水面齐平，头部比手臂先入水，抬头的时候比较低，入水后手臂沿曲线向外和抱水，再下划抓水，两手分开到肩宽，弯肘划水。   \n2、打腿要点：开始时候双腿并拢，脚后跟一露出水面，双脚弯曲向下打水。整体节奏双手划一次，二腿同时打水二次。\n\n--------------\n## 仰泳 Backstroke\n仰泳又称背泳，是一种仰卧在水中的游泳姿势。\n\n<center>![swim](/image/swim/backstroke_1.gif)</center>. \n\n<center>![swim](/image/swim/backstroke_2.gif)</center>. \n\n<center>![swim](/image/swim/backstroke_3.gif)</center>.     \n          \n- 关键点 \n1、就如躺在水面上，两耳刚刚进入水中，臀部恰好就在水面下，面部向上，双脚刚好露出水面。\n2、手臂入水向后抱水向下划。\n3、双腿并拢上下轮流打水，膝部弯曲不露出水面。一般节奏是头部保持稳定不能左右摇摆，二手臂各划一次，二腿一共打水六次，呼吸一次。","slug":"zh-游泳动图","published":1,"updated":"2017-08-29T13:14:43.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z1001fog64loa0wdy0","content":"<p>（本文来自网上，方便矫正泳姿）游泳一般有以下四种最常见的姿势：自由泳、蛙泳、蝶泳、仰泳。下面来具体细说这四种动作以及用动态的图片分解姿势。</p>\n<hr>\n<h2 id=\"蛙泳-Breaststroke\"><a href=\"#蛙泳-Breaststroke\" class=\"headerlink\" title=\"蛙泳 Breaststroke\"></a>蛙泳 Breaststroke</h2><p>蛙泳是最平常的一种泳姿，就如青蛙游水。<br>动作要领：双手先划、胳膊伸后蹬腿，手划的时候腿不要动，手先收腿再收，手腿伸直并拢再滑行一会儿（大约1~2秒左右）。<br>配合动作：双手前伸，手掌掌心向外，倾斜45°左右，小拇指向上，双手向外划时候冒头换气（吸气），双手内划时腿收憋气，双手伸过头的同时蹬腿吐气。  </p>\n<p><center><img src=\"/image/swim/breaststroke_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/breaststroke_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/breaststroke_3.gif\" alt=\"swim\"></center>. </p>\n<ul>\n<li>关键点<br>1、手臂：先向外划相对于抱水姿势，再向下、向内、伸展保持原来姿势。<br>2、二腿：收腿、翻脚、蹬夹水，再滑行。(蹬腿动作就如青蛙游水)<br>3、呼吸：手臂有力外划时、抬头吸气，双手内划腿收头在水里憋气，双手伸过头，同时蹬腿呼气。<br>4、节奏：双臂伸展开来时，蹬腿动作已经完成。</li>\n</ul>\n<hr>\n<h2 id=\"自由泳-Free-style\"><a href=\"#自由泳-Free-style\" class=\"headerlink\" title=\"自由泳 Free style\"></a>自由泳 Free style</h2><p>自由泳：依靠手臂划水，腿部打水产生推动力。</p>\n<p><center><img src=\"/image/swim/freestyle_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/freestyle_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/freestyle_3.gif\" alt=\"swim\"></center>. </p>\n<ul>\n<li>关键点<br>1、身体要平直，腿面要刚刚好露出水面时打水<br>2、身体入水后，手、肘、前伸伸展、抱水，手臂向内，向上划。出手后，手臂要放松并经空中前移，继续下一个动作，循环反复。<br>3、双腿并拢，膝部稍微弯曲，向下打水。（鞭状打水）<br>4、头朝左、朝右肩膀出水面转动时呼吸。<br>5、一般的节奏：左右腿打水六次，两臂各划水一次、呼吸一次。</li>\n</ul>\n<hr>\n<h2 id=\"蝶泳-Butterfly\"><a href=\"#蝶泳-Butterfly\" class=\"headerlink\" title=\"蝶泳 Butterfly\"></a>蝶泳 Butterfly</h2><p>蝶泳顾名思义就如蝴蝶状飞翔游水。   </p>\n<p><center><img src=\"/image/swim/butterfly_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/butterfly_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/butterfly_3.gif\" alt=\"swim\"></center>.     </p>\n<ul>\n<li>关键点<br>1、肩部与水面齐平，头部比手臂先入水，抬头的时候比较低，入水后手臂沿曲线向外和抱水，再下划抓水，两手分开到肩宽，弯肘划水。<br>2、打腿要点：开始时候双腿并拢，脚后跟一露出水面，双脚弯曲向下打水。整体节奏双手划一次，二腿同时打水二次。</li>\n</ul>\n<hr>\n<h2 id=\"仰泳-Backstroke\"><a href=\"#仰泳-Backstroke\" class=\"headerlink\" title=\"仰泳 Backstroke\"></a>仰泳 Backstroke</h2><p>仰泳又称背泳，是一种仰卧在水中的游泳姿势。</p>\n<p><center><img src=\"/image/swim/backstroke_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/backstroke_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/backstroke_3.gif\" alt=\"swim\"></center>.     </p>\n<ul>\n<li>关键点<br>1、就如躺在水面上，两耳刚刚进入水中，臀部恰好就在水面下，面部向上，双脚刚好露出水面。<br>2、手臂入水向后抱水向下划。<br>3、双腿并拢上下轮流打水，膝部弯曲不露出水面。一般节奏是头部保持稳定不能左右摇摆，二手臂各划一次，二腿一共打水六次，呼吸一次。</li>\n</ul>\n","excerpt":"","more":"<p>（本文来自网上，方便矫正泳姿）游泳一般有以下四种最常见的姿势：自由泳、蛙泳、蝶泳、仰泳。下面来具体细说这四种动作以及用动态的图片分解姿势。</p>\n<hr>\n<h2 id=\"蛙泳-Breaststroke\"><a href=\"#蛙泳-Breaststroke\" class=\"headerlink\" title=\"蛙泳 Breaststroke\"></a>蛙泳 Breaststroke</h2><p>蛙泳是最平常的一种泳姿，就如青蛙游水。<br>动作要领：双手先划、胳膊伸后蹬腿，手划的时候腿不要动，手先收腿再收，手腿伸直并拢再滑行一会儿（大约1~2秒左右）。<br>配合动作：双手前伸，手掌掌心向外，倾斜45°左右，小拇指向上，双手向外划时候冒头换气（吸气），双手内划时腿收憋气，双手伸过头的同时蹬腿吐气。  </p>\n<p><center><img src=\"/image/swim/breaststroke_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/breaststroke_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/breaststroke_3.gif\" alt=\"swim\"></center>. </p>\n<ul>\n<li>关键点<br>1、手臂：先向外划相对于抱水姿势，再向下、向内、伸展保持原来姿势。<br>2、二腿：收腿、翻脚、蹬夹水，再滑行。(蹬腿动作就如青蛙游水)<br>3、呼吸：手臂有力外划时、抬头吸气，双手内划腿收头在水里憋气，双手伸过头，同时蹬腿呼气。<br>4、节奏：双臂伸展开来时，蹬腿动作已经完成。</li>\n</ul>\n<hr>\n<h2 id=\"自由泳-Free-style\"><a href=\"#自由泳-Free-style\" class=\"headerlink\" title=\"自由泳 Free style\"></a>自由泳 Free style</h2><p>自由泳：依靠手臂划水，腿部打水产生推动力。</p>\n<p><center><img src=\"/image/swim/freestyle_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/freestyle_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/freestyle_3.gif\" alt=\"swim\"></center>. </p>\n<ul>\n<li>关键点<br>1、身体要平直，腿面要刚刚好露出水面时打水<br>2、身体入水后，手、肘、前伸伸展、抱水，手臂向内，向上划。出手后，手臂要放松并经空中前移，继续下一个动作，循环反复。<br>3、双腿并拢，膝部稍微弯曲，向下打水。（鞭状打水）<br>4、头朝左、朝右肩膀出水面转动时呼吸。<br>5、一般的节奏：左右腿打水六次，两臂各划水一次、呼吸一次。</li>\n</ul>\n<hr>\n<h2 id=\"蝶泳-Butterfly\"><a href=\"#蝶泳-Butterfly\" class=\"headerlink\" title=\"蝶泳 Butterfly\"></a>蝶泳 Butterfly</h2><p>蝶泳顾名思义就如蝴蝶状飞翔游水。   </p>\n<p><center><img src=\"/image/swim/butterfly_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/butterfly_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/butterfly_3.gif\" alt=\"swim\"></center>.     </p>\n<ul>\n<li>关键点<br>1、肩部与水面齐平，头部比手臂先入水，抬头的时候比较低，入水后手臂沿曲线向外和抱水，再下划抓水，两手分开到肩宽，弯肘划水。<br>2、打腿要点：开始时候双腿并拢，脚后跟一露出水面，双脚弯曲向下打水。整体节奏双手划一次，二腿同时打水二次。</li>\n</ul>\n<hr>\n<h2 id=\"仰泳-Backstroke\"><a href=\"#仰泳-Backstroke\" class=\"headerlink\" title=\"仰泳 Backstroke\"></a>仰泳 Backstroke</h2><p>仰泳又称背泳，是一种仰卧在水中的游泳姿势。</p>\n<p><center><img src=\"/image/swim/backstroke_1.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/backstroke_2.gif\" alt=\"swim\"></center>. </p>\n<p><center><img src=\"/image/swim/backstroke_3.gif\" alt=\"swim\"></center>.     </p>\n<ul>\n<li>关键点<br>1、就如躺在水面上，两耳刚刚进入水中，臀部恰好就在水面下，面部向上，双脚刚好露出水面。<br>2、手臂入水向后抱水向下划。<br>3、双腿并拢上下轮流打水，膝部弯曲不露出水面。一般节奏是头部保持稳定不能左右摇摆，二手臂各划一次，二腿一共打水六次，呼吸一次。</li>\n</ul>\n"},{"title":"石寨古村","lang":"zh","date":"2016-11-29T16:13:50.000Z","_content":"\n最近特别想念家乡，我的出生之地在里面一个叫石寨的古村，在临近的一个东海的镇上长大，逢年过节的时候，家里人都会带我们兄妹过石寨古村，拜访亲戚。吾辈在东瀛笔名叫“东海石寨”。\n\n前天父亲在微信发了一条消息，说石寨古村刚刚被获评为“中国乡村旅游模范村”，还附上了和家乡的几位伯伯叔叔的授牌仪式照片。看得出父亲很开心，他补充说这是继大安石寨村被国家评为“首批中国传统村落”、“中国古村落”、入选第六批“中国历史文化名镇村”之后，目前荣获的第四个国家级荣誉。作为家乡的一员，也感到十分开心。在东京，我跟日本人做自我介绍时，会说明来自石寨古村，并且会跟他们介绍一下这座古村，他们也对这座古寨充满兴趣。\n\n石寨古村于唐朝武德五年（公元622年）设陆安县时开始建村，到明朝为简陋土寨，至清初形成今日人们看到的有古城墙、宗祠、街巷和排水系统等格局。石寨村有石寨村、新寨村二个自然村，历史传统建筑包括石城、和安里、黄忠贞公祠三部分，总面积约十万平方米。石寨有“学堂、祠堂、庵堂”配套体系。这些古建筑记录着石寨村发展的沧桑岁月，同时它也是石寨村文化变迁的历史见证。希望家乡普及传承保护意识，将石寨古村落保护好、建设好。因为近年来，寨内一些城墙损毁，一部分老屋坍塌，部分文物受损严重甚至一些御赐牌匾、宫廷古画、清代屏风都急需修复。\n\n石寨村是黄姓集居地,座东向西，依山而筑。石寨城的地下排水系统设计，亦可谓一绝。记得我小时候下雨的时候到跑到石寨的巷道去玩，石寨城依靠山势而建，因山势呈螺旋状，石板下隐约可见又深又宽的排污暗渠，没见过有堵塞的情况，路面一直都保持洁净。\n\n石寨是一部用石头写就的历史，同时也一个磊筑传说的古村落。有机会去翻阅下这本古书吧。  \n\n<center>![shizhai](/image/shizhai.jpg)</center>\n","source":"_posts/zh/石寨古村.md","raw":"---\ntitle: 石寨古村\nlang: zh\ndate: 2016-11-30 01:13:50\n---\n\n最近特别想念家乡，我的出生之地在里面一个叫石寨的古村，在临近的一个东海的镇上长大，逢年过节的时候，家里人都会带我们兄妹过石寨古村，拜访亲戚。吾辈在东瀛笔名叫“东海石寨”。\n\n前天父亲在微信发了一条消息，说石寨古村刚刚被获评为“中国乡村旅游模范村”，还附上了和家乡的几位伯伯叔叔的授牌仪式照片。看得出父亲很开心，他补充说这是继大安石寨村被国家评为“首批中国传统村落”、“中国古村落”、入选第六批“中国历史文化名镇村”之后，目前荣获的第四个国家级荣誉。作为家乡的一员，也感到十分开心。在东京，我跟日本人做自我介绍时，会说明来自石寨古村，并且会跟他们介绍一下这座古村，他们也对这座古寨充满兴趣。\n\n石寨古村于唐朝武德五年（公元622年）设陆安县时开始建村，到明朝为简陋土寨，至清初形成今日人们看到的有古城墙、宗祠、街巷和排水系统等格局。石寨村有石寨村、新寨村二个自然村，历史传统建筑包括石城、和安里、黄忠贞公祠三部分，总面积约十万平方米。石寨有“学堂、祠堂、庵堂”配套体系。这些古建筑记录着石寨村发展的沧桑岁月，同时它也是石寨村文化变迁的历史见证。希望家乡普及传承保护意识，将石寨古村落保护好、建设好。因为近年来，寨内一些城墙损毁，一部分老屋坍塌，部分文物受损严重甚至一些御赐牌匾、宫廷古画、清代屏风都急需修复。\n\n石寨村是黄姓集居地,座东向西，依山而筑。石寨城的地下排水系统设计，亦可谓一绝。记得我小时候下雨的时候到跑到石寨的巷道去玩，石寨城依靠山势而建，因山势呈螺旋状，石板下隐约可见又深又宽的排污暗渠，没见过有堵塞的情况，路面一直都保持洁净。\n\n石寨是一部用石头写就的历史，同时也一个磊筑传说的古村落。有机会去翻阅下这本古书吧。  \n\n<center>![shizhai](/image/shizhai.jpg)</center>\n","slug":"zh-石寨古村","published":1,"updated":"2018-11-05T14:35:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z2001hog64pe2nwh78","content":"<p>最近特别想念家乡，我的出生之地在里面一个叫石寨的古村，在临近的一个东海的镇上长大，逢年过节的时候，家里人都会带我们兄妹过石寨古村，拜访亲戚。吾辈在东瀛笔名叫“东海石寨”。</p>\n<p>前天父亲在微信发了一条消息，说石寨古村刚刚被获评为“中国乡村旅游模范村”，还附上了和家乡的几位伯伯叔叔的授牌仪式照片。看得出父亲很开心，他补充说这是继大安石寨村被国家评为“首批中国传统村落”、“中国古村落”、入选第六批“中国历史文化名镇村”之后，目前荣获的第四个国家级荣誉。作为家乡的一员，也感到十分开心。在东京，我跟日本人做自我介绍时，会说明来自石寨古村，并且会跟他们介绍一下这座古村，他们也对这座古寨充满兴趣。</p>\n<p>石寨古村于唐朝武德五年（公元622年）设陆安县时开始建村，到明朝为简陋土寨，至清初形成今日人们看到的有古城墙、宗祠、街巷和排水系统等格局。石寨村有石寨村、新寨村二个自然村，历史传统建筑包括石城、和安里、黄忠贞公祠三部分，总面积约十万平方米。石寨有“学堂、祠堂、庵堂”配套体系。这些古建筑记录着石寨村发展的沧桑岁月，同时它也是石寨村文化变迁的历史见证。希望家乡普及传承保护意识，将石寨古村落保护好、建设好。因为近年来，寨内一些城墙损毁，一部分老屋坍塌，部分文物受损严重甚至一些御赐牌匾、宫廷古画、清代屏风都急需修复。</p>\n<p>石寨村是黄姓集居地,座东向西，依山而筑。石寨城的地下排水系统设计，亦可谓一绝。记得我小时候下雨的时候到跑到石寨的巷道去玩，石寨城依靠山势而建，因山势呈螺旋状，石板下隐约可见又深又宽的排污暗渠，没见过有堵塞的情况，路面一直都保持洁净。</p>\n<p>石寨是一部用石头写就的历史，同时也一个磊筑传说的古村落。有机会去翻阅下这本古书吧。  </p>\n<center><img src=\"/image/shizhai.jpg\" alt=\"shizhai\"></center>\n","excerpt":"","more":"<p>最近特别想念家乡，我的出生之地在里面一个叫石寨的古村，在临近的一个东海的镇上长大，逢年过节的时候，家里人都会带我们兄妹过石寨古村，拜访亲戚。吾辈在东瀛笔名叫“东海石寨”。</p>\n<p>前天父亲在微信发了一条消息，说石寨古村刚刚被获评为“中国乡村旅游模范村”，还附上了和家乡的几位伯伯叔叔的授牌仪式照片。看得出父亲很开心，他补充说这是继大安石寨村被国家评为“首批中国传统村落”、“中国古村落”、入选第六批“中国历史文化名镇村”之后，目前荣获的第四个国家级荣誉。作为家乡的一员，也感到十分开心。在东京，我跟日本人做自我介绍时，会说明来自石寨古村，并且会跟他们介绍一下这座古村，他们也对这座古寨充满兴趣。</p>\n<p>石寨古村于唐朝武德五年（公元622年）设陆安县时开始建村，到明朝为简陋土寨，至清初形成今日人们看到的有古城墙、宗祠、街巷和排水系统等格局。石寨村有石寨村、新寨村二个自然村，历史传统建筑包括石城、和安里、黄忠贞公祠三部分，总面积约十万平方米。石寨有“学堂、祠堂、庵堂”配套体系。这些古建筑记录着石寨村发展的沧桑岁月，同时它也是石寨村文化变迁的历史见证。希望家乡普及传承保护意识，将石寨古村落保护好、建设好。因为近年来，寨内一些城墙损毁，一部分老屋坍塌，部分文物受损严重甚至一些御赐牌匾、宫廷古画、清代屏风都急需修复。</p>\n<p>石寨村是黄姓集居地,座东向西，依山而筑。石寨城的地下排水系统设计，亦可谓一绝。记得我小时候下雨的时候到跑到石寨的巷道去玩，石寨城依靠山势而建，因山势呈螺旋状，石板下隐约可见又深又宽的排污暗渠，没见过有堵塞的情况，路面一直都保持洁净。</p>\n<p>石寨是一部用石头写就的历史，同时也一个磊筑传说的古村落。有机会去翻阅下这本古书吧。  </p>\n<center><img src=\"/image/shizhai.jpg\" alt=\"shizhai\"></center>\n"},{"title":"اللغة الإنجليزية في المطبخ الصيني","lang":"fr","date":"2016-12-09T16:13:57.000Z","_content":" صدور الترجمة الإنغليزية الموحدة لقائمة الأطباق الصينية\n\nقام مكتب الشؤون الخارجية لحكومة مدينة بكين و مكتب اللغات الأجنبية لسكان مدينة بكين مؤخرا بإصدار \" حديقة ترجمة فنون الطبخ—-الترجمة الإنغليزية لقائمة الأطباق الصينية\"، حيث قامت هذه الطبعة بتوحيد الترجمة الإنغليزية لـ2158 طبق. و أشار مسؤول بمكتب الشؤون الخارجية لمدينة بكين، أن هذا الكتاب هو مرجع لطباعة قوائم الأطباق، ولايمكن فرضه قسرا.\n\nو تظهر الوثائق الإحصائية، وجود 70 ألف مطعم في مدينة بكين، و تسجل قائمات الأطباق ترجمات إنغليزية مختلفة للأطباق الصينية، لذا قام كتاب \"حديقة ترجمة فنون الطبخ\" بإعادة صياغة الأسماء الإنغليزية للأطباق الرئيسية و الأطباق المنزلية و مختلف أنواع أطباق الأطعمة الكبرى المنتمية لـ8 أقسام كبرى من الأطباق الصينية.\n\nو اعتمدت اللجنة المترجمة و فريق الخبراء على سبعة قواعد في الترجمة، منها: الإعتماد على المادة الأساسية كعامل رئيسي و البهارات كعامل مساعد، مثل طبق \"الفطر مع سيقان البط\"، و الإعتماد على طريقة الطبخ كعامل رئيسي و المادة الأولية كعامل مساعد، مثل \"كلية الخنزير المقلية\"، و وفقا للشكل و المذاق كعاملين رئيسييْن و المواد الأولية كعامل مساعد، مثل \"الدجاج المقرمش\"، أو التسمية وفقا لأسماء العلم و المكان مثل \"توفو مابوه\"\n\nو بالنسبة لأسماء الأطباق الإستثنائية في الثقافة الغذائية الصينية تم ترجمتها حرفيا بإبدال الحروف الصينية بالحروف اللاتينية، مثل الجياوزي الصيني الذي كان بترجم في السابق إلى dumpling تم تغييره إلى Jiaozi. كما تم اعتماد هذا المبدأ بالنسبة لبقية الأطباق التي لايمكن ترجمتها مباشرة أو لاتمكن ترجمتها من تجسيد طريقة اعدادها. و أشار المسؤولون إلى أملهم في أن يلعب هذا الأسلوب في الترجمة (الأسماء الصينية بالأحرف اللاتينية) دورا في المزج بين الثقافتين.\n\nإستقطب كتاب\"حديقة ترجمة فنون الطبخ\" تعليقات من مختلف الأوساط. و ترى الشخصيات العاملة في القطاع أن هذا الكتاب بإمكانه يوفر مرجعا للمطاعم، و ينقذ المطاعم من إرتكاب بعض \"الأخطاء المضحة\" في الترجمة، في المقابل يرى بعض المستهلكين أن ثقافة الطبخ الصينية العريقة، تحتوي على العديد من المعاني العميقة التي لا يمكن ترجمتها إلى الإنغليزية.","source":"_posts/ar/اللغة الإنجليزية في المطبخ الصيني.md","raw":"---\ntitle: اللغة الإنجليزية في المطبخ الصيني\nlang: fr\ndate: 2016-12-10 01:13:57\n---\n صدور الترجمة الإنغليزية الموحدة لقائمة الأطباق الصينية\n\nقام مكتب الشؤون الخارجية لحكومة مدينة بكين و مكتب اللغات الأجنبية لسكان مدينة بكين مؤخرا بإصدار \" حديقة ترجمة فنون الطبخ—-الترجمة الإنغليزية لقائمة الأطباق الصينية\"، حيث قامت هذه الطبعة بتوحيد الترجمة الإنغليزية لـ2158 طبق. و أشار مسؤول بمكتب الشؤون الخارجية لمدينة بكين، أن هذا الكتاب هو مرجع لطباعة قوائم الأطباق، ولايمكن فرضه قسرا.\n\nو تظهر الوثائق الإحصائية، وجود 70 ألف مطعم في مدينة بكين، و تسجل قائمات الأطباق ترجمات إنغليزية مختلفة للأطباق الصينية، لذا قام كتاب \"حديقة ترجمة فنون الطبخ\" بإعادة صياغة الأسماء الإنغليزية للأطباق الرئيسية و الأطباق المنزلية و مختلف أنواع أطباق الأطعمة الكبرى المنتمية لـ8 أقسام كبرى من الأطباق الصينية.\n\nو اعتمدت اللجنة المترجمة و فريق الخبراء على سبعة قواعد في الترجمة، منها: الإعتماد على المادة الأساسية كعامل رئيسي و البهارات كعامل مساعد، مثل طبق \"الفطر مع سيقان البط\"، و الإعتماد على طريقة الطبخ كعامل رئيسي و المادة الأولية كعامل مساعد، مثل \"كلية الخنزير المقلية\"، و وفقا للشكل و المذاق كعاملين رئيسييْن و المواد الأولية كعامل مساعد، مثل \"الدجاج المقرمش\"، أو التسمية وفقا لأسماء العلم و المكان مثل \"توفو مابوه\"\n\nو بالنسبة لأسماء الأطباق الإستثنائية في الثقافة الغذائية الصينية تم ترجمتها حرفيا بإبدال الحروف الصينية بالحروف اللاتينية، مثل الجياوزي الصيني الذي كان بترجم في السابق إلى dumpling تم تغييره إلى Jiaozi. كما تم اعتماد هذا المبدأ بالنسبة لبقية الأطباق التي لايمكن ترجمتها مباشرة أو لاتمكن ترجمتها من تجسيد طريقة اعدادها. و أشار المسؤولون إلى أملهم في أن يلعب هذا الأسلوب في الترجمة (الأسماء الصينية بالأحرف اللاتينية) دورا في المزج بين الثقافتين.\n\nإستقطب كتاب\"حديقة ترجمة فنون الطبخ\" تعليقات من مختلف الأوساط. و ترى الشخصيات العاملة في القطاع أن هذا الكتاب بإمكانه يوفر مرجعا للمطاعم، و ينقذ المطاعم من إرتكاب بعض \"الأخطاء المضحة\" في الترجمة، في المقابل يرى بعض المستهلكين أن ثقافة الطبخ الصينية العريقة، تحتوي على العديد من المعاني العميقة التي لا يمكن ترجمتها إلى الإنغليزية.","slug":"ar-اللغة-الإنجليزية-في-المطبخ-الصيني","published":1,"updated":"2016-12-10T16:28:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z3001jog64kez26bum","content":"<p> صدور الترجمة الإنغليزية الموحدة لقائمة الأطباق الصينية</p>\n<p>قام مكتب الشؤون الخارجية لحكومة مدينة بكين و مكتب اللغات الأجنبية لسكان مدينة بكين مؤخرا بإصدار “ حديقة ترجمة فنون الطبخ—-الترجمة الإنغليزية لقائمة الأطباق الصينية”، حيث قامت هذه الطبعة بتوحيد الترجمة الإنغليزية لـ2158 طبق. و أشار مسؤول بمكتب الشؤون الخارجية لمدينة بكين، أن هذا الكتاب هو مرجع لطباعة قوائم الأطباق، ولايمكن فرضه قسرا.</p>\n<p>و تظهر الوثائق الإحصائية، وجود 70 ألف مطعم في مدينة بكين، و تسجل قائمات الأطباق ترجمات إنغليزية مختلفة للأطباق الصينية، لذا قام كتاب “حديقة ترجمة فنون الطبخ” بإعادة صياغة الأسماء الإنغليزية للأطباق الرئيسية و الأطباق المنزلية و مختلف أنواع أطباق الأطعمة الكبرى المنتمية لـ8 أقسام كبرى من الأطباق الصينية.</p>\n<p>و اعتمدت اللجنة المترجمة و فريق الخبراء على سبعة قواعد في الترجمة، منها: الإعتماد على المادة الأساسية كعامل رئيسي و البهارات كعامل مساعد، مثل طبق “الفطر مع سيقان البط”، و الإعتماد على طريقة الطبخ كعامل رئيسي و المادة الأولية كعامل مساعد، مثل “كلية الخنزير المقلية”، و وفقا للشكل و المذاق كعاملين رئيسييْن و المواد الأولية كعامل مساعد، مثل “الدجاج المقرمش”، أو التسمية وفقا لأسماء العلم و المكان مثل “توفو مابوه”</p>\n<p>و بالنسبة لأسماء الأطباق الإستثنائية في الثقافة الغذائية الصينية تم ترجمتها حرفيا بإبدال الحروف الصينية بالحروف اللاتينية، مثل الجياوزي الصيني الذي كان بترجم في السابق إلى dumpling تم تغييره إلى Jiaozi. كما تم اعتماد هذا المبدأ بالنسبة لبقية الأطباق التي لايمكن ترجمتها مباشرة أو لاتمكن ترجمتها من تجسيد طريقة اعدادها. و أشار المسؤولون إلى أملهم في أن يلعب هذا الأسلوب في الترجمة (الأسماء الصينية بالأحرف اللاتينية) دورا في المزج بين الثقافتين.</p>\n<p>إستقطب كتاب”حديقة ترجمة فنون الطبخ” تعليقات من مختلف الأوساط. و ترى الشخصيات العاملة في القطاع أن هذا الكتاب بإمكانه يوفر مرجعا للمطاعم، و ينقذ المطاعم من إرتكاب بعض “الأخطاء المضحة” في الترجمة، في المقابل يرى بعض المستهلكين أن ثقافة الطبخ الصينية العريقة، تحتوي على العديد من المعاني العميقة التي لا يمكن ترجمتها إلى الإنغليزية.</p>\n","excerpt":"","more":"<p> صدور الترجمة الإنغليزية الموحدة لقائمة الأطباق الصينية</p>\n<p>قام مكتب الشؤون الخارجية لحكومة مدينة بكين و مكتب اللغات الأجنبية لسكان مدينة بكين مؤخرا بإصدار “ حديقة ترجمة فنون الطبخ—-الترجمة الإنغليزية لقائمة الأطباق الصينية”، حيث قامت هذه الطبعة بتوحيد الترجمة الإنغليزية لـ2158 طبق. و أشار مسؤول بمكتب الشؤون الخارجية لمدينة بكين، أن هذا الكتاب هو مرجع لطباعة قوائم الأطباق، ولايمكن فرضه قسرا.</p>\n<p>و تظهر الوثائق الإحصائية، وجود 70 ألف مطعم في مدينة بكين، و تسجل قائمات الأطباق ترجمات إنغليزية مختلفة للأطباق الصينية، لذا قام كتاب “حديقة ترجمة فنون الطبخ” بإعادة صياغة الأسماء الإنغليزية للأطباق الرئيسية و الأطباق المنزلية و مختلف أنواع أطباق الأطعمة الكبرى المنتمية لـ8 أقسام كبرى من الأطباق الصينية.</p>\n<p>و اعتمدت اللجنة المترجمة و فريق الخبراء على سبعة قواعد في الترجمة، منها: الإعتماد على المادة الأساسية كعامل رئيسي و البهارات كعامل مساعد، مثل طبق “الفطر مع سيقان البط”، و الإعتماد على طريقة الطبخ كعامل رئيسي و المادة الأولية كعامل مساعد، مثل “كلية الخنزير المقلية”، و وفقا للشكل و المذاق كعاملين رئيسييْن و المواد الأولية كعامل مساعد، مثل “الدجاج المقرمش”، أو التسمية وفقا لأسماء العلم و المكان مثل “توفو مابوه”</p>\n<p>و بالنسبة لأسماء الأطباق الإستثنائية في الثقافة الغذائية الصينية تم ترجمتها حرفيا بإبدال الحروف الصينية بالحروف اللاتينية، مثل الجياوزي الصيني الذي كان بترجم في السابق إلى dumpling تم تغييره إلى Jiaozi. كما تم اعتماد هذا المبدأ بالنسبة لبقية الأطباق التي لايمكن ترجمتها مباشرة أو لاتمكن ترجمتها من تجسيد طريقة اعدادها. و أشار المسؤولون إلى أملهم في أن يلعب هذا الأسلوب في الترجمة (الأسماء الصينية بالأحرف اللاتينية) دورا في المزج بين الثقافتين.</p>\n<p>إستقطب كتاب”حديقة ترجمة فنون الطبخ” تعليقات من مختلف الأوساط. و ترى الشخصيات العاملة في القطاع أن هذا الكتاب بإمكانه يوفر مرجعا للمطاعم، و ينقذ المطاعم من إرتكاب بعض “الأخطاء المضحة” في الترجمة، في المقابل يرى بعض المستهلكين أن ثقافة الطبخ الصينية العريقة، تحتوي على العديد من المعاني العميقة التي لا يمكن ترجمتها إلى الإنغليزية.</p>\n"},{"title":"Un Plan — Le Voyage en France","lang":"fr","date":"2016-12-09T16:13:56.000Z","_content":"Pour le vovage de quinze jours en France, je vais visiter ces monuments avec mes aims ou ma famille.\n\nD'abord,j'aimerais aller à Paris. J'essaye de séjour à Paris pendant une semaine. Qu'est-ce que je voudrais voir? Bien sur, je vais voir L'Arc de Triomphe et Notre-Dame de Paris, parce que ils sont des monuments historique. Puis, je souhaiterais voir la Tour Eiffel, parce que du haut la Tour Eiffel, On peut voir tout Paris.\n\nEnsuite,je voudrais aller à Lyon pour 3 jours. \"Si Paris est la capitale de la France, Lyon est la capitale de la province.\" — Albert Thibaudet. Je vais vister les sites touristiques: L'ancienne laudanum et la Crox Rousse.\n\nPuis, je vais aller à Toulouse et voyager à Cordes sur Ciel pour 2 jours. Cordes sur Ciel fascine tous ceux qui la découvrent parce qu’elle est d’abord une vision : celle d’une cité montant à l’assaut du ciel.\n\nEnfin,je vais aller à Nice qui est une ville de la Côté d'Azur pour 2 jours. Elle est etablie sur les bords de la mer Méditerranée. Je vais nager dans la plage.\n\nJ'espère un bon voyage. C'est tout. Merci beaucoup. ","source":"_posts/fr/Un Plan — Le Voyage en France.md","raw":"---\ntitle: Un Plan — Le Voyage en France\nlang: fr\ndate: 2016-12-10 01:13:56\n---\nPour le vovage de quinze jours en France, je vais visiter ces monuments avec mes aims ou ma famille.\n\nD'abord,j'aimerais aller à Paris. J'essaye de séjour à Paris pendant une semaine. Qu'est-ce que je voudrais voir? Bien sur, je vais voir L'Arc de Triomphe et Notre-Dame de Paris, parce que ils sont des monuments historique. Puis, je souhaiterais voir la Tour Eiffel, parce que du haut la Tour Eiffel, On peut voir tout Paris.\n\nEnsuite,je voudrais aller à Lyon pour 3 jours. \"Si Paris est la capitale de la France, Lyon est la capitale de la province.\" — Albert Thibaudet. Je vais vister les sites touristiques: L'ancienne laudanum et la Crox Rousse.\n\nPuis, je vais aller à Toulouse et voyager à Cordes sur Ciel pour 2 jours. Cordes sur Ciel fascine tous ceux qui la découvrent parce qu’elle est d’abord une vision : celle d’une cité montant à l’assaut du ciel.\n\nEnfin,je vais aller à Nice qui est une ville de la Côté d'Azur pour 2 jours. Elle est etablie sur les bords de la mer Méditerranée. Je vais nager dans la plage.\n\nJ'espère un bon voyage. C'est tout. Merci beaucoup. ","slug":"fr-Un-Plan-—-Le-Voyage-en-France","published":1,"updated":"2016-12-10T16:24:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8z4001log64isum322u","content":"<p>Pour le vovage de quinze jours en France, je vais visiter ces monuments avec mes aims ou ma famille.</p>\n<p>D’abord,j’aimerais aller à Paris. J’essaye de séjour à Paris pendant une semaine. Qu’est-ce que je voudrais voir? Bien sur, je vais voir L’Arc de Triomphe et Notre-Dame de Paris, parce que ils sont des monuments historique. Puis, je souhaiterais voir la Tour Eiffel, parce que du haut la Tour Eiffel, On peut voir tout Paris.</p>\n<p>Ensuite,je voudrais aller à Lyon pour 3 jours. “Si Paris est la capitale de la France, Lyon est la capitale de la province.” — Albert Thibaudet. Je vais vister les sites touristiques: L’ancienne laudanum et la Crox Rousse.</p>\n<p>Puis, je vais aller à Toulouse et voyager à Cordes sur Ciel pour 2 jours. Cordes sur Ciel fascine tous ceux qui la découvrent parce qu’elle est d’abord une vision : celle d’une cité montant à l’assaut du ciel.</p>\n<p>Enfin,je vais aller à Nice qui est une ville de la Côté d’Azur pour 2 jours. Elle est etablie sur les bords de la mer Méditerranée. Je vais nager dans la plage.</p>\n<p>J’espère un bon voyage. C’est tout. Merci beaucoup. </p>\n","excerpt":"","more":"<p>Pour le vovage de quinze jours en France, je vais visiter ces monuments avec mes aims ou ma famille.</p>\n<p>D’abord,j’aimerais aller à Paris. J’essaye de séjour à Paris pendant une semaine. Qu’est-ce que je voudrais voir? Bien sur, je vais voir L’Arc de Triomphe et Notre-Dame de Paris, parce que ils sont des monuments historique. Puis, je souhaiterais voir la Tour Eiffel, parce que du haut la Tour Eiffel, On peut voir tout Paris.</p>\n<p>Ensuite,je voudrais aller à Lyon pour 3 jours. “Si Paris est la capitale de la France, Lyon est la capitale de la province.” — Albert Thibaudet. Je vais vister les sites touristiques: L’ancienne laudanum et la Crox Rousse.</p>\n<p>Puis, je vais aller à Toulouse et voyager à Cordes sur Ciel pour 2 jours. Cordes sur Ciel fascine tous ceux qui la découvrent parce qu’elle est d’abord une vision : celle d’une cité montant à l’assaut du ciel.</p>\n<p>Enfin,je vais aller à Nice qui est une ville de la Côté d’Azur pour 2 jours. Elle est etablie sur les bords de la mer Méditerranée. Je vais nager dans la plage.</p>\n<p>J’espère un bon voyage. C’est tout. Merci beaucoup. </p>\n"},{"title":"柔道用語-Judo terminology","lang":"ja","date":"2016-11-30T02:14:30.000Z","_content":"<center>![W](/image/Judo/Judo.jpg)</center>\n# 一 投技 (67本)\n投技是以手、腰、足等为主要发力点，通过柔道动作，使对手倒地的技术。\n具体分为五类：手技、腰技、足技、真舍身技、横舍身技。   \n\n##  1.手技 (15本)\n| 序号 | 名称       | 日语               | 英语              |\n|------|------------|--------------------|-------------------|\n|      | 手技       | てわざ             | TE-WAZA           |\n| 1    | 背负投     | せおいなげ         | Seoi-nage         |\n| 2    | 体落       | たいおとし         | Tai-otoshi        |\n| 3    | 肩车       | かたぐるま         | Kata-guruma       |\n| 4    | 掬投       | すくいなげ         | Sukui-nage        |\n| 5    | 浮落       | うきおとし         | Uki-otoshi        |\n| 6    | 隅落       | すみおとし         | Sumi-otoshi       |\n| 7    | 帯落       | おびおとし         | Obi-otoshi        |\n| 8    | 背负落     | せおいおとし       | Seoi-otoshi       |\n| 9    | 山岚       | やまあらし         | Yama-arashi       |\n| 10   | 双手刈     | もろてがり         | Morote-gari       |\n| 11   | 朽木倒     | くちきたおし       | Kuchiki-taoshi    |\n| 12   | 踵返       | きびすがえし       | Kibisu-gaeshi     |\n| 13   | 内股       | うちまたすかし     | Uchi-mata-sukashi |\n| 14   | 小内返     | こうちがえし       | Kouchi-gaeshi     |\n| 15   | 一本背负投 | いっぽんせおいなげ | Ippon-seoi-nage    |  \n\n## 2.腰技(11本)\n| 序号 | 名称       | 日语              | 英语                 |\n|------|------------|-------------------|----------------------|\n|      | 腰技       | こしわざ          | KOSHI-WAZA           |\n| 1    | 浮腰       | うきごし          | Uki-goshi            |\n| 2    | 大腰       | おおごし          | O-goshi              |\n| 3    | 腰车       | こしぐるま        | Koshi-guruma         |\n| 4    | 钓込腰     | つりこみごし      | Tsurikomi-goshi      |\n| 5    | 払腰       | はらいごし        | Harai-goshi          |\n| 6    | 钓腰       | つりごし          | Tsuri-goshi          |\n| 7    | 跳腰       | はねごし          | Hane-goshi           |\n| 8    | 移腰       | うつりごし        | Utsuri-goshi         |\n| 9    | 后腰       | うしろごし        | Ushiro-goshi         |\n| 10   | 抱上       | だきあげ          | Daki-age             |\n|      | *比赛中没  | *試合では有効な技 |                      |\n|      | 有得分效果 | とみなされない    |                      |\n| 11   | 袖钓込腰   | そでつりこみごし  | Sode-tsurikomi-goshi |  \n\n## 3.足技 (21本)\n| 序号 | 名称     | 日语               | 英语                 |\n|------|----------|--------------------|----------------------|\n|      | 足技     | あしわざ           | ASHI-WAZA            |\n| 1    | 出足払   | であしはらい       | Deashi-harai         |\n| 2    | 膝车     | ひざぐるま         | Hiza-guruma          |\n| 3    | 支钓込足 | ささえつりこみあし | Sasae-tsurikomi-ashi |\n| 4    | 大外刈   | おおそとがり       | Osoto-gari           |\n| 5    | 大内刈   | おおうちがり       | Ouchi-gari           |\n| 6    | 小外刈   | こそとがり         | Kosoto-gari          |\n| 7    | 小内刈   | こうちがり         | Kouchi-gari          |\n| 8    | 送足払   | おくりあしはらい   | Okuri-ashi-harai     |\n| 9    | 内股     | うちまた           | Uchi-mata            |\n| 10   | 小外挂   | こそとがけ         | Kosoto-gake          |\n| 11   | 足车     | あしぐるま         | Ashi-guruma          |\n| 12   | 払钓込足 | はらいつりこみあし | Harai-tsurikomi-ashi |\n| 13   | 大车     | おおぐるま         | O-guruma             |\n| 14   | 大外车   | おおそとぐるま     | Osoto-guruma         |\n| 15   | 大外落   | おおそとおとし     | Osoto-otoshi         |\n| 16   | 燕返     | つばめがえし       | Tsubame-gaeshi       |\n| 17   | 大外返   | おおそとがえし     | Osoto-gaeshi         |\n| 18   | 大内返   | おおうちがえし     | Ouchi-gaeshi         |\n| 19   | 跳腰返   | はねごしがえし     | Hane-goshi-gaeshi    |\n| 20   | 払腰返   | はらいごしがえし   | Harai-goshi-gaeshi   |\n| 21   | 内股返   | うちまたがえし     | Uchi-mata-gaeshi     |  \n\n## 4.真舍身技 (5本)\n| 序号 | 名称     | 日语           | 英语            |\n|------|----------|----------------|-----------------|\n|      | 真舍身技 | ますてみわざ   | MASUTEMI-WAZA   |\n| 1    | 巴投     | ともえなげ     | Tomoe-nage      |\n| 2    | 隅返     | すみがえし     | Sumi-gaeshi     |\n| 3    | 里投     | うらなげ       | Ura-nage        |\n| 4    | 引込返   | ひきこみがえし | Hikikomi-gaeshi |\n| 5    | 俵返     | たわらがえし   | Tawara-gaeshi   |  \n\n## 5.横舍身技 (15本)\n| 序号 | 名称     | 日语             | 英语               |\n|------|----------|------------------|--------------------|\n|      | 横舍身技 | よこすてみわざ   | YOKOSUTEMI-WAZA    |\n| 1    | 横落     | よこおとし       | Yoko-otoshi        |\n| 2    | 谷落     | たにおとし       | Tani-otoshi        |\n| 3    | 跳巻込   | はねまきこみ     | Hane-makikomi      |\n| 4    | 外巻込   | そとまきこみ     | Soto-makikomi      |\n| 5    | 浮技     | うきわざ         | Uki-waza           |\n| 6    | 横分     | よこわかれ       | Yoko-wakare        |\n| 7    | 横车     | よこぐるま       | Yoko-guruma        |\n| 8    | 横挂     | よこがけ         | Yoko-gake          |\n| 9    | 抱分     | だきわかれ       | Daki-wakare        |\n| 10   | 内巻込   | うちまきこみ     | Uchi-makikomi      |\n| 11   | 蟹挟     | かにばさみ       | Kani-basami        |\n| 12   | 大外巻込 | おおそとまきこみ | Osoto-makikomi     |\n| 13   | 内股巻込 | うちまたまきこみ | Uchi-mata-makikomi |\n| 14   | 払巻込   | はらいまきこみ   | Harai-makikomi     |\n| 15   | 河津挂   | かわづがけ       | Kawazu-gake        |\n|      | *禁止技  | *禁止技          | .                  |  \n\n\n# 二 固技 (29本)\n投技是以手、臂、腿、躯干等为主要发力点，通过柔道动作，使对手因窒息或肌肉、关节等的疼痛而在地面上被降服的技术。\n具体分为三类：抑込技、绞技、关节技。  \n\n## 1.抑込技 (7本)\n| 序号 | 名称       | 日语                   | 英语                     |\n|------|------------|------------------------|--------------------------|\n|      | 抑込技     | おさえこみわざ         | OSAEKOMI-WAZA            |\n| 1    | 崩袈裟固   | くずれけさがため        | Kuzure-kesa-gatame       |\n| 2    | 肩固       | かたがため             | Kata-gatame              |\n| 3    | 上四方固   | かみしほうがため       | Kami-shiho-gatame        |\n| 4    | 崩上四方固 | くずれかみしほうがため | Kuzure-kami-shiho-gatame |\n| 5    | 横四方固   | よこしほうがため       | Yoko-shiho-gatame        |\n| 6    | 纵四方固   | たてしほうがため       | Tate-shiho-gatame        |\n| 7    | 袈裟固     | けさがため             | Kesa-gatame              |  \n\n## 2.绞技  (12本)\n| 序号 | 名称     | 日语               | 英语             |\n|------|----------|--------------------|------------------|\n|      | 绞技     | しめわざ           | SHIME-WAZA       |\n| 1    | 并十字绞 | なみじゅうじじめ   | Nami-juji-jime   |\n| 2    | 逆十字绞 | ぎゃくじゅうじじめ | Gyaku-juji-jime  |\n| 3    | 片十字绞 | かたじゅうじじめ   | Kata-juji-jime   |\n| 4    | 裸绞     | はだかじめ         | Hadaka-jime      |\n| 5    | 送襟绞   | おくりえりじめ     | Okuri-eri-jime   |\n| 6    | 片羽绞   | かたはじめ         | Kata-ha-jime     |\n| 7    | 胴绞     | どうじめ           | Do-jime          |\n|      | *禁止技  | *禁止技            |                  |\n| 8    | 袖车绞   | そでぐるまじめ     | Sode-guruma-jime |\n| 9    | 片手绞   | かたてじめ         | Kata-te-jime     |\n| 10   | 両手绞   | りょうてじめ       | Ryo-te-jime      |\n| 11   | 突込绞   | つっこみじめ       | Tsukkomi-jime    |\n| 12   | 三角绞   | さんかくじめ       | Sankaku-jime     |  \n\n## 3.关节技 (10本)\n| 序号 | 名称       | 日语                     | 英语                       |\n|------|------------|--------------------------|----------------------------|\n|      | 关节技     | かんせつわざ             | KANSETSU-WAZA              |\n| 1    | 腕缄       | うでがらみ               | Ude-garami                 |\n| 2    | 腕挫十字固 | うでひしぎじゅうじがため | Ude-hishigi-juji-gatame    |\n| 3    | 腕挫腕固   | うでひしぎうでがため     | Ude-hishigi-ude-gatame     |\n| 4    | 腕挫膝固   | うでひしぎひざがため     | Ude-hishigi-hiza-gatame    |\n| 5    | 腕挫腋固   | うでひしぎわきがため     | Ude-hishigi-waki-gatame    |\n| 6    | 腕挫腹固   | うてひしぎはらがため     | Ude-hishigi-hara-gatame    |\n| 7    | 足缄       | あしがらみ               | Ashi-garami                |\n|      | *禁止技    | *禁止技                  |                            |\n| 8    | 腕挫脚固   | うでひしぎあしがため     | Ude-hishigi-ashi-gatame    |\n| 9    | 腕挫手固   | うでひしぎてがため       | Ude-hishigi-te-gatame      |\n| 10   | 腕挫三角固 | うてひしぎさんかくがため | Ude-hishigi-sankaku-gatame  |  \n\n\n# 三 当身技 (あてみわざ)\n当身技是通过戳、打、踢等，直接攻击对手要害部位或生理弱点的技术。由于该技术危险性太大，容易对选手造成严重伤害，所以在柔道比赛和训练中被禁止使用。\n具体分为两类：当所和急所。  \n\n## 1.当所\n臂（うで）：\n指先当（ゆびさきあて）：突出、両眼突\n拳当（こぶしあて）：斜当、横当、上当、突上、下突、後突、後隅突、突掛、横打、後打、打下\n手刀当（てがたなあて、手掌の小指側縁）：切下、斜打\n肘当（ひじあて）：後当. \n \n脚（あし）：\n膝頭当（ひざがしらあて）：前当\n蹠頭当（せきとうあて、足蹠の前端）：斜蹴、前蹴、高蹴\n踵当（かかとあて）：後蹴、横蹴. \n\n## 2.急所\n天倒、霞、鳥兎、獨鈷、人中、三日月、松風、村雨、秘中、タン中、水月、雁下、明星、月影、電光、稲妻、臍下丹田、釣鐘（金的）、肘詰、伏兎、向骨。  \n","source":"_posts/ja/柔道用語-Judo terminology.md","raw":"---\ntitle: 柔道用語-Judo terminology\nlang: ja\ndate: 2016-11-30 11:14:30\ntags: Judo\n---\n<center>![W](/image/Judo/Judo.jpg)</center>\n# 一 投技 (67本)\n投技是以手、腰、足等为主要发力点，通过柔道动作，使对手倒地的技术。\n具体分为五类：手技、腰技、足技、真舍身技、横舍身技。   \n\n##  1.手技 (15本)\n| 序号 | 名称       | 日语               | 英语              |\n|------|------------|--------------------|-------------------|\n|      | 手技       | てわざ             | TE-WAZA           |\n| 1    | 背负投     | せおいなげ         | Seoi-nage         |\n| 2    | 体落       | たいおとし         | Tai-otoshi        |\n| 3    | 肩车       | かたぐるま         | Kata-guruma       |\n| 4    | 掬投       | すくいなげ         | Sukui-nage        |\n| 5    | 浮落       | うきおとし         | Uki-otoshi        |\n| 6    | 隅落       | すみおとし         | Sumi-otoshi       |\n| 7    | 帯落       | おびおとし         | Obi-otoshi        |\n| 8    | 背负落     | せおいおとし       | Seoi-otoshi       |\n| 9    | 山岚       | やまあらし         | Yama-arashi       |\n| 10   | 双手刈     | もろてがり         | Morote-gari       |\n| 11   | 朽木倒     | くちきたおし       | Kuchiki-taoshi    |\n| 12   | 踵返       | きびすがえし       | Kibisu-gaeshi     |\n| 13   | 内股       | うちまたすかし     | Uchi-mata-sukashi |\n| 14   | 小内返     | こうちがえし       | Kouchi-gaeshi     |\n| 15   | 一本背负投 | いっぽんせおいなげ | Ippon-seoi-nage    |  \n\n## 2.腰技(11本)\n| 序号 | 名称       | 日语              | 英语                 |\n|------|------------|-------------------|----------------------|\n|      | 腰技       | こしわざ          | KOSHI-WAZA           |\n| 1    | 浮腰       | うきごし          | Uki-goshi            |\n| 2    | 大腰       | おおごし          | O-goshi              |\n| 3    | 腰车       | こしぐるま        | Koshi-guruma         |\n| 4    | 钓込腰     | つりこみごし      | Tsurikomi-goshi      |\n| 5    | 払腰       | はらいごし        | Harai-goshi          |\n| 6    | 钓腰       | つりごし          | Tsuri-goshi          |\n| 7    | 跳腰       | はねごし          | Hane-goshi           |\n| 8    | 移腰       | うつりごし        | Utsuri-goshi         |\n| 9    | 后腰       | うしろごし        | Ushiro-goshi         |\n| 10   | 抱上       | だきあげ          | Daki-age             |\n|      | *比赛中没  | *試合では有効な技 |                      |\n|      | 有得分效果 | とみなされない    |                      |\n| 11   | 袖钓込腰   | そでつりこみごし  | Sode-tsurikomi-goshi |  \n\n## 3.足技 (21本)\n| 序号 | 名称     | 日语               | 英语                 |\n|------|----------|--------------------|----------------------|\n|      | 足技     | あしわざ           | ASHI-WAZA            |\n| 1    | 出足払   | であしはらい       | Deashi-harai         |\n| 2    | 膝车     | ひざぐるま         | Hiza-guruma          |\n| 3    | 支钓込足 | ささえつりこみあし | Sasae-tsurikomi-ashi |\n| 4    | 大外刈   | おおそとがり       | Osoto-gari           |\n| 5    | 大内刈   | おおうちがり       | Ouchi-gari           |\n| 6    | 小外刈   | こそとがり         | Kosoto-gari          |\n| 7    | 小内刈   | こうちがり         | Kouchi-gari          |\n| 8    | 送足払   | おくりあしはらい   | Okuri-ashi-harai     |\n| 9    | 内股     | うちまた           | Uchi-mata            |\n| 10   | 小外挂   | こそとがけ         | Kosoto-gake          |\n| 11   | 足车     | あしぐるま         | Ashi-guruma          |\n| 12   | 払钓込足 | はらいつりこみあし | Harai-tsurikomi-ashi |\n| 13   | 大车     | おおぐるま         | O-guruma             |\n| 14   | 大外车   | おおそとぐるま     | Osoto-guruma         |\n| 15   | 大外落   | おおそとおとし     | Osoto-otoshi         |\n| 16   | 燕返     | つばめがえし       | Tsubame-gaeshi       |\n| 17   | 大外返   | おおそとがえし     | Osoto-gaeshi         |\n| 18   | 大内返   | おおうちがえし     | Ouchi-gaeshi         |\n| 19   | 跳腰返   | はねごしがえし     | Hane-goshi-gaeshi    |\n| 20   | 払腰返   | はらいごしがえし   | Harai-goshi-gaeshi   |\n| 21   | 内股返   | うちまたがえし     | Uchi-mata-gaeshi     |  \n\n## 4.真舍身技 (5本)\n| 序号 | 名称     | 日语           | 英语            |\n|------|----------|----------------|-----------------|\n|      | 真舍身技 | ますてみわざ   | MASUTEMI-WAZA   |\n| 1    | 巴投     | ともえなげ     | Tomoe-nage      |\n| 2    | 隅返     | すみがえし     | Sumi-gaeshi     |\n| 3    | 里投     | うらなげ       | Ura-nage        |\n| 4    | 引込返   | ひきこみがえし | Hikikomi-gaeshi |\n| 5    | 俵返     | たわらがえし   | Tawara-gaeshi   |  \n\n## 5.横舍身技 (15本)\n| 序号 | 名称     | 日语             | 英语               |\n|------|----------|------------------|--------------------|\n|      | 横舍身技 | よこすてみわざ   | YOKOSUTEMI-WAZA    |\n| 1    | 横落     | よこおとし       | Yoko-otoshi        |\n| 2    | 谷落     | たにおとし       | Tani-otoshi        |\n| 3    | 跳巻込   | はねまきこみ     | Hane-makikomi      |\n| 4    | 外巻込   | そとまきこみ     | Soto-makikomi      |\n| 5    | 浮技     | うきわざ         | Uki-waza           |\n| 6    | 横分     | よこわかれ       | Yoko-wakare        |\n| 7    | 横车     | よこぐるま       | Yoko-guruma        |\n| 8    | 横挂     | よこがけ         | Yoko-gake          |\n| 9    | 抱分     | だきわかれ       | Daki-wakare        |\n| 10   | 内巻込   | うちまきこみ     | Uchi-makikomi      |\n| 11   | 蟹挟     | かにばさみ       | Kani-basami        |\n| 12   | 大外巻込 | おおそとまきこみ | Osoto-makikomi     |\n| 13   | 内股巻込 | うちまたまきこみ | Uchi-mata-makikomi |\n| 14   | 払巻込   | はらいまきこみ   | Harai-makikomi     |\n| 15   | 河津挂   | かわづがけ       | Kawazu-gake        |\n|      | *禁止技  | *禁止技          | .                  |  \n\n\n# 二 固技 (29本)\n投技是以手、臂、腿、躯干等为主要发力点，通过柔道动作，使对手因窒息或肌肉、关节等的疼痛而在地面上被降服的技术。\n具体分为三类：抑込技、绞技、关节技。  \n\n## 1.抑込技 (7本)\n| 序号 | 名称       | 日语                   | 英语                     |\n|------|------------|------------------------|--------------------------|\n|      | 抑込技     | おさえこみわざ         | OSAEKOMI-WAZA            |\n| 1    | 崩袈裟固   | くずれけさがため        | Kuzure-kesa-gatame       |\n| 2    | 肩固       | かたがため             | Kata-gatame              |\n| 3    | 上四方固   | かみしほうがため       | Kami-shiho-gatame        |\n| 4    | 崩上四方固 | くずれかみしほうがため | Kuzure-kami-shiho-gatame |\n| 5    | 横四方固   | よこしほうがため       | Yoko-shiho-gatame        |\n| 6    | 纵四方固   | たてしほうがため       | Tate-shiho-gatame        |\n| 7    | 袈裟固     | けさがため             | Kesa-gatame              |  \n\n## 2.绞技  (12本)\n| 序号 | 名称     | 日语               | 英语             |\n|------|----------|--------------------|------------------|\n|      | 绞技     | しめわざ           | SHIME-WAZA       |\n| 1    | 并十字绞 | なみじゅうじじめ   | Nami-juji-jime   |\n| 2    | 逆十字绞 | ぎゃくじゅうじじめ | Gyaku-juji-jime  |\n| 3    | 片十字绞 | かたじゅうじじめ   | Kata-juji-jime   |\n| 4    | 裸绞     | はだかじめ         | Hadaka-jime      |\n| 5    | 送襟绞   | おくりえりじめ     | Okuri-eri-jime   |\n| 6    | 片羽绞   | かたはじめ         | Kata-ha-jime     |\n| 7    | 胴绞     | どうじめ           | Do-jime          |\n|      | *禁止技  | *禁止技            |                  |\n| 8    | 袖车绞   | そでぐるまじめ     | Sode-guruma-jime |\n| 9    | 片手绞   | かたてじめ         | Kata-te-jime     |\n| 10   | 両手绞   | りょうてじめ       | Ryo-te-jime      |\n| 11   | 突込绞   | つっこみじめ       | Tsukkomi-jime    |\n| 12   | 三角绞   | さんかくじめ       | Sankaku-jime     |  \n\n## 3.关节技 (10本)\n| 序号 | 名称       | 日语                     | 英语                       |\n|------|------------|--------------------------|----------------------------|\n|      | 关节技     | かんせつわざ             | KANSETSU-WAZA              |\n| 1    | 腕缄       | うでがらみ               | Ude-garami                 |\n| 2    | 腕挫十字固 | うでひしぎじゅうじがため | Ude-hishigi-juji-gatame    |\n| 3    | 腕挫腕固   | うでひしぎうでがため     | Ude-hishigi-ude-gatame     |\n| 4    | 腕挫膝固   | うでひしぎひざがため     | Ude-hishigi-hiza-gatame    |\n| 5    | 腕挫腋固   | うでひしぎわきがため     | Ude-hishigi-waki-gatame    |\n| 6    | 腕挫腹固   | うてひしぎはらがため     | Ude-hishigi-hara-gatame    |\n| 7    | 足缄       | あしがらみ               | Ashi-garami                |\n|      | *禁止技    | *禁止技                  |                            |\n| 8    | 腕挫脚固   | うでひしぎあしがため     | Ude-hishigi-ashi-gatame    |\n| 9    | 腕挫手固   | うでひしぎてがため       | Ude-hishigi-te-gatame      |\n| 10   | 腕挫三角固 | うてひしぎさんかくがため | Ude-hishigi-sankaku-gatame  |  \n\n\n# 三 当身技 (あてみわざ)\n当身技是通过戳、打、踢等，直接攻击对手要害部位或生理弱点的技术。由于该技术危险性太大，容易对选手造成严重伤害，所以在柔道比赛和训练中被禁止使用。\n具体分为两类：当所和急所。  \n\n## 1.当所\n臂（うで）：\n指先当（ゆびさきあて）：突出、両眼突\n拳当（こぶしあて）：斜当、横当、上当、突上、下突、後突、後隅突、突掛、横打、後打、打下\n手刀当（てがたなあて、手掌の小指側縁）：切下、斜打\n肘当（ひじあて）：後当. \n \n脚（あし）：\n膝頭当（ひざがしらあて）：前当\n蹠頭当（せきとうあて、足蹠の前端）：斜蹴、前蹴、高蹴\n踵当（かかとあて）：後蹴、横蹴. \n\n## 2.急所\n天倒、霞、鳥兎、獨鈷、人中、三日月、松風、村雨、秘中、タン中、水月、雁下、明星、月影、電光、稲妻、臍下丹田、釣鐘（金的）、肘詰、伏兎、向骨。  \n","slug":"ja-柔道用語-Judo-terminology","published":1,"updated":"2020-11-10T17:41:54.823Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zf0022og64ij8upyk1","content":"<p><center><img src=\"/image/Judo/Judo.jpg\" alt=\"W\"></center></p>\n<h1 id=\"一-投技-67本\"><a href=\"#一-投技-67本\" class=\"headerlink\" title=\"一 投技 (67本)\"></a>一 投技 (67本)</h1><p>投技是以手、腰、足等为主要发力点，通过柔道动作，使对手倒地的技术。<br>具体分为五类：手技、腰技、足技、真舍身技、横舍身技。   </p>\n<h2 id=\"1-手技-15本\"><a href=\"#1-手技-15本\" class=\"headerlink\" title=\"1.手技 (15本)\"></a>1.手技 (15本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>手技</td>\n<td>てわざ</td>\n<td>TE-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>背负投</td>\n<td>せおいなげ</td>\n<td>Seoi-nage</td>\n</tr>\n<tr>\n<td>2</td>\n<td>体落</td>\n<td>たいおとし</td>\n<td>Tai-otoshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>肩车</td>\n<td>かたぐるま</td>\n<td>Kata-guruma</td>\n</tr>\n<tr>\n<td>4</td>\n<td>掬投</td>\n<td>すくいなげ</td>\n<td>Sukui-nage</td>\n</tr>\n<tr>\n<td>5</td>\n<td>浮落</td>\n<td>うきおとし</td>\n<td>Uki-otoshi</td>\n</tr>\n<tr>\n<td>6</td>\n<td>隅落</td>\n<td>すみおとし</td>\n<td>Sumi-otoshi</td>\n</tr>\n<tr>\n<td>7</td>\n<td>帯落</td>\n<td>おびおとし</td>\n<td>Obi-otoshi</td>\n</tr>\n<tr>\n<td>8</td>\n<td>背负落</td>\n<td>せおいおとし</td>\n<td>Seoi-otoshi</td>\n</tr>\n<tr>\n<td>9</td>\n<td>山岚</td>\n<td>やまあらし</td>\n<td>Yama-arashi</td>\n</tr>\n<tr>\n<td>10</td>\n<td>双手刈</td>\n<td>もろてがり</td>\n<td>Morote-gari</td>\n</tr>\n<tr>\n<td>11</td>\n<td>朽木倒</td>\n<td>くちきたおし</td>\n<td>Kuchiki-taoshi</td>\n</tr>\n<tr>\n<td>12</td>\n<td>踵返</td>\n<td>きびすがえし</td>\n<td>Kibisu-gaeshi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>内股</td>\n<td>うちまたすかし</td>\n<td>Uchi-mata-sukashi</td>\n</tr>\n<tr>\n<td>14</td>\n<td>小内返</td>\n<td>こうちがえし</td>\n<td>Kouchi-gaeshi</td>\n</tr>\n<tr>\n<td>15</td>\n<td>一本背负投</td>\n<td>いっぽんせおいなげ</td>\n<td>Ippon-seoi-nage</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-腰技-11本\"><a href=\"#2-腰技-11本\" class=\"headerlink\" title=\"2.腰技(11本)\"></a>2.腰技(11本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>腰技</td>\n<td>こしわざ</td>\n<td>KOSHI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>浮腰</td>\n<td>うきごし</td>\n<td>Uki-goshi</td>\n</tr>\n<tr>\n<td>2</td>\n<td>大腰</td>\n<td>おおごし</td>\n<td>O-goshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>腰车</td>\n<td>こしぐるま</td>\n<td>Koshi-guruma</td>\n</tr>\n<tr>\n<td>4</td>\n<td>钓込腰</td>\n<td>つりこみごし</td>\n<td>Tsurikomi-goshi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>払腰</td>\n<td>はらいごし</td>\n<td>Harai-goshi</td>\n</tr>\n<tr>\n<td>6</td>\n<td>钓腰</td>\n<td>つりごし</td>\n<td>Tsuri-goshi</td>\n</tr>\n<tr>\n<td>7</td>\n<td>跳腰</td>\n<td>はねごし</td>\n<td>Hane-goshi</td>\n</tr>\n<tr>\n<td>8</td>\n<td>移腰</td>\n<td>うつりごし</td>\n<td>Utsuri-goshi</td>\n</tr>\n<tr>\n<td>9</td>\n<td>后腰</td>\n<td>うしろごし</td>\n<td>Ushiro-goshi</td>\n</tr>\n<tr>\n<td>10</td>\n<td>抱上</td>\n<td>だきあげ</td>\n<td>Daki-age</td>\n</tr>\n<tr>\n<td></td>\n<td>*比赛中没</td>\n<td>*試合では有効な技</td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>有得分效果</td>\n<td>とみなされない</td>\n<td></td>\n</tr>\n<tr>\n<td>11</td>\n<td>袖钓込腰</td>\n<td>そでつりこみごし</td>\n<td>Sode-tsurikomi-goshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-足技-21本\"><a href=\"#3-足技-21本\" class=\"headerlink\" title=\"3.足技 (21本)\"></a>3.足技 (21本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>足技</td>\n<td>あしわざ</td>\n<td>ASHI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>出足払</td>\n<td>であしはらい</td>\n<td>Deashi-harai</td>\n</tr>\n<tr>\n<td>2</td>\n<td>膝车</td>\n<td>ひざぐるま</td>\n<td>Hiza-guruma</td>\n</tr>\n<tr>\n<td>3</td>\n<td>支钓込足</td>\n<td>ささえつりこみあし</td>\n<td>Sasae-tsurikomi-ashi</td>\n</tr>\n<tr>\n<td>4</td>\n<td>大外刈</td>\n<td>おおそとがり</td>\n<td>Osoto-gari</td>\n</tr>\n<tr>\n<td>5</td>\n<td>大内刈</td>\n<td>おおうちがり</td>\n<td>Ouchi-gari</td>\n</tr>\n<tr>\n<td>6</td>\n<td>小外刈</td>\n<td>こそとがり</td>\n<td>Kosoto-gari</td>\n</tr>\n<tr>\n<td>7</td>\n<td>小内刈</td>\n<td>こうちがり</td>\n<td>Kouchi-gari</td>\n</tr>\n<tr>\n<td>8</td>\n<td>送足払</td>\n<td>おくりあしはらい</td>\n<td>Okuri-ashi-harai</td>\n</tr>\n<tr>\n<td>9</td>\n<td>内股</td>\n<td>うちまた</td>\n<td>Uchi-mata</td>\n</tr>\n<tr>\n<td>10</td>\n<td>小外挂</td>\n<td>こそとがけ</td>\n<td>Kosoto-gake</td>\n</tr>\n<tr>\n<td>11</td>\n<td>足车</td>\n<td>あしぐるま</td>\n<td>Ashi-guruma</td>\n</tr>\n<tr>\n<td>12</td>\n<td>払钓込足</td>\n<td>はらいつりこみあし</td>\n<td>Harai-tsurikomi-ashi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>大车</td>\n<td>おおぐるま</td>\n<td>O-guruma</td>\n</tr>\n<tr>\n<td>14</td>\n<td>大外车</td>\n<td>おおそとぐるま</td>\n<td>Osoto-guruma</td>\n</tr>\n<tr>\n<td>15</td>\n<td>大外落</td>\n<td>おおそとおとし</td>\n<td>Osoto-otoshi</td>\n</tr>\n<tr>\n<td>16</td>\n<td>燕返</td>\n<td>つばめがえし</td>\n<td>Tsubame-gaeshi</td>\n</tr>\n<tr>\n<td>17</td>\n<td>大外返</td>\n<td>おおそとがえし</td>\n<td>Osoto-gaeshi</td>\n</tr>\n<tr>\n<td>18</td>\n<td>大内返</td>\n<td>おおうちがえし</td>\n<td>Ouchi-gaeshi</td>\n</tr>\n<tr>\n<td>19</td>\n<td>跳腰返</td>\n<td>はねごしがえし</td>\n<td>Hane-goshi-gaeshi</td>\n</tr>\n<tr>\n<td>20</td>\n<td>払腰返</td>\n<td>はらいごしがえし</td>\n<td>Harai-goshi-gaeshi</td>\n</tr>\n<tr>\n<td>21</td>\n<td>内股返</td>\n<td>うちまたがえし</td>\n<td>Uchi-mata-gaeshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"4-真舍身技-5本\"><a href=\"#4-真舍身技-5本\" class=\"headerlink\" title=\"4.真舍身技 (5本)\"></a>4.真舍身技 (5本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>真舍身技</td>\n<td>ますてみわざ</td>\n<td>MASUTEMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>巴投</td>\n<td>ともえなげ</td>\n<td>Tomoe-nage</td>\n</tr>\n<tr>\n<td>2</td>\n<td>隅返</td>\n<td>すみがえし</td>\n<td>Sumi-gaeshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>里投</td>\n<td>うらなげ</td>\n<td>Ura-nage</td>\n</tr>\n<tr>\n<td>4</td>\n<td>引込返</td>\n<td>ひきこみがえし</td>\n<td>Hikikomi-gaeshi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>俵返</td>\n<td>たわらがえし</td>\n<td>Tawara-gaeshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"5-横舍身技-15本\"><a href=\"#5-横舍身技-15本\" class=\"headerlink\" title=\"5.横舍身技 (15本)\"></a>5.横舍身技 (15本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>横舍身技</td>\n<td>よこすてみわざ</td>\n<td>YOKOSUTEMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>横落</td>\n<td>よこおとし</td>\n<td>Yoko-otoshi</td>\n</tr>\n<tr>\n<td>2</td>\n<td>谷落</td>\n<td>たにおとし</td>\n<td>Tani-otoshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>跳巻込</td>\n<td>はねまきこみ</td>\n<td>Hane-makikomi</td>\n</tr>\n<tr>\n<td>4</td>\n<td>外巻込</td>\n<td>そとまきこみ</td>\n<td>Soto-makikomi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>浮技</td>\n<td>うきわざ</td>\n<td>Uki-waza</td>\n</tr>\n<tr>\n<td>6</td>\n<td>横分</td>\n<td>よこわかれ</td>\n<td>Yoko-wakare</td>\n</tr>\n<tr>\n<td>7</td>\n<td>横车</td>\n<td>よこぐるま</td>\n<td>Yoko-guruma</td>\n</tr>\n<tr>\n<td>8</td>\n<td>横挂</td>\n<td>よこがけ</td>\n<td>Yoko-gake</td>\n</tr>\n<tr>\n<td>9</td>\n<td>抱分</td>\n<td>だきわかれ</td>\n<td>Daki-wakare</td>\n</tr>\n<tr>\n<td>10</td>\n<td>内巻込</td>\n<td>うちまきこみ</td>\n<td>Uchi-makikomi</td>\n</tr>\n<tr>\n<td>11</td>\n<td>蟹挟</td>\n<td>かにばさみ</td>\n<td>Kani-basami</td>\n</tr>\n<tr>\n<td>12</td>\n<td>大外巻込</td>\n<td>おおそとまきこみ</td>\n<td>Osoto-makikomi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>内股巻込</td>\n<td>うちまたまきこみ</td>\n<td>Uchi-mata-makikomi</td>\n</tr>\n<tr>\n<td>14</td>\n<td>払巻込</td>\n<td>はらいまきこみ</td>\n<td>Harai-makikomi</td>\n</tr>\n<tr>\n<td>15</td>\n<td>河津挂</td>\n<td>かわづがけ</td>\n<td>Kawazu-gake</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td>.</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"二-固技-29本\"><a href=\"#二-固技-29本\" class=\"headerlink\" title=\"二 固技 (29本)\"></a>二 固技 (29本)</h1><p>投技是以手、臂、腿、躯干等为主要发力点，通过柔道动作，使对手因窒息或肌肉、关节等的疼痛而在地面上被降服的技术。<br>具体分为三类：抑込技、绞技、关节技。  </p>\n<h2 id=\"1-抑込技-7本\"><a href=\"#1-抑込技-7本\" class=\"headerlink\" title=\"1.抑込技 (7本)\"></a>1.抑込技 (7本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>抑込技</td>\n<td>おさえこみわざ</td>\n<td>OSAEKOMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>崩袈裟固</td>\n<td>くずれけさがため</td>\n<td>Kuzure-kesa-gatame</td>\n</tr>\n<tr>\n<td>2</td>\n<td>肩固</td>\n<td>かたがため</td>\n<td>Kata-gatame</td>\n</tr>\n<tr>\n<td>3</td>\n<td>上四方固</td>\n<td>かみしほうがため</td>\n<td>Kami-shiho-gatame</td>\n</tr>\n<tr>\n<td>4</td>\n<td>崩上四方固</td>\n<td>くずれかみしほうがため</td>\n<td>Kuzure-kami-shiho-gatame</td>\n</tr>\n<tr>\n<td>5</td>\n<td>横四方固</td>\n<td>よこしほうがため</td>\n<td>Yoko-shiho-gatame</td>\n</tr>\n<tr>\n<td>6</td>\n<td>纵四方固</td>\n<td>たてしほうがため</td>\n<td>Tate-shiho-gatame</td>\n</tr>\n<tr>\n<td>7</td>\n<td>袈裟固</td>\n<td>けさがため</td>\n<td>Kesa-gatame</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-绞技-12本\"><a href=\"#2-绞技-12本\" class=\"headerlink\" title=\"2.绞技  (12本)\"></a>2.绞技  (12本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>绞技</td>\n<td>しめわざ</td>\n<td>SHIME-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>并十字绞</td>\n<td>なみじゅうじじめ</td>\n<td>Nami-juji-jime</td>\n</tr>\n<tr>\n<td>2</td>\n<td>逆十字绞</td>\n<td>ぎゃくじゅうじじめ</td>\n<td>Gyaku-juji-jime</td>\n</tr>\n<tr>\n<td>3</td>\n<td>片十字绞</td>\n<td>かたじゅうじじめ</td>\n<td>Kata-juji-jime</td>\n</tr>\n<tr>\n<td>4</td>\n<td>裸绞</td>\n<td>はだかじめ</td>\n<td>Hadaka-jime</td>\n</tr>\n<tr>\n<td>5</td>\n<td>送襟绞</td>\n<td>おくりえりじめ</td>\n<td>Okuri-eri-jime</td>\n</tr>\n<tr>\n<td>6</td>\n<td>片羽绞</td>\n<td>かたはじめ</td>\n<td>Kata-ha-jime</td>\n</tr>\n<tr>\n<td>7</td>\n<td>胴绞</td>\n<td>どうじめ</td>\n<td>Do-jime</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td></td>\n</tr>\n<tr>\n<td>8</td>\n<td>袖车绞</td>\n<td>そでぐるまじめ</td>\n<td>Sode-guruma-jime</td>\n</tr>\n<tr>\n<td>9</td>\n<td>片手绞</td>\n<td>かたてじめ</td>\n<td>Kata-te-jime</td>\n</tr>\n<tr>\n<td>10</td>\n<td>両手绞</td>\n<td>りょうてじめ</td>\n<td>Ryo-te-jime</td>\n</tr>\n<tr>\n<td>11</td>\n<td>突込绞</td>\n<td>つっこみじめ</td>\n<td>Tsukkomi-jime</td>\n</tr>\n<tr>\n<td>12</td>\n<td>三角绞</td>\n<td>さんかくじめ</td>\n<td>Sankaku-jime</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-关节技-10本\"><a href=\"#3-关节技-10本\" class=\"headerlink\" title=\"3.关节技 (10本)\"></a>3.关节技 (10本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>关节技</td>\n<td>かんせつわざ</td>\n<td>KANSETSU-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>腕缄</td>\n<td>うでがらみ</td>\n<td>Ude-garami</td>\n</tr>\n<tr>\n<td>2</td>\n<td>腕挫十字固</td>\n<td>うでひしぎじゅうじがため</td>\n<td>Ude-hishigi-juji-gatame</td>\n</tr>\n<tr>\n<td>3</td>\n<td>腕挫腕固</td>\n<td>うでひしぎうでがため</td>\n<td>Ude-hishigi-ude-gatame</td>\n</tr>\n<tr>\n<td>4</td>\n<td>腕挫膝固</td>\n<td>うでひしぎひざがため</td>\n<td>Ude-hishigi-hiza-gatame</td>\n</tr>\n<tr>\n<td>5</td>\n<td>腕挫腋固</td>\n<td>うでひしぎわきがため</td>\n<td>Ude-hishigi-waki-gatame</td>\n</tr>\n<tr>\n<td>6</td>\n<td>腕挫腹固</td>\n<td>うてひしぎはらがため</td>\n<td>Ude-hishigi-hara-gatame</td>\n</tr>\n<tr>\n<td>7</td>\n<td>足缄</td>\n<td>あしがらみ</td>\n<td>Ashi-garami</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td></td>\n</tr>\n<tr>\n<td>8</td>\n<td>腕挫脚固</td>\n<td>うでひしぎあしがため</td>\n<td>Ude-hishigi-ashi-gatame</td>\n</tr>\n<tr>\n<td>9</td>\n<td>腕挫手固</td>\n<td>うでひしぎてがため</td>\n<td>Ude-hishigi-te-gatame</td>\n</tr>\n<tr>\n<td>10</td>\n<td>腕挫三角固</td>\n<td>うてひしぎさんかくがため</td>\n<td>Ude-hishigi-sankaku-gatame</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"三-当身技-あてみわざ\"><a href=\"#三-当身技-あてみわざ\" class=\"headerlink\" title=\"三 当身技 (あてみわざ)\"></a>三 当身技 (あてみわざ)</h1><p>当身技是通过戳、打、踢等，直接攻击对手要害部位或生理弱点的技术。由于该技术危险性太大，容易对选手造成严重伤害，所以在柔道比赛和训练中被禁止使用。<br>具体分为两类：当所和急所。  </p>\n<h2 id=\"1-当所\"><a href=\"#1-当所\" class=\"headerlink\" title=\"1.当所\"></a>1.当所</h2><p>臂（うで）：<br>指先当（ゆびさきあて）：突出、両眼突<br>拳当（こぶしあて）：斜当、横当、上当、突上、下突、後突、後隅突、突掛、横打、後打、打下<br>手刀当（てがたなあて、手掌の小指側縁）：切下、斜打<br>肘当（ひじあて）：後当. </p>\n<p>脚（あし）：<br>膝頭当（ひざがしらあて）：前当<br>蹠頭当（せきとうあて、足蹠の前端）：斜蹴、前蹴、高蹴<br>踵当（かかとあて）：後蹴、横蹴. </p>\n<h2 id=\"2-急所\"><a href=\"#2-急所\" class=\"headerlink\" title=\"2.急所\"></a>2.急所</h2><p>天倒、霞、鳥兎、獨鈷、人中、三日月、松風、村雨、秘中、タン中、水月、雁下、明星、月影、電光、稲妻、臍下丹田、釣鐘（金的）、肘詰、伏兎、向骨。  </p>\n","excerpt":"","more":"<p><center><img src=\"/image/Judo/Judo.jpg\" alt=\"W\"></center></p>\n<h1 id=\"一-投技-67本\"><a href=\"#一-投技-67本\" class=\"headerlink\" title=\"一 投技 (67本)\"></a>一 投技 (67本)</h1><p>投技是以手、腰、足等为主要发力点，通过柔道动作，使对手倒地的技术。<br>具体分为五类：手技、腰技、足技、真舍身技、横舍身技。   </p>\n<h2 id=\"1-手技-15本\"><a href=\"#1-手技-15本\" class=\"headerlink\" title=\"1.手技 (15本)\"></a>1.手技 (15本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>手技</td>\n<td>てわざ</td>\n<td>TE-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>背负投</td>\n<td>せおいなげ</td>\n<td>Seoi-nage</td>\n</tr>\n<tr>\n<td>2</td>\n<td>体落</td>\n<td>たいおとし</td>\n<td>Tai-otoshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>肩车</td>\n<td>かたぐるま</td>\n<td>Kata-guruma</td>\n</tr>\n<tr>\n<td>4</td>\n<td>掬投</td>\n<td>すくいなげ</td>\n<td>Sukui-nage</td>\n</tr>\n<tr>\n<td>5</td>\n<td>浮落</td>\n<td>うきおとし</td>\n<td>Uki-otoshi</td>\n</tr>\n<tr>\n<td>6</td>\n<td>隅落</td>\n<td>すみおとし</td>\n<td>Sumi-otoshi</td>\n</tr>\n<tr>\n<td>7</td>\n<td>帯落</td>\n<td>おびおとし</td>\n<td>Obi-otoshi</td>\n</tr>\n<tr>\n<td>8</td>\n<td>背负落</td>\n<td>せおいおとし</td>\n<td>Seoi-otoshi</td>\n</tr>\n<tr>\n<td>9</td>\n<td>山岚</td>\n<td>やまあらし</td>\n<td>Yama-arashi</td>\n</tr>\n<tr>\n<td>10</td>\n<td>双手刈</td>\n<td>もろてがり</td>\n<td>Morote-gari</td>\n</tr>\n<tr>\n<td>11</td>\n<td>朽木倒</td>\n<td>くちきたおし</td>\n<td>Kuchiki-taoshi</td>\n</tr>\n<tr>\n<td>12</td>\n<td>踵返</td>\n<td>きびすがえし</td>\n<td>Kibisu-gaeshi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>内股</td>\n<td>うちまたすかし</td>\n<td>Uchi-mata-sukashi</td>\n</tr>\n<tr>\n<td>14</td>\n<td>小内返</td>\n<td>こうちがえし</td>\n<td>Kouchi-gaeshi</td>\n</tr>\n<tr>\n<td>15</td>\n<td>一本背负投</td>\n<td>いっぽんせおいなげ</td>\n<td>Ippon-seoi-nage</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-腰技-11本\"><a href=\"#2-腰技-11本\" class=\"headerlink\" title=\"2.腰技(11本)\"></a>2.腰技(11本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>腰技</td>\n<td>こしわざ</td>\n<td>KOSHI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>浮腰</td>\n<td>うきごし</td>\n<td>Uki-goshi</td>\n</tr>\n<tr>\n<td>2</td>\n<td>大腰</td>\n<td>おおごし</td>\n<td>O-goshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>腰车</td>\n<td>こしぐるま</td>\n<td>Koshi-guruma</td>\n</tr>\n<tr>\n<td>4</td>\n<td>钓込腰</td>\n<td>つりこみごし</td>\n<td>Tsurikomi-goshi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>払腰</td>\n<td>はらいごし</td>\n<td>Harai-goshi</td>\n</tr>\n<tr>\n<td>6</td>\n<td>钓腰</td>\n<td>つりごし</td>\n<td>Tsuri-goshi</td>\n</tr>\n<tr>\n<td>7</td>\n<td>跳腰</td>\n<td>はねごし</td>\n<td>Hane-goshi</td>\n</tr>\n<tr>\n<td>8</td>\n<td>移腰</td>\n<td>うつりごし</td>\n<td>Utsuri-goshi</td>\n</tr>\n<tr>\n<td>9</td>\n<td>后腰</td>\n<td>うしろごし</td>\n<td>Ushiro-goshi</td>\n</tr>\n<tr>\n<td>10</td>\n<td>抱上</td>\n<td>だきあげ</td>\n<td>Daki-age</td>\n</tr>\n<tr>\n<td></td>\n<td>*比赛中没</td>\n<td>*試合では有効な技</td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>有得分效果</td>\n<td>とみなされない</td>\n<td></td>\n</tr>\n<tr>\n<td>11</td>\n<td>袖钓込腰</td>\n<td>そでつりこみごし</td>\n<td>Sode-tsurikomi-goshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-足技-21本\"><a href=\"#3-足技-21本\" class=\"headerlink\" title=\"3.足技 (21本)\"></a>3.足技 (21本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>足技</td>\n<td>あしわざ</td>\n<td>ASHI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>出足払</td>\n<td>であしはらい</td>\n<td>Deashi-harai</td>\n</tr>\n<tr>\n<td>2</td>\n<td>膝车</td>\n<td>ひざぐるま</td>\n<td>Hiza-guruma</td>\n</tr>\n<tr>\n<td>3</td>\n<td>支钓込足</td>\n<td>ささえつりこみあし</td>\n<td>Sasae-tsurikomi-ashi</td>\n</tr>\n<tr>\n<td>4</td>\n<td>大外刈</td>\n<td>おおそとがり</td>\n<td>Osoto-gari</td>\n</tr>\n<tr>\n<td>5</td>\n<td>大内刈</td>\n<td>おおうちがり</td>\n<td>Ouchi-gari</td>\n</tr>\n<tr>\n<td>6</td>\n<td>小外刈</td>\n<td>こそとがり</td>\n<td>Kosoto-gari</td>\n</tr>\n<tr>\n<td>7</td>\n<td>小内刈</td>\n<td>こうちがり</td>\n<td>Kouchi-gari</td>\n</tr>\n<tr>\n<td>8</td>\n<td>送足払</td>\n<td>おくりあしはらい</td>\n<td>Okuri-ashi-harai</td>\n</tr>\n<tr>\n<td>9</td>\n<td>内股</td>\n<td>うちまた</td>\n<td>Uchi-mata</td>\n</tr>\n<tr>\n<td>10</td>\n<td>小外挂</td>\n<td>こそとがけ</td>\n<td>Kosoto-gake</td>\n</tr>\n<tr>\n<td>11</td>\n<td>足车</td>\n<td>あしぐるま</td>\n<td>Ashi-guruma</td>\n</tr>\n<tr>\n<td>12</td>\n<td>払钓込足</td>\n<td>はらいつりこみあし</td>\n<td>Harai-tsurikomi-ashi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>大车</td>\n<td>おおぐるま</td>\n<td>O-guruma</td>\n</tr>\n<tr>\n<td>14</td>\n<td>大外车</td>\n<td>おおそとぐるま</td>\n<td>Osoto-guruma</td>\n</tr>\n<tr>\n<td>15</td>\n<td>大外落</td>\n<td>おおそとおとし</td>\n<td>Osoto-otoshi</td>\n</tr>\n<tr>\n<td>16</td>\n<td>燕返</td>\n<td>つばめがえし</td>\n<td>Tsubame-gaeshi</td>\n</tr>\n<tr>\n<td>17</td>\n<td>大外返</td>\n<td>おおそとがえし</td>\n<td>Osoto-gaeshi</td>\n</tr>\n<tr>\n<td>18</td>\n<td>大内返</td>\n<td>おおうちがえし</td>\n<td>Ouchi-gaeshi</td>\n</tr>\n<tr>\n<td>19</td>\n<td>跳腰返</td>\n<td>はねごしがえし</td>\n<td>Hane-goshi-gaeshi</td>\n</tr>\n<tr>\n<td>20</td>\n<td>払腰返</td>\n<td>はらいごしがえし</td>\n<td>Harai-goshi-gaeshi</td>\n</tr>\n<tr>\n<td>21</td>\n<td>内股返</td>\n<td>うちまたがえし</td>\n<td>Uchi-mata-gaeshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"4-真舍身技-5本\"><a href=\"#4-真舍身技-5本\" class=\"headerlink\" title=\"4.真舍身技 (5本)\"></a>4.真舍身技 (5本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>真舍身技</td>\n<td>ますてみわざ</td>\n<td>MASUTEMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>巴投</td>\n<td>ともえなげ</td>\n<td>Tomoe-nage</td>\n</tr>\n<tr>\n<td>2</td>\n<td>隅返</td>\n<td>すみがえし</td>\n<td>Sumi-gaeshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>里投</td>\n<td>うらなげ</td>\n<td>Ura-nage</td>\n</tr>\n<tr>\n<td>4</td>\n<td>引込返</td>\n<td>ひきこみがえし</td>\n<td>Hikikomi-gaeshi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>俵返</td>\n<td>たわらがえし</td>\n<td>Tawara-gaeshi</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"5-横舍身技-15本\"><a href=\"#5-横舍身技-15本\" class=\"headerlink\" title=\"5.横舍身技 (15本)\"></a>5.横舍身技 (15本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>横舍身技</td>\n<td>よこすてみわざ</td>\n<td>YOKOSUTEMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>横落</td>\n<td>よこおとし</td>\n<td>Yoko-otoshi</td>\n</tr>\n<tr>\n<td>2</td>\n<td>谷落</td>\n<td>たにおとし</td>\n<td>Tani-otoshi</td>\n</tr>\n<tr>\n<td>3</td>\n<td>跳巻込</td>\n<td>はねまきこみ</td>\n<td>Hane-makikomi</td>\n</tr>\n<tr>\n<td>4</td>\n<td>外巻込</td>\n<td>そとまきこみ</td>\n<td>Soto-makikomi</td>\n</tr>\n<tr>\n<td>5</td>\n<td>浮技</td>\n<td>うきわざ</td>\n<td>Uki-waza</td>\n</tr>\n<tr>\n<td>6</td>\n<td>横分</td>\n<td>よこわかれ</td>\n<td>Yoko-wakare</td>\n</tr>\n<tr>\n<td>7</td>\n<td>横车</td>\n<td>よこぐるま</td>\n<td>Yoko-guruma</td>\n</tr>\n<tr>\n<td>8</td>\n<td>横挂</td>\n<td>よこがけ</td>\n<td>Yoko-gake</td>\n</tr>\n<tr>\n<td>9</td>\n<td>抱分</td>\n<td>だきわかれ</td>\n<td>Daki-wakare</td>\n</tr>\n<tr>\n<td>10</td>\n<td>内巻込</td>\n<td>うちまきこみ</td>\n<td>Uchi-makikomi</td>\n</tr>\n<tr>\n<td>11</td>\n<td>蟹挟</td>\n<td>かにばさみ</td>\n<td>Kani-basami</td>\n</tr>\n<tr>\n<td>12</td>\n<td>大外巻込</td>\n<td>おおそとまきこみ</td>\n<td>Osoto-makikomi</td>\n</tr>\n<tr>\n<td>13</td>\n<td>内股巻込</td>\n<td>うちまたまきこみ</td>\n<td>Uchi-mata-makikomi</td>\n</tr>\n<tr>\n<td>14</td>\n<td>払巻込</td>\n<td>はらいまきこみ</td>\n<td>Harai-makikomi</td>\n</tr>\n<tr>\n<td>15</td>\n<td>河津挂</td>\n<td>かわづがけ</td>\n<td>Kawazu-gake</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td>.</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"二-固技-29本\"><a href=\"#二-固技-29本\" class=\"headerlink\" title=\"二 固技 (29本)\"></a>二 固技 (29本)</h1><p>投技是以手、臂、腿、躯干等为主要发力点，通过柔道动作，使对手因窒息或肌肉、关节等的疼痛而在地面上被降服的技术。<br>具体分为三类：抑込技、绞技、关节技。  </p>\n<h2 id=\"1-抑込技-7本\"><a href=\"#1-抑込技-7本\" class=\"headerlink\" title=\"1.抑込技 (7本)\"></a>1.抑込技 (7本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>抑込技</td>\n<td>おさえこみわざ</td>\n<td>OSAEKOMI-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>崩袈裟固</td>\n<td>くずれけさがため</td>\n<td>Kuzure-kesa-gatame</td>\n</tr>\n<tr>\n<td>2</td>\n<td>肩固</td>\n<td>かたがため</td>\n<td>Kata-gatame</td>\n</tr>\n<tr>\n<td>3</td>\n<td>上四方固</td>\n<td>かみしほうがため</td>\n<td>Kami-shiho-gatame</td>\n</tr>\n<tr>\n<td>4</td>\n<td>崩上四方固</td>\n<td>くずれかみしほうがため</td>\n<td>Kuzure-kami-shiho-gatame</td>\n</tr>\n<tr>\n<td>5</td>\n<td>横四方固</td>\n<td>よこしほうがため</td>\n<td>Yoko-shiho-gatame</td>\n</tr>\n<tr>\n<td>6</td>\n<td>纵四方固</td>\n<td>たてしほうがため</td>\n<td>Tate-shiho-gatame</td>\n</tr>\n<tr>\n<td>7</td>\n<td>袈裟固</td>\n<td>けさがため</td>\n<td>Kesa-gatame</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"2-绞技-12本\"><a href=\"#2-绞技-12本\" class=\"headerlink\" title=\"2.绞技  (12本)\"></a>2.绞技  (12本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>绞技</td>\n<td>しめわざ</td>\n<td>SHIME-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>并十字绞</td>\n<td>なみじゅうじじめ</td>\n<td>Nami-juji-jime</td>\n</tr>\n<tr>\n<td>2</td>\n<td>逆十字绞</td>\n<td>ぎゃくじゅうじじめ</td>\n<td>Gyaku-juji-jime</td>\n</tr>\n<tr>\n<td>3</td>\n<td>片十字绞</td>\n<td>かたじゅうじじめ</td>\n<td>Kata-juji-jime</td>\n</tr>\n<tr>\n<td>4</td>\n<td>裸绞</td>\n<td>はだかじめ</td>\n<td>Hadaka-jime</td>\n</tr>\n<tr>\n<td>5</td>\n<td>送襟绞</td>\n<td>おくりえりじめ</td>\n<td>Okuri-eri-jime</td>\n</tr>\n<tr>\n<td>6</td>\n<td>片羽绞</td>\n<td>かたはじめ</td>\n<td>Kata-ha-jime</td>\n</tr>\n<tr>\n<td>7</td>\n<td>胴绞</td>\n<td>どうじめ</td>\n<td>Do-jime</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td></td>\n</tr>\n<tr>\n<td>8</td>\n<td>袖车绞</td>\n<td>そでぐるまじめ</td>\n<td>Sode-guruma-jime</td>\n</tr>\n<tr>\n<td>9</td>\n<td>片手绞</td>\n<td>かたてじめ</td>\n<td>Kata-te-jime</td>\n</tr>\n<tr>\n<td>10</td>\n<td>両手绞</td>\n<td>りょうてじめ</td>\n<td>Ryo-te-jime</td>\n</tr>\n<tr>\n<td>11</td>\n<td>突込绞</td>\n<td>つっこみじめ</td>\n<td>Tsukkomi-jime</td>\n</tr>\n<tr>\n<td>12</td>\n<td>三角绞</td>\n<td>さんかくじめ</td>\n<td>Sankaku-jime</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"3-关节技-10本\"><a href=\"#3-关节技-10本\" class=\"headerlink\" title=\"3.关节技 (10本)\"></a>3.关节技 (10本)</h2><table>\n<thead>\n<tr>\n<th>序号</th>\n<th>名称</th>\n<th>日语</th>\n<th>英语</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>关节技</td>\n<td>かんせつわざ</td>\n<td>KANSETSU-WAZA</td>\n</tr>\n<tr>\n<td>1</td>\n<td>腕缄</td>\n<td>うでがらみ</td>\n<td>Ude-garami</td>\n</tr>\n<tr>\n<td>2</td>\n<td>腕挫十字固</td>\n<td>うでひしぎじゅうじがため</td>\n<td>Ude-hishigi-juji-gatame</td>\n</tr>\n<tr>\n<td>3</td>\n<td>腕挫腕固</td>\n<td>うでひしぎうでがため</td>\n<td>Ude-hishigi-ude-gatame</td>\n</tr>\n<tr>\n<td>4</td>\n<td>腕挫膝固</td>\n<td>うでひしぎひざがため</td>\n<td>Ude-hishigi-hiza-gatame</td>\n</tr>\n<tr>\n<td>5</td>\n<td>腕挫腋固</td>\n<td>うでひしぎわきがため</td>\n<td>Ude-hishigi-waki-gatame</td>\n</tr>\n<tr>\n<td>6</td>\n<td>腕挫腹固</td>\n<td>うてひしぎはらがため</td>\n<td>Ude-hishigi-hara-gatame</td>\n</tr>\n<tr>\n<td>7</td>\n<td>足缄</td>\n<td>あしがらみ</td>\n<td>Ashi-garami</td>\n</tr>\n<tr>\n<td></td>\n<td>*禁止技</td>\n<td>*禁止技</td>\n<td></td>\n</tr>\n<tr>\n<td>8</td>\n<td>腕挫脚固</td>\n<td>うでひしぎあしがため</td>\n<td>Ude-hishigi-ashi-gatame</td>\n</tr>\n<tr>\n<td>9</td>\n<td>腕挫手固</td>\n<td>うでひしぎてがため</td>\n<td>Ude-hishigi-te-gatame</td>\n</tr>\n<tr>\n<td>10</td>\n<td>腕挫三角固</td>\n<td>うてひしぎさんかくがため</td>\n<td>Ude-hishigi-sankaku-gatame</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"三-当身技-あてみわざ\"><a href=\"#三-当身技-あてみわざ\" class=\"headerlink\" title=\"三 当身技 (あてみわざ)\"></a>三 当身技 (あてみわざ)</h1><p>当身技是通过戳、打、踢等，直接攻击对手要害部位或生理弱点的技术。由于该技术危险性太大，容易对选手造成严重伤害，所以在柔道比赛和训练中被禁止使用。<br>具体分为两类：当所和急所。  </p>\n<h2 id=\"1-当所\"><a href=\"#1-当所\" class=\"headerlink\" title=\"1.当所\"></a>1.当所</h2><p>臂（うで）：<br>指先当（ゆびさきあて）：突出、両眼突<br>拳当（こぶしあて）：斜当、横当、上当、突上、下突、後突、後隅突、突掛、横打、後打、打下<br>手刀当（てがたなあて、手掌の小指側縁）：切下、斜打<br>肘当（ひじあて）：後当. </p>\n<p>脚（あし）：<br>膝頭当（ひざがしらあて）：前当<br>蹠頭当（せきとうあて、足蹠の前端）：斜蹴、前蹴、高蹴<br>踵当（かかとあて）：後蹴、横蹴. </p>\n<h2 id=\"2-急所\"><a href=\"#2-急所\" class=\"headerlink\" title=\"2.急所\"></a>2.急所</h2><p>天倒、霞、鳥兎、獨鈷、人中、三日月、松風、村雨、秘中、タン中、水月、雁下、明星、月影、電光、稲妻、臍下丹田、釣鐘（金的）、肘詰、伏兎、向骨。  </p>\n"},{"title":"Deep learning笔记3-RNN循环神经网络","lang":"zh","date":"2017-10-16T09:18:56.000Z","_content":"\n### 1. 循环神经网络（RNN）\n\n原图和公式说明来自：[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-1.jpg)</center> \n\n-------------------------------------\n\nx是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；\n\ns是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；\n\nU是输入层到隐藏层的权重矩阵；\n\no是一个向量，表示输出层的值；\n\nV是隐藏层到输出层的权重矩阵。\n\nW权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。\n\n#### 1.1. 循环神经网络的计算\n\n##### 1.1.1. 基本循环神经网络\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(输出层)\\\\\n\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(隐藏层)\\\\\n\\end{align}\n\n如果反复把隐藏层带入到输出层得到：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\\\\n&=Vf(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+W\\mathrm{s}_{t-2}))\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+W\\mathrm{s}_{t-3})))\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+Wf(U\\mathrm{x}_{t-3}+...))))\n\\end{align}\n\n##### 1.1.2. 双向循环神经网络\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-2.jpg)</center> \n\n-------------------------------------\n最终的输出值取决于和。其计算方法为：\n\n\\begin{align}\n\\mathrm{y}_2=g(VA_2+V'A_2')\n\\end{align}\n\n其中:\n\n\\begin{align}\nA_2&=f(WA_1+U\\mathrm{x}_2)\\\\\nA_2'&=f(W'A_3'+U'\\mathrm{x}_2)\\\\\n\\end{align}\n\n正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S't与S't+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t+V'\\mathrm{s}_t')\\\\\n\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'&=f(U'\\mathrm{x}_t+W'\\mathrm{s}_{t+1}')\\\\\n\\end{align}\n\n正向计算和反向计算不共享权重: U和U'、W和W'、V和V'都是不同的权重矩阵。\n\n##### 1.1.3. 深度循环神经网络\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-3.jpg)</center> \n\n-------------------------------------\n\n堆叠两个以上的隐藏层，就得到了深度循环神经网络。其计算方法为：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V^{(i)}\\mathrm{s}_t^{(i)}+V'^{(i)}\\mathrm{s}_t'^{(i)})\\\\\n\\mathrm{s}_t^{(i)}&=f(U^{(i)}\\mathrm{s}_t^{(i-1)}+W^{(i)}\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'^{(i)}&=f(U'^{(i)}\\mathrm{s}_t'^{(i-1)}+W'^{(i)}\\mathrm{s}_{t+1}')\\\\\n...\\\\\n\\mathrm{s}_t^{(1)}&=f(U^{(1)}\\mathrm{x}_t+W^{(1)}\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'^{(1)}&=f(U'^{(1)}\\mathrm{x}_t+W'^{(1)}\\mathrm{s}_{t+1}')\\\\\n\\end{align}\n\n#### 1.2. 循环神经网络的训练\n\n- 循环神经网络的训练算法：BPTT  \n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。\n\n- 前向计算\n\n\\begin{align}\n\\mathrm{s}_t=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\n\\end{align}\n\n- 误差项的计算\n\nBTPP算法将第l层t时刻的误差项值沿两个方向传播：\n\n① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：\n\n\\begin{align}\n(\\delta_t^{l-1})^T=&(\\delta_t^l)^TUdiag[f'^{l-1}(\\mathrm{net}_t^{l-1})]\n\\end{align}\n\n② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：\n\n\\begin{align}\n\\delta_k^T=&\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f'(\\mathrm{net}_{i})]\n\\end{align}\n\nRNN在训练中很容易发生梯度爆炸和梯度消失（取决于大于1还是小于1），这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。\n\n### 2. 长短时记忆网络（LSTM）\n\n原图和公式说明来自：- [零基础入门深度学习(6) - 长短时记忆网络](https://zybuluo.com/hanbingtao/note/581764 \"Title\") \n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-1.jpg)</center> \n\n-------------------------------------\n\n长短时记忆网络相对于普通循环神经网络，增加了一个状态c来保存长期的状态。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-2.jpg)</center> \n\n-------------------------------------\n\n#### 2.1. 长短时记忆网络的计算\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-3.jpg)</center> \n\n-------------------------------------\n\n\\begin{align}\ng(\\mathbf{x})=\\sigma(W\\mathbf{x}+\\mathbf{b})\n\\end{align}\n\n● 遗忘门（forget gate）决定了上一时刻的单元状态有多少保留到当前时刻：\n\n\\begin{align}\n\\mathbf{f}_t=\\sigma(W_f\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f)\n\\end{align}\n\n● 输入门（input gate）决定了当前时刻网络的输入有多少保存到单元状态：\n\n\\begin{align}\n\\mathbf{i}_t=\\sigma(W_i\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i)\n\\end{align}\n\n● 输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值：\n\n\\begin{align}\n\\mathbf{\\tilde{c}}_t=\\tanh(W_c\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c)\n\\end{align}\n\n推荐阅读 - [强的笔记-我对LSTM的理解](http://blog.qiangzhonghua.com/blog/tech/lstm#15-%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98---%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F \"Title\")\n\n-------------------------------------\n\n#### 2.2. 长短时记忆网络的训练\n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。\n\n第l层t时刻的误差项值沿两个方向传播：\n\n① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{\\mathbf{net}_t^{l-1}}}&=(\\delta_{f,t}^TW_{fx}+\\delta_{i,t}^TW_{ix}+\\delta_{\\tilde{c},t}^TW_{cx}+\\delta_{o,t}^TW_{ox})\\circ f'(\\mathbf{net}_t^{l-1})\n\\end{align}\n\n② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：\n\n\\begin{align}\n\\delta_k^T=\\prod_{j=k}^{t-1}\\delta_{o,j}^TW_{oh}\n+\\delta_{f,j}^TW_{fh}\n+\\delta_{i,j}^TW_{ih}\n+\\delta_{\\tilde{c},j}^TW_{ch}\n\\end{align}\n\n### 3. 基于TensorFlow的实现（RNN/LSTM with TF）\n\nTensorFlow中的RNN的抽象基类“ [RNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell \"Title\")”，是实现RNN的基本单元。\n\nGithub - [源码阅读](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py \"Title\")\n\n● call方法\n\n每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。\n\n每调用一次RNNCell的call方法，就相当于在时间上推进了一步。\n\n假设有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：\n  \n-------------------------------------\n\n<center>![RNN](/image/DL/3/3-1.jpg)</center> \n\n-------------------------------------\n\n再调用一次call(x2, h1)就可以得到(output2, h2)：\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/3-2.jpg)</center> \n\n-------------------------------------\n\n● 两个重要类变量\n\n隐藏层的大小：state_size\n\n输出层的大小：output_size\n\n● 常用的三个子类\n\n[BasicRNNCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell \"Title\")\n\n[BasicLSTMCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell \"Title\")\n\n[MultiRNNCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/MultiRNNCell \"Title\")\n\n#### 3.1. 单层LSTM例\n\n```python\n\nimport tensorflow as tf\nimport numpy as np\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\ninputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\nh0 = lstm_cell.zero_state(32, np.float32) # 得到一个全0的初始状态\noutput, h1 = lstm_cell.call(inputs, h0)\n\n# LSTM可以看做有两个隐状态h和c，对应的隐层就是一个Tuple元组\n# 每个都是(batch_size, state_size)的形状\nprint(h1.h)  # shape=(32, 128)\nprint(h1.c)  # shape=(32, 128)\n\n```\n#### 3.2. 一次执行多步\n\nTensorFlow提供了一个tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。\n\n```python\n# inputs: shape = (batch_size, time_steps, input_size) \n# cell: RNNCell\n# initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取0矩阵\noutputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n\n```\n\n官方 [tf.nn.dynamic_rnn文档](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn \"Title\")\n \n#### 3.3. 堆叠RNNCell\n\n即实现多层的RNN。将x输入第一层RNN的后得到隐层状态h，这个隐层状态就相当于第二层RNN的输入，第二层RNN的隐层状态又相当于第三层RNN的输入，以此类推。\n\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\n# 每调用一次这个函数就返回一个BasicRNNCell\ndef get_a_cell():\n    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\ncell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n# 得到的cell实际也是RNNCell的子类\n# 它的state_size是(128, 128, 128)\n# (128, 128, 128)并不是128x128x128的意思\n# 而是表示共有3个隐层状态，每个隐层状态的大小为128\nprint(cell.state_size) # (128, 128, 128)\n\n# 使用对应的call函数\ninputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\nh0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态\noutput, h1 = cell.call(inputs, h0)\n\nprint(h1) # tuple中含有3个32x128的向量\n\n```\nMultiRNNCell也是RNNCell的子类，因此也有call方法、state_size和output_size变量。同样可以通过tf.nn.dynamic_rnn来一次运行多步。\n\n- Attention 版本问题\n\n目前使用TensorFlow 1.2运行堆叠RNN的代码是：\n\n```python\n# 每调用一次这个函数就返回一个BasicRNNCell\ndef get_a_cell():\n    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\ncell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n\n```\n更早一些的版本中（比如TensorFlow 1.0.0），实现方式是：\n\n```python\none_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128)\ncell = tf.nn.rnn_cell.MultiRNNCell([one_cell] * 3) # 3层RNN\n\n```\n新版本按照这种的方式定义，就会引起报错。\n\n阅读参考 - [RNN入门：多层LSTM网络（四）](http://www.jianshu.com/p/b3c7883e3ddf \"Title\") \n\n#### 3.4. TF循环神经网络的实现例\n\nMake TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.\n\nRNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。\n\n程序实例 - [Github Link](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") \n\n### 4. 词分析框架（Word Framework）\n\n#### 4.1. word2vec\n\n单词嵌入模型（简称为  word2vec 模型）。将word映射成连续（高维）向量，这样通过训练，就可以把对文本内容的处理简化为K维向量空间中向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。\n\n一般来说， word2vec输出的词向量可以被用来做NLP相关的工作，比如聚类、找同义词、词性分析等等。\n\n有两种实现的架构，CBOW(Continuous Bag-Of-Words)模型以及Skip-gram模型：\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-1.png)</center> \n\n-------------------------------------\n\n阅读链接 - [word2vec入门基础](http://www.cnblogs.com/tina-smile/p/5176441.html \"Title\")\n\n\n#### 4.2. seq2seq\n\n序列到序列模型（简称为 seq2seq 模型）。seq2seq模型就像一个翻译模型,输入是一个序列(比如一个英文句子),输出也是一个序列(比如该英文句子所对应的法文翻译)。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-2.png)</center> \n\n-------------------------------------\n\n● 编码器：这是一个[tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn \"Title\")函数。\n\n● 解码器：这是一个[tf.contrib.seq2seq.dynamic_rnn_decoder](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder \"Title\")函数。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-3.jpg)</center> \n\n-------------------------------------\n\n阅读链接 - [Seq2Seq的DIY简介](http://www.jianshu.com/p/124b777e0c55 \"Title\")\n\n阅读链接 - [聊天机器人深度学习](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ \"Title\")\n\n阅读链接 - [tensorflow-seq2seq-tutorials ](https://github.com/ematvey/tensorflow-seq2seq-tutorials \"Title\")\n\n-------------------------------------\n\n● word2vec输出的是一个词向量，seq2seq输出的是一个词序列。\n\n推荐阅读 - [Deep Learning for Chatbots, Part 1](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ \"Title\")\n\n推荐阅读 - [《安娜卡列尼娜》文本生成](https://zhuanlan.zhihu.com/p/27087310 \"Title\")\n\n推荐阅读 - [LSTMで夏目漱石ぽい文章の生成](https://qiita.com/elm200/items/6f84d3a42eebe6c47caa \"Title\")\n\n### 程序实例（Program Example）\n\n- [Github Link - tv-script-generation](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") \n\n- [Github Link - language-translation](https://github.com/HJTSO/language-translation/blob/master/dlnd_language_translation.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \"Title\") \n\n- [TensorFlow中RNN实现](https://zhuanlan.zhihu.com/p/28196873 \"Title\") \n\n- [零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\n- [零基础入门深度学习(6) - 长短时记忆网络](https://zybuluo.com/hanbingtao/note/581764 \"Title\") \n\n- [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca \"Title\") \n\n- [わかるLSTM](https://qiita.com/t_Signull/items/21b82be280b46f467d1b \"Title\") \n","source":"_posts/zh/Deep learning笔记3-RNN循环神经网络.md","raw":"\n---\ntitle: Deep learning笔记3-RNN循环神经网络\nlang: zh\ndate: 2017-10-16 18:18:56\ntags: Deep Learning\n---\n\n### 1. 循环神经网络（RNN）\n\n原图和公式说明来自：[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-1.jpg)</center> \n\n-------------------------------------\n\nx是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；\n\ns是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；\n\nU是输入层到隐藏层的权重矩阵；\n\no是一个向量，表示输出层的值；\n\nV是隐藏层到输出层的权重矩阵。\n\nW权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。\n\n#### 1.1. 循环神经网络的计算\n\n##### 1.1.1. 基本循环神经网络\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(输出层)\\\\\n\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(隐藏层)\\\\\n\\end{align}\n\n如果反复把隐藏层带入到输出层得到：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\\\\n&=Vf(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+W\\mathrm{s}_{t-2}))\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+W\\mathrm{s}_{t-3})))\\\\\n&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+Wf(U\\mathrm{x}_{t-3}+...))))\n\\end{align}\n\n##### 1.1.2. 双向循环神经网络\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-2.jpg)</center> \n\n-------------------------------------\n最终的输出值取决于和。其计算方法为：\n\n\\begin{align}\n\\mathrm{y}_2=g(VA_2+V'A_2')\n\\end{align}\n\n其中:\n\n\\begin{align}\nA_2&=f(WA_1+U\\mathrm{x}_2)\\\\\nA_2'&=f(W'A_3'+U'\\mathrm{x}_2)\\\\\n\\end{align}\n\n正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S't与S't+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V\\mathrm{s}_t+V'\\mathrm{s}_t')\\\\\n\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'&=f(U'\\mathrm{x}_t+W'\\mathrm{s}_{t+1}')\\\\\n\\end{align}\n\n正向计算和反向计算不共享权重: U和U'、W和W'、V和V'都是不同的权重矩阵。\n\n##### 1.1.3. 深度循环神经网络\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/1-3.jpg)</center> \n\n-------------------------------------\n\n堆叠两个以上的隐藏层，就得到了深度循环神经网络。其计算方法为：\n\n\\begin{align}\n\\mathrm{o}_t&=g(V^{(i)}\\mathrm{s}_t^{(i)}+V'^{(i)}\\mathrm{s}_t'^{(i)})\\\\\n\\mathrm{s}_t^{(i)}&=f(U^{(i)}\\mathrm{s}_t^{(i-1)}+W^{(i)}\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'^{(i)}&=f(U'^{(i)}\\mathrm{s}_t'^{(i-1)}+W'^{(i)}\\mathrm{s}_{t+1}')\\\\\n...\\\\\n\\mathrm{s}_t^{(1)}&=f(U^{(1)}\\mathrm{x}_t+W^{(1)}\\mathrm{s}_{t-1})\\\\\n\\mathrm{s}_t'^{(1)}&=f(U'^{(1)}\\mathrm{x}_t+W'^{(1)}\\mathrm{s}_{t+1}')\\\\\n\\end{align}\n\n#### 1.2. 循环神经网络的训练\n\n- 循环神经网络的训练算法：BPTT  \n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。\n\n- 前向计算\n\n\\begin{align}\n\\mathrm{s}_t=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\n\\end{align}\n\n- 误差项的计算\n\nBTPP算法将第l层t时刻的误差项值沿两个方向传播：\n\n① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：\n\n\\begin{align}\n(\\delta_t^{l-1})^T=&(\\delta_t^l)^TUdiag[f'^{l-1}(\\mathrm{net}_t^{l-1})]\n\\end{align}\n\n② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：\n\n\\begin{align}\n\\delta_k^T=&\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f'(\\mathrm{net}_{i})]\n\\end{align}\n\nRNN在训练中很容易发生梯度爆炸和梯度消失（取决于大于1还是小于1），这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。\n\n### 2. 长短时记忆网络（LSTM）\n\n原图和公式说明来自：- [零基础入门深度学习(6) - 长短时记忆网络](https://zybuluo.com/hanbingtao/note/581764 \"Title\") \n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-1.jpg)</center> \n\n-------------------------------------\n\n长短时记忆网络相对于普通循环神经网络，增加了一个状态c来保存长期的状态。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-2.jpg)</center> \n\n-------------------------------------\n\n#### 2.1. 长短时记忆网络的计算\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/2-3.jpg)</center> \n\n-------------------------------------\n\n\\begin{align}\ng(\\mathbf{x})=\\sigma(W\\mathbf{x}+\\mathbf{b})\n\\end{align}\n\n● 遗忘门（forget gate）决定了上一时刻的单元状态有多少保留到当前时刻：\n\n\\begin{align}\n\\mathbf{f}_t=\\sigma(W_f\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f)\n\\end{align}\n\n● 输入门（input gate）决定了当前时刻网络的输入有多少保存到单元状态：\n\n\\begin{align}\n\\mathbf{i}_t=\\sigma(W_i\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i)\n\\end{align}\n\n● 输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值：\n\n\\begin{align}\n\\mathbf{\\tilde{c}}_t=\\tanh(W_c\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c)\n\\end{align}\n\n推荐阅读 - [强的笔记-我对LSTM的理解](http://blog.qiangzhonghua.com/blog/tech/lstm#15-%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98---%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F \"Title\")\n\n-------------------------------------\n\n#### 2.2. 长短时记忆网络的训练\n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。\n\n第l层t时刻的误差项值沿两个方向传播：\n\n① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：\n\n\\begin{align}\n\\frac{\\partial{E}}{\\partial{\\mathbf{net}_t^{l-1}}}&=(\\delta_{f,t}^TW_{fx}+\\delta_{i,t}^TW_{ix}+\\delta_{\\tilde{c},t}^TW_{cx}+\\delta_{o,t}^TW_{ox})\\circ f'(\\mathbf{net}_t^{l-1})\n\\end{align}\n\n② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：\n\n\\begin{align}\n\\delta_k^T=\\prod_{j=k}^{t-1}\\delta_{o,j}^TW_{oh}\n+\\delta_{f,j}^TW_{fh}\n+\\delta_{i,j}^TW_{ih}\n+\\delta_{\\tilde{c},j}^TW_{ch}\n\\end{align}\n\n### 3. 基于TensorFlow的实现（RNN/LSTM with TF）\n\nTensorFlow中的RNN的抽象基类“ [RNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell \"Title\")”，是实现RNN的基本单元。\n\nGithub - [源码阅读](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py \"Title\")\n\n● call方法\n\n每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。\n\n每调用一次RNNCell的call方法，就相当于在时间上推进了一步。\n\n假设有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：\n  \n-------------------------------------\n\n<center>![RNN](/image/DL/3/3-1.jpg)</center> \n\n-------------------------------------\n\n再调用一次call(x2, h1)就可以得到(output2, h2)：\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/3-2.jpg)</center> \n\n-------------------------------------\n\n● 两个重要类变量\n\n隐藏层的大小：state_size\n\n输出层的大小：output_size\n\n● 常用的三个子类\n\n[BasicRNNCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell \"Title\")\n\n[BasicLSTMCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell \"Title\")\n\n[MultiRNNCell](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/MultiRNNCell \"Title\")\n\n#### 3.1. 单层LSTM例\n\n```python\n\nimport tensorflow as tf\nimport numpy as np\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\ninputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\nh0 = lstm_cell.zero_state(32, np.float32) # 得到一个全0的初始状态\noutput, h1 = lstm_cell.call(inputs, h0)\n\n# LSTM可以看做有两个隐状态h和c，对应的隐层就是一个Tuple元组\n# 每个都是(batch_size, state_size)的形状\nprint(h1.h)  # shape=(32, 128)\nprint(h1.c)  # shape=(32, 128)\n\n```\n#### 3.2. 一次执行多步\n\nTensorFlow提供了一个tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。\n\n```python\n# inputs: shape = (batch_size, time_steps, input_size) \n# cell: RNNCell\n# initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取0矩阵\noutputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n\n```\n\n官方 [tf.nn.dynamic_rnn文档](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn \"Title\")\n \n#### 3.3. 堆叠RNNCell\n\n即实现多层的RNN。将x输入第一层RNN的后得到隐层状态h，这个隐层状态就相当于第二层RNN的输入，第二层RNN的隐层状态又相当于第三层RNN的输入，以此类推。\n\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\n# 每调用一次这个函数就返回一个BasicRNNCell\ndef get_a_cell():\n    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\ncell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n# 得到的cell实际也是RNNCell的子类\n# 它的state_size是(128, 128, 128)\n# (128, 128, 128)并不是128x128x128的意思\n# 而是表示共有3个隐层状态，每个隐层状态的大小为128\nprint(cell.state_size) # (128, 128, 128)\n\n# 使用对应的call函数\ninputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\nh0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态\noutput, h1 = cell.call(inputs, h0)\n\nprint(h1) # tuple中含有3个32x128的向量\n\n```\nMultiRNNCell也是RNNCell的子类，因此也有call方法、state_size和output_size变量。同样可以通过tf.nn.dynamic_rnn来一次运行多步。\n\n- Attention 版本问题\n\n目前使用TensorFlow 1.2运行堆叠RNN的代码是：\n\n```python\n# 每调用一次这个函数就返回一个BasicRNNCell\ndef get_a_cell():\n    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\ncell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n\n```\n更早一些的版本中（比如TensorFlow 1.0.0），实现方式是：\n\n```python\none_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128)\ncell = tf.nn.rnn_cell.MultiRNNCell([one_cell] * 3) # 3层RNN\n\n```\n新版本按照这种的方式定义，就会引起报错。\n\n阅读参考 - [RNN入门：多层LSTM网络（四）](http://www.jianshu.com/p/b3c7883e3ddf \"Title\") \n\n#### 3.4. TF循环神经网络的实现例\n\nMake TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.\n\nRNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。\n\n程序实例 - [Github Link](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") \n\n### 4. 词分析框架（Word Framework）\n\n#### 4.1. word2vec\n\n单词嵌入模型（简称为  word2vec 模型）。将word映射成连续（高维）向量，这样通过训练，就可以把对文本内容的处理简化为K维向量空间中向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。\n\n一般来说， word2vec输出的词向量可以被用来做NLP相关的工作，比如聚类、找同义词、词性分析等等。\n\n有两种实现的架构，CBOW(Continuous Bag-Of-Words)模型以及Skip-gram模型：\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-1.png)</center> \n\n-------------------------------------\n\n阅读链接 - [word2vec入门基础](http://www.cnblogs.com/tina-smile/p/5176441.html \"Title\")\n\n\n#### 4.2. seq2seq\n\n序列到序列模型（简称为 seq2seq 模型）。seq2seq模型就像一个翻译模型,输入是一个序列(比如一个英文句子),输出也是一个序列(比如该英文句子所对应的法文翻译)。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-2.png)</center> \n\n-------------------------------------\n\n● 编码器：这是一个[tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn \"Title\")函数。\n\n● 解码器：这是一个[tf.contrib.seq2seq.dynamic_rnn_decoder](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder \"Title\")函数。\n\n-------------------------------------\n\n<center>![RNN](/image/DL/3/4-3.jpg)</center> \n\n-------------------------------------\n\n阅读链接 - [Seq2Seq的DIY简介](http://www.jianshu.com/p/124b777e0c55 \"Title\")\n\n阅读链接 - [聊天机器人深度学习](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ \"Title\")\n\n阅读链接 - [tensorflow-seq2seq-tutorials ](https://github.com/ematvey/tensorflow-seq2seq-tutorials \"Title\")\n\n-------------------------------------\n\n● word2vec输出的是一个词向量，seq2seq输出的是一个词序列。\n\n推荐阅读 - [Deep Learning for Chatbots, Part 1](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/ \"Title\")\n\n推荐阅读 - [《安娜卡列尼娜》文本生成](https://zhuanlan.zhihu.com/p/27087310 \"Title\")\n\n推荐阅读 - [LSTMで夏目漱石ぽい文章の生成](https://qiita.com/elm200/items/6f84d3a42eebe6c47caa \"Title\")\n\n### 程序实例（Program Example）\n\n- [Github Link - tv-script-generation](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") \n\n- [Github Link - language-translation](https://github.com/HJTSO/language-translation/blob/master/dlnd_language_translation.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \"Title\") \n\n- [TensorFlow中RNN实现](https://zhuanlan.zhihu.com/p/28196873 \"Title\") \n\n- [零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n\n- [零基础入门深度学习(6) - 长短时记忆网络](https://zybuluo.com/hanbingtao/note/581764 \"Title\") \n\n- [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca \"Title\") \n\n- [わかるLSTM](https://qiita.com/t_Signull/items/21b82be280b46f467d1b \"Title\") \n","slug":"zh-Deep-learning笔记3-RNN循环神经网络","published":1,"updated":"2018-01-01T13:12:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zf0023og648fvmxnax","content":"<h3 id=\"1-循环神经网络（RNN）\"><a href=\"#1-循环神经网络（RNN）\" class=\"headerlink\" title=\"1. 循环神经网络（RNN）\"></a>1. 循环神经网络（RNN）</h3><p>原图和公式说明来自：<a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/3/1-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；</p>\n<p>s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</p>\n<p>U是输入层到隐藏层的权重矩阵；</p>\n<p>o是一个向量，表示输出层的值；</p>\n<p>V是隐藏层到输出层的权重矩阵。</p>\n<p>W权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。</p>\n<h4 id=\"1-1-循环神经网络的计算\"><a href=\"#1-1-循环神经网络的计算\" class=\"headerlink\" title=\"1.1. 循环神经网络的计算\"></a>1.1. 循环神经网络的计算</h4><h5 id=\"1-1-1-基本循环神经网络\"><a href=\"#1-1-1-基本循环神经网络\" class=\"headerlink\" title=\"1.1.1. 基本循环神经网络\"></a>1.1.1. 基本循环神经网络</h5><p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(输出层)\\\\<br>\\mathrm{s}_t&amp;=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(隐藏层)\\\\<br>\\end{align}</p>\n<p>如果反复把隐藏层带入到输出层得到：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t)\\\\<br>&amp;=Vf(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+W\\mathrm{s}_{t-2}))\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+W\\mathrm{s}_{t-3})))\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+Wf(U\\mathrm{x}_{t-3}+…))))<br>\\end{align}</p>\n<h5 id=\"1-1-2-双向循环神经网络\"><a href=\"#1-1-2-双向循环神经网络\" class=\"headerlink\" title=\"1.1.2. 双向循环神经网络\"></a>1.1.2. 双向循环神经网络</h5><hr>\n<center><img src=\"/image/DL/3/1-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>最终的输出值取决于和。其计算方法为：</p>\n<p>\\begin{align}<br>\\mathrm{y}_2=g(VA_2+V’A_2’)<br>\\end{align}</p>\n<p>其中:</p>\n<p>\\begin{align}<br>A_2&amp;=f(WA_1+U\\mathrm{x}_2)\\\\<br>A_2’&amp;=f(W’A_3’+U’\\mathrm{x}_2)\\\\<br>\\end{align}</p>\n<p>正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S’t与S’t+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t+V’\\mathrm{s}_t’)\\\\<br>\\mathrm{s}_t&amp;=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’&amp;=f(U’\\mathrm{x}_t+W’\\mathrm{s}_{t+1}’)\\\\<br>\\end{align}</p>\n<p>正向计算和反向计算不共享权重: U和U’、W和W’、V和V’都是不同的权重矩阵。</p>\n<h5 id=\"1-1-3-深度循环神经网络\"><a href=\"#1-1-3-深度循环神经网络\" class=\"headerlink\" title=\"1.1.3. 深度循环神经网络\"></a>1.1.3. 深度循环神经网络</h5><hr>\n<center><img src=\"/image/DL/3/1-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>堆叠两个以上的隐藏层，就得到了深度循环神经网络。其计算方法为：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V^{(i)}\\mathrm{s}_t^{(i)}+V’^{(i)}\\mathrm{s}_t’^{(i)})\\\\<br>\\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\\mathrm{s}_t^{(i-1)}+W^{(i)}\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\\mathrm{s}_t’^{(i-1)}+W’^{(i)}\\mathrm{s}_{t+1}’)\\\\<br>…\\\\<br>\\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\\mathrm{x}_t+W^{(1)}\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\\mathrm{x}_t+W’^{(1)}\\mathrm{s}_{t+1}’)\\\\<br>\\end{align}</p>\n<h4 id=\"1-2-循环神经网络的训练\"><a href=\"#1-2-循环神经网络的训练\" class=\"headerlink\" title=\"1.2. 循环神经网络的训练\"></a>1.2. 循环神经网络的训练</h4><ul>\n<li>循环神经网络的训练算法：BPTT  </li>\n</ul>\n<p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>\n<ul>\n<li>前向计算</li>\n</ul>\n<p>\\begin{align}<br>\\mathrm{s}_t=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})<br>\\end{align}</p>\n<ul>\n<li>误差项的计算</li>\n</ul>\n<p>BTPP算法将第l层t时刻的误差项值沿两个方向传播：</p>\n<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>\n<p>\\begin{align}<br>(\\delta_t^{l-1})^T=&amp;(\\delta_t^l)^TUdiag[f’^{l-1}(\\mathrm{net}_t^{l-1})]<br>\\end{align}</p>\n<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>\n<p>\\begin{align}<br>\\delta_k^T=&amp;\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f’(\\mathrm{net}_{i})]<br>\\end{align}</p>\n<p>RNN在训练中很容易发生梯度爆炸和梯度消失（取决于大于1还是小于1），这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>\n<h3 id=\"2-长短时记忆网络（LSTM）\"><a href=\"#2-长短时记忆网络（LSTM）\" class=\"headerlink\" title=\"2. 长短时记忆网络（LSTM）\"></a>2. 长短时记忆网络（LSTM）</h3><p>原图和公式说明来自：- <a href=\"https://zybuluo.com/hanbingtao/note/581764\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(6) - 长短时记忆网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/3/2-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>长短时记忆网络相对于普通循环神经网络，增加了一个状态c来保存长期的状态。</p>\n<hr>\n<center><img src=\"/image/DL/3/2-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<h4 id=\"2-1-长短时记忆网络的计算\"><a href=\"#2-1-长短时记忆网络的计算\" class=\"headerlink\" title=\"2.1. 长短时记忆网络的计算\"></a>2.1. 长短时记忆网络的计算</h4><hr>\n<center><img src=\"/image/DL/3/2-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>\\begin{align}<br>g(\\mathbf{x})=\\sigma(W\\mathbf{x}+\\mathbf{b})<br>\\end{align}</p>\n<p>● 遗忘门（forget gate）决定了上一时刻的单元状态有多少保留到当前时刻：</p>\n<p>\\begin{align}<br>\\mathbf{f}_t=\\sigma(W_f\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f)<br>\\end{align}</p>\n<p>● 输入门（input gate）决定了当前时刻网络的输入有多少保存到单元状态：</p>\n<p>\\begin{align}<br>\\mathbf{i}_t=\\sigma(W_i\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i)<br>\\end{align}</p>\n<p>● 输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值：</p>\n<p>\\begin{align}<br>\\mathbf{\\tilde{c}}_t=\\tanh(W_c\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c)<br>\\end{align}</p>\n<p>推荐阅读 - <a href=\"http://blog.qiangzhonghua.com/blog/tech/lstm#15-%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98---%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F\" title=\"Title\" target=\"_blank\" rel=\"external\">强的笔记-我对LSTM的理解</a></p>\n<hr>\n<h4 id=\"2-2-长短时记忆网络的训练\"><a href=\"#2-2-长短时记忆网络的训练\" class=\"headerlink\" title=\"2.2. 长短时记忆网络的训练\"></a>2.2. 长短时记忆网络的训练</h4><p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>\n<p>第l层t时刻的误差项值沿两个方向传播：</p>\n<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{\\mathbf{net}_t^{l-1}}}&amp;=(\\delta_{f,t}^TW_{fx}+\\delta_{i,t}^TW_{ix}+\\delta_{\\tilde{c},t}^TW_{cx}+\\delta_{o,t}^TW_{ox})\\circ f’(\\mathbf{net}_t^{l-1})<br>\\end{align}</p>\n<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>\n<p>\\begin{align}<br>\\delta_k^T=\\prod_{j=k}^{t-1}\\delta_{o,j}^TW_{oh}<br>+\\delta_{f,j}^TW_{fh}<br>+\\delta_{i,j}^TW_{ih}<br>+\\delta_{\\tilde{c},j}^TW_{ch}<br>\\end{align}</p>\n<h3 id=\"3-基于TensorFlow的实现（RNN-LSTM-with-TF）\"><a href=\"#3-基于TensorFlow的实现（RNN-LSTM-with-TF）\" class=\"headerlink\" title=\"3. 基于TensorFlow的实现（RNN/LSTM with TF）\"></a>3. 基于TensorFlow的实现（RNN/LSTM with TF）</h3><p>TensorFlow中的RNN的抽象基类“ <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell\" title=\"Title\" target=\"_blank\" rel=\"external\">RNNCell</a>”，是实现RNN的基本单元。</p>\n<p>Github - <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py\" title=\"Title\" target=\"_blank\" rel=\"external\">源码阅读</a></p>\n<p>● call方法</p>\n<p>每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。</p>\n<p>每调用一次RNNCell的call方法，就相当于在时间上推进了一步。</p>\n<p>假设有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：</p>\n<hr>\n<center><img src=\"/image/DL/3/3-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>再调用一次call(x2, h1)就可以得到(output2, h2)：</p>\n<hr>\n<center><img src=\"/image/DL/3/3-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>● 两个重要类变量</p>\n<p>隐藏层的大小：state_size</p>\n<p>输出层的大小：output_size</p>\n<p>● 常用的三个子类</p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell\" title=\"Title\" target=\"_blank\" rel=\"external\">BasicRNNCell</a></p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell\" title=\"Title\" target=\"_blank\" rel=\"external\">BasicLSTMCell</a></p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/MultiRNNCell\" title=\"Title\" target=\"_blank\" rel=\"external\">MultiRNNCell</a></p>\n<h4 id=\"3-1-单层LSTM例\"><a href=\"#3-1-单层LSTM例\" class=\"headerlink\" title=\"3.1. 单层LSTM例\"></a>3.1. 单层LSTM例</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\">inputs = tf.placeholder(np.float32, shape=(<span class=\"number\">32</span>, <span class=\"number\">100</span>)) <span class=\"comment\"># 32 是 batch_size</span></div><div class=\"line\">h0 = lstm_cell.zero_state(<span class=\"number\">32</span>, np.float32) <span class=\"comment\"># 得到一个全0的初始状态</span></div><div class=\"line\">output, h1 = lstm_cell.call(inputs, h0)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># LSTM可以看做有两个隐状态h和c，对应的隐层就是一个Tuple元组</span></div><div class=\"line\"><span class=\"comment\"># 每个都是(batch_size, state_size)的形状</span></div><div class=\"line\">print(h1.h)  <span class=\"comment\"># shape=(32, 128)</span></div><div class=\"line\">print(h1.c)  <span class=\"comment\"># shape=(32, 128)</span></div></pre></td></tr></table></figure>\n<h4 id=\"3-2-一次执行多步\"><a href=\"#3-2-一次执行多步\" class=\"headerlink\" title=\"3.2. 一次执行多步\"></a>3.2. 一次执行多步</h4><p>TensorFlow提供了一个tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># inputs: shape = (batch_size, time_steps, input_size) </span></div><div class=\"line\"><span class=\"comment\"># cell: RNNCell</span></div><div class=\"line\"><span class=\"comment\"># initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取0矩阵</span></div><div class=\"line\">outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)</div></pre></td></tr></table></figure>\n<p>官方 <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\" title=\"Title\" target=\"_blank\" rel=\"external\">tf.nn.dynamic_rnn文档</a></p>\n<h4 id=\"3-3-堆叠RNNCell\"><a href=\"#3-3-堆叠RNNCell\" class=\"headerlink\" title=\"3.3. 堆叠RNNCell\"></a>3.3. 堆叠RNNCell</h4><p>即实现多层的RNN。将x输入第一层RNN的后得到隐层状态h，这个隐层状态就相当于第二层RNN的输入，第二层RNN的隐层状态又相当于第三层RNN的输入，以此类推。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_a_cell</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\"><span class=\"comment\"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>)]) <span class=\"comment\"># 3层RNN</span></div><div class=\"line\"><span class=\"comment\"># 得到的cell实际也是RNNCell的子类</span></div><div class=\"line\"><span class=\"comment\"># 它的state_size是(128, 128, 128)</span></div><div class=\"line\"><span class=\"comment\"># (128, 128, 128)并不是128x128x128的意思</span></div><div class=\"line\"><span class=\"comment\"># 而是表示共有3个隐层状态，每个隐层状态的大小为128</span></div><div class=\"line\">print(cell.state_size) <span class=\"comment\"># (128, 128, 128)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 使用对应的call函数</span></div><div class=\"line\">inputs = tf.placeholder(np.float32, shape=(<span class=\"number\">32</span>, <span class=\"number\">100</span>)) <span class=\"comment\"># 32 是 batch_size</span></div><div class=\"line\">h0 = cell.zero_state(<span class=\"number\">32</span>, np.float32) <span class=\"comment\"># 通过zero_state得到一个全0的初始状态</span></div><div class=\"line\">output, h1 = cell.call(inputs, h0)</div><div class=\"line\"></div><div class=\"line\">print(h1) <span class=\"comment\"># tuple中含有3个32x128的向量</span></div></pre></td></tr></table></figure>\n<p>MultiRNNCell也是RNNCell的子类，因此也有call方法、state_size和output_size变量。同样可以通过tf.nn.dynamic_rnn来一次运行多步。</p>\n<ul>\n<li>Attention 版本问题</li>\n</ul>\n<p>目前使用TensorFlow 1.2运行堆叠RNN的代码是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_a_cell</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\"><span class=\"comment\"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>)]) <span class=\"comment\"># 3层RNN</span></div></pre></td></tr></table></figure>\n<p>更早一些的版本中（比如TensorFlow 1.0.0），实现方式是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">one_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([one_cell] * <span class=\"number\">3</span>) <span class=\"comment\"># 3层RNN</span></div></pre></td></tr></table></figure>\n<p>新版本按照这种的方式定义，就会引起报错。</p>\n<p>阅读参考 - <a href=\"http://www.jianshu.com/p/b3c7883e3ddf\" title=\"Title\" target=\"_blank\" rel=\"external\">RNN入门：多层LSTM网络（四）</a> </p>\n<h4 id=\"3-4-TF循环神经网络的实现例\"><a href=\"#3-4-TF循环神经网络的实现例\" class=\"headerlink\" title=\"3.4. TF循环神经网络的实现例\"></a>3.4. TF循环神经网络的实现例</h4><p>Make TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.</p>\n<p>RNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。</p>\n<p>程序实例 - <a href=\"https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </p>\n<h3 id=\"4-词分析框架（Word-Framework）\"><a href=\"#4-词分析框架（Word-Framework）\" class=\"headerlink\" title=\"4. 词分析框架（Word Framework）\"></a>4. 词分析框架（Word Framework）</h3><h4 id=\"4-1-word2vec\"><a href=\"#4-1-word2vec\" class=\"headerlink\" title=\"4.1. word2vec\"></a>4.1. word2vec</h4><p>单词嵌入模型（简称为  word2vec 模型）。将word映射成连续（高维）向量，这样通过训练，就可以把对文本内容的处理简化为K维向量空间中向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。</p>\n<p>一般来说， word2vec输出的词向量可以被用来做NLP相关的工作，比如聚类、找同义词、词性分析等等。</p>\n<p>有两种实现的架构，CBOW(Continuous Bag-Of-Words)模型以及Skip-gram模型：</p>\n<hr>\n<center><img src=\"/image/DL/3/4-1.png\" alt=\"RNN\"></center> \n\n<hr>\n<p>阅读链接 - <a href=\"http://www.cnblogs.com/tina-smile/p/5176441.html\" title=\"Title\" target=\"_blank\" rel=\"external\">word2vec入门基础</a></p>\n<h4 id=\"4-2-seq2seq\"><a href=\"#4-2-seq2seq\" class=\"headerlink\" title=\"4.2. seq2seq\"></a>4.2. seq2seq</h4><p>序列到序列模型（简称为 seq2seq 模型）。seq2seq模型就像一个翻译模型,输入是一个序列(比如一个英文句子),输出也是一个序列(比如该英文句子所对应的法文翻译)。</p>\n<hr>\n<center><img src=\"/image/DL/3/4-2.png\" alt=\"RNN\"></center> \n\n<hr>\n<p>● 编码器：这是一个<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\" title=\"Title\" target=\"_blank\" rel=\"external\">tf.nn.dynamic_rnn</a>函数。</p>\n<p>● 解码器：这是一个<a href=\"https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder\" title=\"Title\" target=\"_blank\" rel=\"external\">tf.contrib.seq2seq.dynamic_rnn_decoder</a>函数。</p>\n<hr>\n<center><img src=\"/image/DL/3/4-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>阅读链接 - <a href=\"http://www.jianshu.com/p/124b777e0c55\" title=\"Title\" target=\"_blank\" rel=\"external\">Seq2Seq的DIY简介</a></p>\n<p>阅读链接 - <a href=\"http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\" title=\"Title\" target=\"_blank\" rel=\"external\">聊天机器人深度学习</a></p>\n<p>阅读链接 - <a href=\"https://github.com/ematvey/tensorflow-seq2seq-tutorials\" title=\"Title\" target=\"_blank\" rel=\"external\">tensorflow-seq2seq-tutorials </a></p>\n<hr>\n<p>● word2vec输出的是一个词向量，seq2seq输出的是一个词序列。</p>\n<p>推荐阅读 - <a href=\"http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\" title=\"Title\" target=\"_blank\" rel=\"external\">Deep Learning for Chatbots, Part 1</a></p>\n<p>推荐阅读 - <a href=\"https://zhuanlan.zhihu.com/p/27087310\" title=\"Title\" target=\"_blank\" rel=\"external\">《安娜卡列尼娜》文本生成</a></p>\n<p>推荐阅读 - <a href=\"https://qiita.com/elm200/items/6f84d3a42eebe6c47caa\" title=\"Title\" target=\"_blank\" rel=\"external\">LSTMで夏目漱石ぽい文章の生成</a></p>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><p><a href=\"https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link - tv-script-generation</a> </p>\n</li>\n<li><p><a href=\"https://github.com/HJTSO/language-translation/blob/master/dlnd_language_translation.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link - language-translation</a> </p>\n</li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" title=\"Title\" target=\"_blank\" rel=\"external\">Understanding LSTM Networks</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/28196873\" title=\"Title\" target=\"_blank\" rel=\"external\">TensorFlow中RNN实现</a> </p>\n</li>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n</li>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/581764\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(6) - 长短时记忆网络</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca\" title=\"Title\" target=\"_blank\" rel=\"external\">LSTMネットワークの概要</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/t_Signull/items/21b82be280b46f467d1b\" title=\"Title\" target=\"_blank\" rel=\"external\">わかるLSTM</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-循环神经网络（RNN）\"><a href=\"#1-循环神经网络（RNN）\" class=\"headerlink\" title=\"1. 循环神经网络（RNN）\"></a>1. 循环神经网络（RNN）</h3><p>原图和公式说明来自：<a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/3/1-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；</p>\n<p>s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</p>\n<p>U是输入层到隐藏层的权重矩阵；</p>\n<p>o是一个向量，表示输出层的值；</p>\n<p>V是隐藏层到输出层的权重矩阵。</p>\n<p>W权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。</p>\n<h4 id=\"1-1-循环神经网络的计算\"><a href=\"#1-1-循环神经网络的计算\" class=\"headerlink\" title=\"1.1. 循环神经网络的计算\"></a>1.1. 循环神经网络的计算</h4><h5 id=\"1-1-1-基本循环神经网络\"><a href=\"#1-1-1-基本循环神经网络\" class=\"headerlink\" title=\"1.1.1. 基本循环神经网络\"></a>1.1.1. 基本循环神经网络</h5><p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(输出层)\\\\<br>\\mathrm{s}_t&amp;=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(隐藏层)\\\\<br>\\end{align}</p>\n<p>如果反复把隐藏层带入到输出层得到：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t)\\\\<br>&amp;=Vf(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+W\\mathrm{s}_{t-2}))\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+W\\mathrm{s}_{t-3})))\\\\<br>&amp;=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+Wf(U\\mathrm{x}_{t-3}+…))))<br>\\end{align}</p>\n<h5 id=\"1-1-2-双向循环神经网络\"><a href=\"#1-1-2-双向循环神经网络\" class=\"headerlink\" title=\"1.1.2. 双向循环神经网络\"></a>1.1.2. 双向循环神经网络</h5><hr>\n<center><img src=\"/image/DL/3/1-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>最终的输出值取决于和。其计算方法为：</p>\n<p>\\begin{align}<br>\\mathrm{y}_2=g(VA_2+V’A_2’)<br>\\end{align}</p>\n<p>其中:</p>\n<p>\\begin{align}<br>A_2&amp;=f(WA_1+U\\mathrm{x}_2)\\\\<br>A_2’&amp;=f(W’A_3’+U’\\mathrm{x}_2)\\\\<br>\\end{align}</p>\n<p>正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S’t与S’t+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V\\mathrm{s}_t+V’\\mathrm{s}_t’)\\\\<br>\\mathrm{s}_t&amp;=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’&amp;=f(U’\\mathrm{x}_t+W’\\mathrm{s}_{t+1}’)\\\\<br>\\end{align}</p>\n<p>正向计算和反向计算不共享权重: U和U’、W和W’、V和V’都是不同的权重矩阵。</p>\n<h5 id=\"1-1-3-深度循环神经网络\"><a href=\"#1-1-3-深度循环神经网络\" class=\"headerlink\" title=\"1.1.3. 深度循环神经网络\"></a>1.1.3. 深度循环神经网络</h5><hr>\n<center><img src=\"/image/DL/3/1-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>堆叠两个以上的隐藏层，就得到了深度循环神经网络。其计算方法为：</p>\n<p>\\begin{align}<br>\\mathrm{o}_t&amp;=g(V^{(i)}\\mathrm{s}_t^{(i)}+V’^{(i)}\\mathrm{s}_t’^{(i)})\\\\<br>\\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\\mathrm{s}_t^{(i-1)}+W^{(i)}\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\\mathrm{s}_t’^{(i-1)}+W’^{(i)}\\mathrm{s}_{t+1}’)\\\\<br>…\\\\<br>\\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\\mathrm{x}_t+W^{(1)}\\mathrm{s}_{t-1})\\\\<br>\\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\\mathrm{x}_t+W’^{(1)}\\mathrm{s}_{t+1}’)\\\\<br>\\end{align}</p>\n<h4 id=\"1-2-循环神经网络的训练\"><a href=\"#1-2-循环神经网络的训练\" class=\"headerlink\" title=\"1.2. 循环神经网络的训练\"></a>1.2. 循环神经网络的训练</h4><ul>\n<li>循环神经网络的训练算法：BPTT  </li>\n</ul>\n<p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>\n<ul>\n<li>前向计算</li>\n</ul>\n<p>\\begin{align}<br>\\mathrm{s}_t=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})<br>\\end{align}</p>\n<ul>\n<li>误差项的计算</li>\n</ul>\n<p>BTPP算法将第l层t时刻的误差项值沿两个方向传播：</p>\n<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>\n<p>\\begin{align}<br>(\\delta_t^{l-1})^T=&amp;(\\delta_t^l)^TUdiag[f’^{l-1}(\\mathrm{net}_t^{l-1})]<br>\\end{align}</p>\n<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>\n<p>\\begin{align}<br>\\delta_k^T=&amp;\\delta_t^T\\prod_{i=k}^{t-1}Wdiag[f’(\\mathrm{net}_{i})]<br>\\end{align}</p>\n<p>RNN在训练中很容易发生梯度爆炸和梯度消失（取决于大于1还是小于1），这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>\n<h3 id=\"2-长短时记忆网络（LSTM）\"><a href=\"#2-长短时记忆网络（LSTM）\" class=\"headerlink\" title=\"2. 长短时记忆网络（LSTM）\"></a>2. 长短时记忆网络（LSTM）</h3><p>原图和公式说明来自：- <a href=\"https://zybuluo.com/hanbingtao/note/581764\" title=\"Title\">零基础入门深度学习(6) - 长短时记忆网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/3/2-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>长短时记忆网络相对于普通循环神经网络，增加了一个状态c来保存长期的状态。</p>\n<hr>\n<center><img src=\"/image/DL/3/2-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<h4 id=\"2-1-长短时记忆网络的计算\"><a href=\"#2-1-长短时记忆网络的计算\" class=\"headerlink\" title=\"2.1. 长短时记忆网络的计算\"></a>2.1. 长短时记忆网络的计算</h4><hr>\n<center><img src=\"/image/DL/3/2-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>\\begin{align}<br>g(\\mathbf{x})=\\sigma(W\\mathbf{x}+\\mathbf{b})<br>\\end{align}</p>\n<p>● 遗忘门（forget gate）决定了上一时刻的单元状态有多少保留到当前时刻：</p>\n<p>\\begin{align}<br>\\mathbf{f}_t=\\sigma(W_f\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_f)<br>\\end{align}</p>\n<p>● 输入门（input gate）决定了当前时刻网络的输入有多少保存到单元状态：</p>\n<p>\\begin{align}<br>\\mathbf{i}_t=\\sigma(W_i\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_i)<br>\\end{align}</p>\n<p>● 输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值：</p>\n<p>\\begin{align}<br>\\mathbf{\\tilde{c}}_t=\\tanh(W_c\\cdot[\\mathbf{h}_{t-1},\\mathbf{x}_t]+\\mathbf{b}_c)<br>\\end{align}</p>\n<p>推荐阅读 - <a href=\"http://blog.qiangzhonghua.com/blog/tech/lstm#15-%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98---%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F\" title=\"Title\">强的笔记-我对LSTM的理解</a></p>\n<hr>\n<h4 id=\"2-2-长短时记忆网络的训练\"><a href=\"#2-2-长短时记忆网络的训练\" class=\"headerlink\" title=\"2.2. 长短时记忆网络的训练\"></a>2.2. 长短时记忆网络的训练</h4><p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>\n<p>第l层t时刻的误差项值沿两个方向传播：</p>\n<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>\n<p>\\begin{align}<br>\\frac{\\partial{E}}{\\partial{\\mathbf{net}_t^{l-1}}}&amp;=(\\delta_{f,t}^TW_{fx}+\\delta_{i,t}^TW_{ix}+\\delta_{\\tilde{c},t}^TW_{cx}+\\delta_{o,t}^TW_{ox})\\circ f’(\\mathbf{net}_t^{l-1})<br>\\end{align}</p>\n<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>\n<p>\\begin{align}<br>\\delta_k^T=\\prod_{j=k}^{t-1}\\delta_{o,j}^TW_{oh}<br>+\\delta_{f,j}^TW_{fh}<br>+\\delta_{i,j}^TW_{ih}<br>+\\delta_{\\tilde{c},j}^TW_{ch}<br>\\end{align}</p>\n<h3 id=\"3-基于TensorFlow的实现（RNN-LSTM-with-TF）\"><a href=\"#3-基于TensorFlow的实现（RNN-LSTM-with-TF）\" class=\"headerlink\" title=\"3. 基于TensorFlow的实现（RNN/LSTM with TF）\"></a>3. 基于TensorFlow的实现（RNN/LSTM with TF）</h3><p>TensorFlow中的RNN的抽象基类“ <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell\" title=\"Title\">RNNCell</a>”，是实现RNN的基本单元。</p>\n<p>Github - <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py\" title=\"Title\">源码阅读</a></p>\n<p>● call方法</p>\n<p>每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。</p>\n<p>每调用一次RNNCell的call方法，就相当于在时间上推进了一步。</p>\n<p>假设有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：</p>\n<hr>\n<center><img src=\"/image/DL/3/3-1.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>再调用一次call(x2, h1)就可以得到(output2, h2)：</p>\n<hr>\n<center><img src=\"/image/DL/3/3-2.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>● 两个重要类变量</p>\n<p>隐藏层的大小：state_size</p>\n<p>输出层的大小：output_size</p>\n<p>● 常用的三个子类</p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell\" title=\"Title\">BasicRNNCell</a></p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell\" title=\"Title\">BasicLSTMCell</a></p>\n<p><a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/MultiRNNCell\" title=\"Title\">MultiRNNCell</a></p>\n<h4 id=\"3-1-单层LSTM例\"><a href=\"#3-1-单层LSTM例\" class=\"headerlink\" title=\"3.1. 单层LSTM例\"></a>3.1. 单层LSTM例</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\">inputs = tf.placeholder(np.float32, shape=(<span class=\"number\">32</span>, <span class=\"number\">100</span>)) <span class=\"comment\"># 32 是 batch_size</span></div><div class=\"line\">h0 = lstm_cell.zero_state(<span class=\"number\">32</span>, np.float32) <span class=\"comment\"># 得到一个全0的初始状态</span></div><div class=\"line\">output, h1 = lstm_cell.call(inputs, h0)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># LSTM可以看做有两个隐状态h和c，对应的隐层就是一个Tuple元组</span></div><div class=\"line\"><span class=\"comment\"># 每个都是(batch_size, state_size)的形状</span></div><div class=\"line\">print(h1.h)  <span class=\"comment\"># shape=(32, 128)</span></div><div class=\"line\">print(h1.c)  <span class=\"comment\"># shape=(32, 128)</span></div></pre></td></tr></table></figure>\n<h4 id=\"3-2-一次执行多步\"><a href=\"#3-2-一次执行多步\" class=\"headerlink\" title=\"3.2. 一次执行多步\"></a>3.2. 一次执行多步</h4><p>TensorFlow提供了一个tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># inputs: shape = (batch_size, time_steps, input_size) </span></div><div class=\"line\"><span class=\"comment\"># cell: RNNCell</span></div><div class=\"line\"><span class=\"comment\"># initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取0矩阵</span></div><div class=\"line\">outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)</div></pre></td></tr></table></figure>\n<p>官方 <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\" title=\"Title\">tf.nn.dynamic_rnn文档</a></p>\n<h4 id=\"3-3-堆叠RNNCell\"><a href=\"#3-3-堆叠RNNCell\" class=\"headerlink\" title=\"3.3. 堆叠RNNCell\"></a>3.3. 堆叠RNNCell</h4><p>即实现多层的RNN。将x输入第一层RNN的后得到隐层状态h，这个隐层状态就相当于第二层RNN的输入，第二层RNN的隐层状态又相当于第三层RNN的输入，以此类推。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</div><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_a_cell</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\"><span class=\"comment\"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>)]) <span class=\"comment\"># 3层RNN</span></div><div class=\"line\"><span class=\"comment\"># 得到的cell实际也是RNNCell的子类</span></div><div class=\"line\"><span class=\"comment\"># 它的state_size是(128, 128, 128)</span></div><div class=\"line\"><span class=\"comment\"># (128, 128, 128)并不是128x128x128的意思</span></div><div class=\"line\"><span class=\"comment\"># 而是表示共有3个隐层状态，每个隐层状态的大小为128</span></div><div class=\"line\">print(cell.state_size) <span class=\"comment\"># (128, 128, 128)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># 使用对应的call函数</span></div><div class=\"line\">inputs = tf.placeholder(np.float32, shape=(<span class=\"number\">32</span>, <span class=\"number\">100</span>)) <span class=\"comment\"># 32 是 batch_size</span></div><div class=\"line\">h0 = cell.zero_state(<span class=\"number\">32</span>, np.float32) <span class=\"comment\"># 通过zero_state得到一个全0的初始状态</span></div><div class=\"line\">output, h1 = cell.call(inputs, h0)</div><div class=\"line\"></div><div class=\"line\">print(h1) <span class=\"comment\"># tuple中含有3个32x128的向量</span></div></pre></td></tr></table></figure>\n<p>MultiRNNCell也是RNNCell的子类，因此也有call方法、state_size和output_size变量。同样可以通过tf.nn.dynamic_rnn来一次运行多步。</p>\n<ul>\n<li>Attention 版本问题</li>\n</ul>\n<p>目前使用TensorFlow 1.2运行堆叠RNN的代码是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_a_cell</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"keyword\">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\"><span class=\"comment\"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>)]) <span class=\"comment\"># 3层RNN</span></div></pre></td></tr></table></figure>\n<p>更早一些的版本中（比如TensorFlow 1.0.0），实现方式是：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">one_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=<span class=\"number\">128</span>)</div><div class=\"line\">cell = tf.nn.rnn_cell.MultiRNNCell([one_cell] * <span class=\"number\">3</span>) <span class=\"comment\"># 3层RNN</span></div></pre></td></tr></table></figure>\n<p>新版本按照这种的方式定义，就会引起报错。</p>\n<p>阅读参考 - <a href=\"http://www.jianshu.com/p/b3c7883e3ddf\" title=\"Title\">RNN入门：多层LSTM网络（四）</a> </p>\n<h4 id=\"3-4-TF循环神经网络的实现例\"><a href=\"#3-4-TF循环神经网络的实现例\" class=\"headerlink\" title=\"3.4. TF循环神经网络的实现例\"></a>3.4. TF循环神经网络的实现例</h4><p>Make TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.</p>\n<p>RNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。</p>\n<p>程序实例 - <a href=\"https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb\" title=\"Title\">Github Link</a> </p>\n<h3 id=\"4-词分析框架（Word-Framework）\"><a href=\"#4-词分析框架（Word-Framework）\" class=\"headerlink\" title=\"4. 词分析框架（Word Framework）\"></a>4. 词分析框架（Word Framework）</h3><h4 id=\"4-1-word2vec\"><a href=\"#4-1-word2vec\" class=\"headerlink\" title=\"4.1. word2vec\"></a>4.1. word2vec</h4><p>单词嵌入模型（简称为  word2vec 模型）。将word映射成连续（高维）向量，这样通过训练，就可以把对文本内容的处理简化为K维向量空间中向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。</p>\n<p>一般来说， word2vec输出的词向量可以被用来做NLP相关的工作，比如聚类、找同义词、词性分析等等。</p>\n<p>有两种实现的架构，CBOW(Continuous Bag-Of-Words)模型以及Skip-gram模型：</p>\n<hr>\n<center><img src=\"/image/DL/3/4-1.png\" alt=\"RNN\"></center> \n\n<hr>\n<p>阅读链接 - <a href=\"http://www.cnblogs.com/tina-smile/p/5176441.html\" title=\"Title\">word2vec入门基础</a></p>\n<h4 id=\"4-2-seq2seq\"><a href=\"#4-2-seq2seq\" class=\"headerlink\" title=\"4.2. seq2seq\"></a>4.2. seq2seq</h4><p>序列到序列模型（简称为 seq2seq 模型）。seq2seq模型就像一个翻译模型,输入是一个序列(比如一个英文句子),输出也是一个序列(比如该英文句子所对应的法文翻译)。</p>\n<hr>\n<center><img src=\"/image/DL/3/4-2.png\" alt=\"RNN\"></center> \n\n<hr>\n<p>● 编码器：这是一个<a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\" title=\"Title\">tf.nn.dynamic_rnn</a>函数。</p>\n<p>● 解码器：这是一个<a href=\"https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder\" title=\"Title\">tf.contrib.seq2seq.dynamic_rnn_decoder</a>函数。</p>\n<hr>\n<center><img src=\"/image/DL/3/4-3.jpg\" alt=\"RNN\"></center> \n\n<hr>\n<p>阅读链接 - <a href=\"http://www.jianshu.com/p/124b777e0c55\" title=\"Title\">Seq2Seq的DIY简介</a></p>\n<p>阅读链接 - <a href=\"http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\" title=\"Title\">聊天机器人深度学习</a></p>\n<p>阅读链接 - <a href=\"https://github.com/ematvey/tensorflow-seq2seq-tutorials\" title=\"Title\">tensorflow-seq2seq-tutorials </a></p>\n<hr>\n<p>● word2vec输出的是一个词向量，seq2seq输出的是一个词序列。</p>\n<p>推荐阅读 - <a href=\"http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\" title=\"Title\">Deep Learning for Chatbots, Part 1</a></p>\n<p>推荐阅读 - <a href=\"https://zhuanlan.zhihu.com/p/27087310\" title=\"Title\">《安娜卡列尼娜》文本生成</a></p>\n<p>推荐阅读 - <a href=\"https://qiita.com/elm200/items/6f84d3a42eebe6c47caa\" title=\"Title\">LSTMで夏目漱石ぽい文章の生成</a></p>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><p><a href=\"https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb\" title=\"Title\">Github Link - tv-script-generation</a> </p>\n</li>\n<li><p><a href=\"https://github.com/HJTSO/language-translation/blob/master/dlnd_language_translation.ipynb\" title=\"Title\">Github Link - language-translation</a> </p>\n</li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" title=\"Title\">Understanding LSTM Networks</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/28196873\" title=\"Title\">TensorFlow中RNN实现</a> </p>\n</li>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/541458\" title=\"Title\">零基础入门深度学习(5) - 循环神经网络</a> </p>\n</li>\n<li><p><a href=\"https://zybuluo.com/hanbingtao/note/581764\" title=\"Title\">零基础入门深度学习(6) - 长短时记忆网络</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca\" title=\"Title\">LSTMネットワークの概要</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/t_Signull/items/21b82be280b46f467d1b\" title=\"Title\">わかるLSTM</a> </p>\n</li>\n</ul>\n"},{"title":"Deep learning笔记2-CNN卷积神经网络","lang":"zh","date":"2017-09-16T09:18:56.000Z","_content":"\n### 1. 卷积神经网络简介（CNN）\n\n原图和公式说明来自：[零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\") \n\n-------------------------------------\n\n<center>![CNN](/image/DL/2/1.png)</center> \n\n-------------------------------------\n\n一个卷积神经网络（Convolutional Neural Network）由若干卷积层、池化层、全连接层组成。\n\n#### 1.1. 卷积神经网络输出值的计算\n\n以一个5*5的图像，使用一个3*3的filter进行卷积，得到一个3*3的Feature Map为例：\n\n<center>![CNN](/image/DL/2/2.png)</center>\n\n-------------------------------------\n\n以步幅(stride)为1，依次计算出Feature Map中所有元素的值，计算过程：\n\n<center>![CNN](/image/DL/2/3.gif)</center>  \n\n-------------------------------------\n\n用例来自 - [零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\")   \n\n推荐阅读 - [Gluon - 卷积神经网络](https://zh.gluon.ai/cnn-scratch.html \"Title\") \n\n##### 1.1.1. 卷积层输出值的计算\n\n图像大小、步幅和卷积后的Feature Map大小，满足下面的关系：\n\n\\begin{align}\nW_2 &= (W_1 - F + 2P)/S + 1\\qquad\\\\\nH_2 &= (H_1 - F + 2P)/S + 1\\qquad\n\\end{align}\n\n在上面两个公式中：W2是卷积后Feature Map的宽度；W1是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；H2是卷积后Feature Map的高度；H1是卷积前图像的宽度。\n\n下面的显示包含两个filter的卷积层的计算。可以看到7*7*3输入，经过两个3*3*3filter的卷积(步幅为2)，得到了3*3*2的输出。另外也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。\n\n<center>![CNN](/image/DL/2/4.gif)</center>  \n\n##### 1.1.2. 池化层输出值的计算\n\n池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法最常用的是Max Pooling，即样本中取最大值，作为采样后的样本值。下例是2*2 max pooling：\n\n<center>![CNN](/image/DL/2/5.png)</center>  \n\n-------------------------------------\n\n还有Mean Pooling，即取各样本的平均值。\n\n#### 1.2. 卷积公式的计算\n\n用X[i,j]表示图像的第i行第j列元素；对filter的每个权重进行编号，用W[m,n]表示第m行第n列权重，用Wb表示filter的偏置项；对Feature Map的每个元素进行编号，a[i,j]用表示Feature Map的第i行第j列元素；用f表示激活函数。\n\n$$a_{i,j}=f(\\sum_{m=0}^{2}\\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_b)$$\n\n深度大于1的卷积计算公式：  \n\n$$a_{i,j}=f(\\sum_{d=0}^{D-1}\\sum_{m=0}^{F-1}\\sum_{n=0}^{F-1}w_{d,m,n}x_{d,i+m,j+n}+w_b)$$\n\n#### 1.3. 卷积神经网络的训练\n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w\n\n#### 1.4. 关于权重w与偏置项b的初始化\n\n##### 1.4.1. 权重w的初始化\n\n① 均匀分布：tf.random_uniform()\n\n② 正太分布：tf.random_normal()\n\n③ 从截断的正态取值--横轴区间（μ-2σ，μ+2σ）95%面积：tf.truncated_normal()\n\n④ 还有一个经验公式：从[-y, y]取值，其中 $y=1/\\sqrt{n}$\n\n根据经验使用③效果较佳…\n\n##### 1.4.2. 偏置项b的初始化\n\n一般使用tf.zeros()来初始化为零值. \n\n阅读参考 - [Role of Bias in Neural Networks](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks \"Title\")\n\n“a bias value allows you to shift the activation function to the left or right”\n\n### 2. 基于TensorFlow的实现（CNN with TF）\n\n#### 2.1. TF卷积神经网络的基本实现\n\n##### 2.1.1. 卷积层\n\n```python\n\n# Input/Image\ninput = tf.placeholder(\n    tf.float32,\n    shape=[None, image_height, image_width, color_channels])\n\n# Weight and bias\nweight = tf.Variable(tf.truncated_normal(\n    [filter_size_height, filter_size_width, color_channels, k_output]))\nbias = tf.Variable(tf.zeros(k_output))\n\n# Apply Convolution\nconv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n# Add bias\nconv_layer = tf.nn.bias_add(conv_layer, bias)\n# Apply activation function\nconv_layer = tf.nn.relu(conv_layer)\n\n```\n\nweights 作为滤波器，[1, 2, 2, 1] 作为 strides。TensorFlow 对每一个 input 维度使用一个单独的 stride 参数，[batch, input_height, input_width, input_channels]。通常把 batch 和 input_channels （strides 序列中的第一个第四个）的 stride 设为 1。\n\n##### 2.1.2. 池化层\n\n```python\n\n# Apply Max Pooling\nconv_layer = tf.nn.max_pool(\n    conv_layer,\n    ksize=[1, 2, 2, 1],\n    strides=[1, 2, 2, 1],\n    padding='SAME')\n    \n```\n\ntf.nn.max_pool() 函数实现最大池化时， ksize参数是滤波器大小，strides参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。\n\nksize 和 strides 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 ([batch, height, width, channels])，对 ksize 和 strides 来说，batch 和 channel 通常都设置成 1。\n\n#### 2.2. TF卷积神经网络的实现例\n\n对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html \"Title\") 中的图片进行分类。\n该数据集包含飞机、猫狗和其他物体。先预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。构建卷积（convolution）、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后在样本图片上看到神经网络的预测结果。\n\n具体实现不赘述，过程直接看Github，吾辈准确率不高，才57%左右 - [Github Link](https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb \"Title\")   \n\n#### 2.3. 卷积神经网络内部一窥\n-------------------------------------\n<center>![CNN](/image/DL/2/CNN.jpg)</center> \n\n-------------------------------------\n瑞尔森大学的Adam Harley创建了一个交互式视觉化模型，能够帮助解释卷积神经网络内部每一层是如何工作的：[Link](http://scs.ryerson.ca/~aharley/vis/conv/ \"Title\")   \n\n### 3. 自编码器（Autoencoder）\n\n自编码器（Autoencoder）由于处理过程中有单元减少，解压缩效果不如MP3与JPEG，但是在图像去噪（Denoising）与降维（dimensionality reduction）方面取得不错的效果。\n\n<center>![CNN](/image/DL/2/autoencoder_1.png)</center> \n\n-------------------------------------\n\n#### 3.1. 使用dense实现\n\nEncoder：使用tf.layers.dense与relu激活函数实现\nDecoder：使用tf.layers.dense与sigmoid激活函数实现\n\n- Autoencoder用例实现1 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Simple_Autoencoder_Solution.ipynb \"Title\") \n\n#### 3.2. 使用卷积实现\n\nEncoder：使用tf.layers.conv2d与tf.layers.max_pooling2d（下采样）实现\nDecoder：使用tf.layers.conv2d与tf.image.resize_nearest_neighbor（上采样）实现\n\n- Autoencoder用例实现2 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Convolutional_Autoencoder_Solution.ipynb \"Title\") \n\n推荐：Deconvolution and Checkerboard Artifacts [Distill](https://distill.pub/2016/deconv-checkerboard/ \"Title\") \n\n### 4. 迁移学习（Transfer Learning）\n\n迁移学习（Transfer Learning）：在实际中，通常会使用预训练模型（比如 AlexNet，VGGNet，Google Inception Net，ResNet），将最后的几个全连接层（基本作用是分类器），替换成自己的分类器，再进行训练与测试。  \n\n介绍其中两种CNN网络模型 AlexNet 与 VGGNet：\n\n#### 4.1 AlexNet\n\nAlexNet是由SuperVision设计，包括成员Alex Krizhevsky，Geoffrey Hinton，Ilya Sutskeve设计的CNN网络模型。该模型在2012年ImageNet Large Scale Visual Recognition Challenge的比赛中以15.3%错误率获得冠军，领先第二名10.8个百分点。\n\n输入要求256(图像大小)，均值是256的，减均值后再crop到227(输入图像大小)   \n\n参考 - [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf \"Title\") \n\n#### 4.2 VGGNet\n\nVGGNet是牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发的的深度卷积神经网络，在ILSVRC 2014上取得了第二名的成绩，将Top-5错误率降到7.3%。\n\n输入要求256(图像大小)，均值是256的，减均值后再crop到224(输入图像大小)  \n\n参考 - [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/ \"Title\") \n\n<center>![cnnarchitecture](/image/DL/2/cnnarchitecture.jpg)</center>  \n\n-------------------------------------\n\n- VGGNet使用例 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/transfer-learning/Transfer_Learning_Solution.ipynb \"Title\") \n\n-------------------------------------\n\n### 5. 目标检测（Object Detection）\n\nObject Detection是在给定的图片中精确找到物体所在位置，并标注出物体的类别。\nObject Detection要解决的问题就是物体在哪里Where，是什么What这整个流程的问题。\n\n但是在现实生活中，问题不是那么简单，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，而且物体的类别还有多种。\n\n关于目标检测区域建议的演化历史：- [A Brief History of CNNs](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4 \"Title\")\n\nCNN->RCNN->Fast-RCNN->Faster-RCNN->Mask-RCNN->...\n\n-------------------------------------\n\n<center>![CNN](/image/DL/2/Mask-RCNN.png)</center> \n\n-------------------------------------\n\n- [RCNN](https://zhuanlan.zhihu.com/p/27473413 \"Title\")\n\n- [Fast-RCNN](https://zhuanlan.zhihu.com/p/27582096 \"Title\")\n\n- [Faster-RCNN](https://zhuanlan.zhihu.com/p/27988828 \"Title\")\n\n- [Mask-RCNN](https://arxiv.org/abs/1703.06870 \"Title\")\n\n### 程序实例（Program Example）\n\n- [Github Link](https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [Gluon - 卷积神经网络](https://zh.gluon.ai/cnn-scratch.html \"Title\") \n\n- [Network-In-Network で CIFAR-10 精度 90%](http://tensorflow.classcat.com/2017/05/08/tensorflow-network-in-network/ \"Title\") \n\n- [零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\") \n\n- [不同的层在CNN网络的作用原理](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ \"Title\") \n","source":"_posts/zh/Deep learning笔记2-CNN卷积神经网络.md","raw":"\n---\ntitle: Deep learning笔记2-CNN卷积神经网络\nlang: zh\ndate: 2017-09-16 18:18:56\ntags: Deep Learning\n---\n\n### 1. 卷积神经网络简介（CNN）\n\n原图和公式说明来自：[零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\") \n\n-------------------------------------\n\n<center>![CNN](/image/DL/2/1.png)</center> \n\n-------------------------------------\n\n一个卷积神经网络（Convolutional Neural Network）由若干卷积层、池化层、全连接层组成。\n\n#### 1.1. 卷积神经网络输出值的计算\n\n以一个5*5的图像，使用一个3*3的filter进行卷积，得到一个3*3的Feature Map为例：\n\n<center>![CNN](/image/DL/2/2.png)</center>\n\n-------------------------------------\n\n以步幅(stride)为1，依次计算出Feature Map中所有元素的值，计算过程：\n\n<center>![CNN](/image/DL/2/3.gif)</center>  \n\n-------------------------------------\n\n用例来自 - [零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\")   \n\n推荐阅读 - [Gluon - 卷积神经网络](https://zh.gluon.ai/cnn-scratch.html \"Title\") \n\n##### 1.1.1. 卷积层输出值的计算\n\n图像大小、步幅和卷积后的Feature Map大小，满足下面的关系：\n\n\\begin{align}\nW_2 &= (W_1 - F + 2P)/S + 1\\qquad\\\\\nH_2 &= (H_1 - F + 2P)/S + 1\\qquad\n\\end{align}\n\n在上面两个公式中：W2是卷积后Feature Map的宽度；W1是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；H2是卷积后Feature Map的高度；H1是卷积前图像的宽度。\n\n下面的显示包含两个filter的卷积层的计算。可以看到7*7*3输入，经过两个3*3*3filter的卷积(步幅为2)，得到了3*3*2的输出。另外也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。\n\n<center>![CNN](/image/DL/2/4.gif)</center>  \n\n##### 1.1.2. 池化层输出值的计算\n\n池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法最常用的是Max Pooling，即样本中取最大值，作为采样后的样本值。下例是2*2 max pooling：\n\n<center>![CNN](/image/DL/2/5.png)</center>  \n\n-------------------------------------\n\n还有Mean Pooling，即取各样本的平均值。\n\n#### 1.2. 卷积公式的计算\n\n用X[i,j]表示图像的第i行第j列元素；对filter的每个权重进行编号，用W[m,n]表示第m行第n列权重，用Wb表示filter的偏置项；对Feature Map的每个元素进行编号，a[i,j]用表示Feature Map的第i行第j列元素；用f表示激活函数。\n\n$$a_{i,j}=f(\\sum_{m=0}^{2}\\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_b)$$\n\n深度大于1的卷积计算公式：  \n\n$$a_{i,j}=f(\\sum_{d=0}^{D-1}\\sum_{m=0}^{F-1}\\sum_{n=0}^{F-1}w_{d,m,n}x_{d,i+m,j+n}+w_b)$$\n\n#### 1.3. 卷积神经网络的训练\n\n先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w\n\n#### 1.4. 关于权重w与偏置项b的初始化\n\n##### 1.4.1. 权重w的初始化\n\n① 均匀分布：tf.random_uniform()\n\n② 正太分布：tf.random_normal()\n\n③ 从截断的正态取值--横轴区间（μ-2σ，μ+2σ）95%面积：tf.truncated_normal()\n\n④ 还有一个经验公式：从[-y, y]取值，其中 $y=1/\\sqrt{n}$\n\n根据经验使用③效果较佳…\n\n##### 1.4.2. 偏置项b的初始化\n\n一般使用tf.zeros()来初始化为零值. \n\n阅读参考 - [Role of Bias in Neural Networks](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks \"Title\")\n\n“a bias value allows you to shift the activation function to the left or right”\n\n### 2. 基于TensorFlow的实现（CNN with TF）\n\n#### 2.1. TF卷积神经网络的基本实现\n\n##### 2.1.1. 卷积层\n\n```python\n\n# Input/Image\ninput = tf.placeholder(\n    tf.float32,\n    shape=[None, image_height, image_width, color_channels])\n\n# Weight and bias\nweight = tf.Variable(tf.truncated_normal(\n    [filter_size_height, filter_size_width, color_channels, k_output]))\nbias = tf.Variable(tf.zeros(k_output))\n\n# Apply Convolution\nconv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n# Add bias\nconv_layer = tf.nn.bias_add(conv_layer, bias)\n# Apply activation function\nconv_layer = tf.nn.relu(conv_layer)\n\n```\n\nweights 作为滤波器，[1, 2, 2, 1] 作为 strides。TensorFlow 对每一个 input 维度使用一个单独的 stride 参数，[batch, input_height, input_width, input_channels]。通常把 batch 和 input_channels （strides 序列中的第一个第四个）的 stride 设为 1。\n\n##### 2.1.2. 池化层\n\n```python\n\n# Apply Max Pooling\nconv_layer = tf.nn.max_pool(\n    conv_layer,\n    ksize=[1, 2, 2, 1],\n    strides=[1, 2, 2, 1],\n    padding='SAME')\n    \n```\n\ntf.nn.max_pool() 函数实现最大池化时， ksize参数是滤波器大小，strides参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。\n\nksize 和 strides 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 ([batch, height, width, channels])，对 ksize 和 strides 来说，batch 和 channel 通常都设置成 1。\n\n#### 2.2. TF卷积神经网络的实现例\n\n对 [CIFAR-10 数据集](https://www.cs.toronto.edu/~kriz/cifar.html \"Title\") 中的图片进行分类。\n该数据集包含飞机、猫狗和其他物体。先预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。构建卷积（convolution）、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后在样本图片上看到神经网络的预测结果。\n\n具体实现不赘述，过程直接看Github，吾辈准确率不高，才57%左右 - [Github Link](https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb \"Title\")   \n\n#### 2.3. 卷积神经网络内部一窥\n-------------------------------------\n<center>![CNN](/image/DL/2/CNN.jpg)</center> \n\n-------------------------------------\n瑞尔森大学的Adam Harley创建了一个交互式视觉化模型，能够帮助解释卷积神经网络内部每一层是如何工作的：[Link](http://scs.ryerson.ca/~aharley/vis/conv/ \"Title\")   \n\n### 3. 自编码器（Autoencoder）\n\n自编码器（Autoencoder）由于处理过程中有单元减少，解压缩效果不如MP3与JPEG，但是在图像去噪（Denoising）与降维（dimensionality reduction）方面取得不错的效果。\n\n<center>![CNN](/image/DL/2/autoencoder_1.png)</center> \n\n-------------------------------------\n\n#### 3.1. 使用dense实现\n\nEncoder：使用tf.layers.dense与relu激活函数实现\nDecoder：使用tf.layers.dense与sigmoid激活函数实现\n\n- Autoencoder用例实现1 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Simple_Autoencoder_Solution.ipynb \"Title\") \n\n#### 3.2. 使用卷积实现\n\nEncoder：使用tf.layers.conv2d与tf.layers.max_pooling2d（下采样）实现\nDecoder：使用tf.layers.conv2d与tf.image.resize_nearest_neighbor（上采样）实现\n\n- Autoencoder用例实现2 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Convolutional_Autoencoder_Solution.ipynb \"Title\") \n\n推荐：Deconvolution and Checkerboard Artifacts [Distill](https://distill.pub/2016/deconv-checkerboard/ \"Title\") \n\n### 4. 迁移学习（Transfer Learning）\n\n迁移学习（Transfer Learning）：在实际中，通常会使用预训练模型（比如 AlexNet，VGGNet，Google Inception Net，ResNet），将最后的几个全连接层（基本作用是分类器），替换成自己的分类器，再进行训练与测试。  \n\n介绍其中两种CNN网络模型 AlexNet 与 VGGNet：\n\n#### 4.1 AlexNet\n\nAlexNet是由SuperVision设计，包括成员Alex Krizhevsky，Geoffrey Hinton，Ilya Sutskeve设计的CNN网络模型。该模型在2012年ImageNet Large Scale Visual Recognition Challenge的比赛中以15.3%错误率获得冠军，领先第二名10.8个百分点。\n\n输入要求256(图像大小)，均值是256的，减均值后再crop到227(输入图像大小)   \n\n参考 - [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf \"Title\") \n\n#### 4.2 VGGNet\n\nVGGNet是牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发的的深度卷积神经网络，在ILSVRC 2014上取得了第二名的成绩，将Top-5错误率降到7.3%。\n\n输入要求256(图像大小)，均值是256的，减均值后再crop到224(输入图像大小)  \n\n参考 - [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/ \"Title\") \n\n<center>![cnnarchitecture](/image/DL/2/cnnarchitecture.jpg)</center>  \n\n-------------------------------------\n\n- VGGNet使用例 - Udacity [Github Link](https://github.com/udacity/cn-deep-learning/blob/master/tutorials/transfer-learning/Transfer_Learning_Solution.ipynb \"Title\") \n\n-------------------------------------\n\n### 5. 目标检测（Object Detection）\n\nObject Detection是在给定的图片中精确找到物体所在位置，并标注出物体的类别。\nObject Detection要解决的问题就是物体在哪里Where，是什么What这整个流程的问题。\n\n但是在现实生活中，问题不是那么简单，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，而且物体的类别还有多种。\n\n关于目标检测区域建议的演化历史：- [A Brief History of CNNs](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4 \"Title\")\n\nCNN->RCNN->Fast-RCNN->Faster-RCNN->Mask-RCNN->...\n\n-------------------------------------\n\n<center>![CNN](/image/DL/2/Mask-RCNN.png)</center> \n\n-------------------------------------\n\n- [RCNN](https://zhuanlan.zhihu.com/p/27473413 \"Title\")\n\n- [Fast-RCNN](https://zhuanlan.zhihu.com/p/27582096 \"Title\")\n\n- [Faster-RCNN](https://zhuanlan.zhihu.com/p/27988828 \"Title\")\n\n- [Mask-RCNN](https://arxiv.org/abs/1703.06870 \"Title\")\n\n### 程序实例（Program Example）\n\n- [Github Link](https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [Gluon - 卷积神经网络](https://zh.gluon.ai/cnn-scratch.html \"Title\") \n\n- [Network-In-Network で CIFAR-10 精度 90%](http://tensorflow.classcat.com/2017/05/08/tensorflow-network-in-network/ \"Title\") \n\n- [零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480 \"Title\") \n\n- [不同的层在CNN网络的作用原理](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ \"Title\") \n","slug":"zh-Deep-learning笔记2-CNN卷积神经网络","published":1,"updated":"2018-10-22T12:47:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zg0025og6407nx5md1","content":"<h3 id=\"1-卷积神经网络简介（CNN）\"><a href=\"#1-卷积神经网络简介（CNN）\" class=\"headerlink\" title=\"1. 卷积神经网络简介（CNN）\"></a>1. 卷积神经网络简介（CNN）</h3><p>原图和公式说明来自：<a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(4) - 卷积神经网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/2/1.png\" alt=\"CNN\"></center> \n\n<hr>\n<p>一个卷积神经网络（Convolutional Neural Network）由若干卷积层、池化层、全连接层组成。</p>\n<h4 id=\"1-1-卷积神经网络输出值的计算\"><a href=\"#1-1-卷积神经网络输出值的计算\" class=\"headerlink\" title=\"1.1. 卷积神经网络输出值的计算\"></a>1.1. 卷积神经网络输出值的计算</h4><p>以一个5<em>5的图像，使用一个3</em>3的filter进行卷积，得到一个3*3的Feature Map为例：</p>\n<center><img src=\"/image/DL/2/2.png\" alt=\"CNN\"></center>\n\n<hr>\n<p>以步幅(stride)为1，依次计算出Feature Map中所有元素的值，计算过程：</p>\n<center><img src=\"/image/DL/2/3.gif\" alt=\"CNN\"></center>  \n\n<hr>\n<p>用例来自 - <a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(4) - 卷积神经网络</a>   </p>\n<p>推荐阅读 - <a href=\"https://zh.gluon.ai/cnn-scratch.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Gluon - 卷积神经网络</a> </p>\n<h5 id=\"1-1-1-卷积层输出值的计算\"><a href=\"#1-1-1-卷积层输出值的计算\" class=\"headerlink\" title=\"1.1.1. 卷积层输出值的计算\"></a>1.1.1. 卷积层输出值的计算</h5><p>图像大小、步幅和卷积后的Feature Map大小，满足下面的关系：</p>\n<p>\\begin{align}<br>W_2 &amp;= (W_1 - F + 2P)/S + 1\\qquad\\\\<br>H_2 &amp;= (H_1 - F + 2P)/S + 1\\qquad<br>\\end{align}</p>\n<p>在上面两个公式中：W2是卷积后Feature Map的宽度；W1是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；H2是卷积后Feature Map的高度；H1是卷积前图像的宽度。</p>\n<p>下面的显示包含两个filter的卷积层的计算。可以看到7<em>7</em>3输入，经过两个3<em>3</em>3filter的卷积(步幅为2)，得到了3<em>3</em>2的输出。另外也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。</p>\n<center><img src=\"/image/DL/2/4.gif\" alt=\"CNN\"></center>  \n\n<h5 id=\"1-1-2-池化层输出值的计算\"><a href=\"#1-1-2-池化层输出值的计算\" class=\"headerlink\" title=\"1.1.2. 池化层输出值的计算\"></a>1.1.2. 池化层输出值的计算</h5><p>池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法最常用的是Max Pooling，即样本中取最大值，作为采样后的样本值。下例是2*2 max pooling：</p>\n<center><img src=\"/image/DL/2/5.png\" alt=\"CNN\"></center>  \n\n<hr>\n<p>还有Mean Pooling，即取各样本的平均值。</p>\n<h4 id=\"1-2-卷积公式的计算\"><a href=\"#1-2-卷积公式的计算\" class=\"headerlink\" title=\"1.2. 卷积公式的计算\"></a>1.2. 卷积公式的计算</h4><p>用X[i,j]表示图像的第i行第j列元素；对filter的每个权重进行编号，用W[m,n]表示第m行第n列权重，用Wb表示filter的偏置项；对Feature Map的每个元素进行编号，a[i,j]用表示Feature Map的第i行第j列元素；用f表示激活函数。</p>\n<p>$$a_{i,j}=f(\\sum_{m=0}^{2}\\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_b)$$</p>\n<p>深度大于1的卷积计算公式：  </p>\n<p>$$a_{i,j}=f(\\sum_{d=0}^{D-1}\\sum_{m=0}^{F-1}\\sum_{n=0}^{F-1}w_{d,m,n}x_{d,i+m,j+n}+w_b)$$</p>\n<h4 id=\"1-3-卷积神经网络的训练\"><a href=\"#1-3-卷积神经网络的训练\" class=\"headerlink\" title=\"1.3. 卷积神经网络的训练\"></a>1.3. 卷积神经网络的训练</h4><p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w</p>\n<h4 id=\"1-4-关于权重w与偏置项b的初始化\"><a href=\"#1-4-关于权重w与偏置项b的初始化\" class=\"headerlink\" title=\"1.4. 关于权重w与偏置项b的初始化\"></a>1.4. 关于权重w与偏置项b的初始化</h4><h5 id=\"1-4-1-权重w的初始化\"><a href=\"#1-4-1-权重w的初始化\" class=\"headerlink\" title=\"1.4.1. 权重w的初始化\"></a>1.4.1. 权重w的初始化</h5><p>① 均匀分布：tf.random_uniform()</p>\n<p>② 正太分布：tf.random_normal()</p>\n<p>③ 从截断的正态取值–横轴区间（μ-2σ，μ+2σ）95%面积：tf.truncated_normal()</p>\n<p>④ 还有一个经验公式：从[-y, y]取值，其中 $y=1/\\sqrt{n}$</p>\n<p>根据经验使用③效果较佳…</p>\n<h5 id=\"1-4-2-偏置项b的初始化\"><a href=\"#1-4-2-偏置项b的初始化\" class=\"headerlink\" title=\"1.4.2. 偏置项b的初始化\"></a>1.4.2. 偏置项b的初始化</h5><p>一般使用tf.zeros()来初始化为零值. </p>\n<p>阅读参考 - <a href=\"https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks\" title=\"Title\" target=\"_blank\" rel=\"external\">Role of Bias in Neural Networks</a></p>\n<p>“a bias value allows you to shift the activation function to the left or right”</p>\n<h3 id=\"2-基于TensorFlow的实现（CNN-with-TF）\"><a href=\"#2-基于TensorFlow的实现（CNN-with-TF）\" class=\"headerlink\" title=\"2. 基于TensorFlow的实现（CNN with TF）\"></a>2. 基于TensorFlow的实现（CNN with TF）</h3><h4 id=\"2-1-TF卷积神经网络的基本实现\"><a href=\"#2-1-TF卷积神经网络的基本实现\" class=\"headerlink\" title=\"2.1. TF卷积神经网络的基本实现\"></a>2.1. TF卷积神经网络的基本实现</h4><h5 id=\"2-1-1-卷积层\"><a href=\"#2-1-1-卷积层\" class=\"headerlink\" title=\"2.1.1. 卷积层\"></a>2.1.1. 卷积层</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Input/Image</span></div><div class=\"line\">input = tf.placeholder(</div><div class=\"line\">    tf.float32,</div><div class=\"line\">    shape=[<span class=\"keyword\">None</span>, image_height, image_width, color_channels])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Weight and bias</span></div><div class=\"line\">weight = tf.Variable(tf.truncated_normal(</div><div class=\"line\">    [filter_size_height, filter_size_width, color_channels, k_output]))</div><div class=\"line\">bias = tf.Variable(tf.zeros(k_output))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Apply Convolution</span></div><div class=\"line\">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>], padding=<span class=\"string\">'SAME'</span>)</div><div class=\"line\"><span class=\"comment\"># Add bias</span></div><div class=\"line\">conv_layer = tf.nn.bias_add(conv_layer, bias)</div><div class=\"line\"><span class=\"comment\"># Apply activation function</span></div><div class=\"line\">conv_layer = tf.nn.relu(conv_layer)</div></pre></td></tr></table></figure>\n<p>weights 作为滤波器，[1, 2, 2, 1] 作为 strides。TensorFlow 对每一个 input 维度使用一个单独的 stride 参数，[batch, input_height, input_width, input_channels]。通常把 batch 和 input_channels （strides 序列中的第一个第四个）的 stride 设为 1。</p>\n<h5 id=\"2-1-2-池化层\"><a href=\"#2-1-2-池化层\" class=\"headerlink\" title=\"2.1.2. 池化层\"></a>2.1.2. 池化层</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Apply Max Pooling</span></div><div class=\"line\">conv_layer = tf.nn.max_pool(</div><div class=\"line\">    conv_layer,</div><div class=\"line\">    ksize=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>],</div><div class=\"line\">    strides=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>],</div><div class=\"line\">    padding=<span class=\"string\">'SAME'</span>)</div></pre></td></tr></table></figure>\n<p>tf.nn.max_pool() 函数实现最大池化时， ksize参数是滤波器大小，strides参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。</p>\n<p>ksize 和 strides 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 ([batch, height, width, channels])，对 ksize 和 strides 来说，batch 和 channel 通常都设置成 1。</p>\n<h4 id=\"2-2-TF卷积神经网络的实现例\"><a href=\"#2-2-TF卷积神经网络的实现例\" class=\"headerlink\" title=\"2.2. TF卷积神经网络的实现例\"></a>2.2. TF卷积神经网络的实现例</h4><p>对 <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" title=\"Title\" target=\"_blank\" rel=\"external\">CIFAR-10 数据集</a> 中的图片进行分类。<br>该数据集包含飞机、猫狗和其他物体。先预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。构建卷积（convolution）、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后在样本图片上看到神经网络的预测结果。</p>\n<p>具体实现不赘述，过程直接看Github，吾辈准确率不高，才57%左右 - <a href=\"https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a>   </p>\n<h4 id=\"2-3-卷积神经网络内部一窥\"><a href=\"#2-3-卷积神经网络内部一窥\" class=\"headerlink\" title=\"2.3. 卷积神经网络内部一窥\"></a>2.3. 卷积神经网络内部一窥</h4><hr>\n<center><img src=\"/image/DL/2/CNN.jpg\" alt=\"CNN\"></center> \n\n<hr>\n<p>瑞尔森大学的Adam Harley创建了一个交互式视觉化模型，能够帮助解释卷积神经网络内部每一层是如何工作的：<a href=\"http://scs.ryerson.ca/~aharley/vis/conv/\" title=\"Title\" target=\"_blank\" rel=\"external\">Link</a>   </p>\n<h3 id=\"3-自编码器（Autoencoder）\"><a href=\"#3-自编码器（Autoencoder）\" class=\"headerlink\" title=\"3. 自编码器（Autoencoder）\"></a>3. 自编码器（Autoencoder）</h3><p>自编码器（Autoencoder）由于处理过程中有单元减少，解压缩效果不如MP3与JPEG，但是在图像去噪（Denoising）与降维（dimensionality reduction）方面取得不错的效果。</p>\n<center><img src=\"/image/DL/2/autoencoder_1.png\" alt=\"CNN\"></center> \n\n<hr>\n<h4 id=\"3-1-使用dense实现\"><a href=\"#3-1-使用dense实现\" class=\"headerlink\" title=\"3.1. 使用dense实现\"></a>3.1. 使用dense实现</h4><p>Encoder：使用tf.layers.dense与relu激活函数实现<br>Decoder：使用tf.layers.dense与sigmoid激活函数实现</p>\n<ul>\n<li>Autoencoder用例实现1 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Simple_Autoencoder_Solution.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<h4 id=\"3-2-使用卷积实现\"><a href=\"#3-2-使用卷积实现\" class=\"headerlink\" title=\"3.2. 使用卷积实现\"></a>3.2. 使用卷积实现</h4><p>Encoder：使用tf.layers.conv2d与tf.layers.max_pooling2d（下采样）实现<br>Decoder：使用tf.layers.conv2d与tf.image.resize_nearest_neighbor（上采样）实现</p>\n<ul>\n<li>Autoencoder用例实现2 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Convolutional_Autoencoder_Solution.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<p>推荐：Deconvolution and Checkerboard Artifacts <a href=\"https://distill.pub/2016/deconv-checkerboard/\" title=\"Title\" target=\"_blank\" rel=\"external\">Distill</a> </p>\n<h3 id=\"4-迁移学习（Transfer-Learning）\"><a href=\"#4-迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"4. 迁移学习（Transfer Learning）\"></a>4. 迁移学习（Transfer Learning）</h3><p>迁移学习（Transfer Learning）：在实际中，通常会使用预训练模型（比如 AlexNet，VGGNet，Google Inception Net，ResNet），将最后的几个全连接层（基本作用是分类器），替换成自己的分类器，再进行训练与测试。  </p>\n<p>介绍其中两种CNN网络模型 AlexNet 与 VGGNet：</p>\n<h4 id=\"4-1-AlexNet\"><a href=\"#4-1-AlexNet\" class=\"headerlink\" title=\"4.1 AlexNet\"></a>4.1 AlexNet</h4><p>AlexNet是由SuperVision设计，包括成员Alex Krizhevsky，Geoffrey Hinton，Ilya Sutskeve设计的CNN网络模型。该模型在2012年ImageNet Large Scale Visual Recognition Challenge的比赛中以15.3%错误率获得冠军，领先第二名10.8个百分点。</p>\n<p>输入要求256(图像大小)，均值是256的，减均值后再crop到227(输入图像大小)   </p>\n<p>参考 - <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" title=\"Title\" target=\"_blank\" rel=\"external\">AlexNet</a> </p>\n<h4 id=\"4-2-VGGNet\"><a href=\"#4-2-VGGNet\" class=\"headerlink\" title=\"4.2 VGGNet\"></a>4.2 VGGNet</h4><p>VGGNet是牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发的的深度卷积神经网络，在ILSVRC 2014上取得了第二名的成绩，将Top-5错误率降到7.3%。</p>\n<p>输入要求256(图像大小)，均值是256的，减均值后再crop到224(输入图像大小)  </p>\n<p>参考 - <a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\" title=\"Title\" target=\"_blank\" rel=\"external\">Visual Geometry Group</a> </p>\n<center><img src=\"/image/DL/2/cnnarchitecture.jpg\" alt=\"cnnarchitecture\"></center>  \n\n<hr>\n<ul>\n<li>VGGNet使用例 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/transfer-learning/Transfer_Learning_Solution.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<hr>\n<h3 id=\"5-目标检测（Object-Detection）\"><a href=\"#5-目标检测（Object-Detection）\" class=\"headerlink\" title=\"5. 目标检测（Object Detection）\"></a>5. 目标检测（Object Detection）</h3><p>Object Detection是在给定的图片中精确找到物体所在位置，并标注出物体的类别。<br>Object Detection要解决的问题就是物体在哪里Where，是什么What这整个流程的问题。</p>\n<p>但是在现实生活中，问题不是那么简单，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，而且物体的类别还有多种。</p>\n<p>关于目标检测区域建议的演化历史：- <a href=\"https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\" title=\"Title\" target=\"_blank\" rel=\"external\">A Brief History of CNNs</a></p>\n<p>CNN-&gt;RCNN-&gt;Fast-RCNN-&gt;Faster-RCNN-&gt;Mask-RCNN-&gt;…</p>\n<hr>\n<center><img src=\"/image/DL/2/Mask-RCNN.png\" alt=\"CNN\"></center> \n\n<hr>\n<ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27473413\" title=\"Title\" target=\"_blank\" rel=\"external\">RCNN</a></p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27582096\" title=\"Title\" target=\"_blank\" rel=\"external\">Fast-RCNN</a></p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27988828\" title=\"Title\" target=\"_blank\" rel=\"external\">Faster-RCNN</a></p>\n</li>\n<li><p><a href=\"https://arxiv.org/abs/1703.06870\" title=\"Title\" target=\"_blank\" rel=\"external\">Mask-RCNN</a></p>\n</li>\n</ul>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><a href=\"https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zh.gluon.ai/cnn-scratch.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Gluon - 卷积神经网络</a> </p>\n</li>\n<li><p><a href=\"http://tensorflow.classcat.com/2017/05/08/tensorflow-network-in-network/\" title=\"Title\" target=\"_blank\" rel=\"external\">Network-In-Network で CIFAR-10 精度 90%</a> </p>\n</li>\n<li><p><a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\" target=\"_blank\" rel=\"external\">零基础入门深度学习(4) - 卷积神经网络</a> </p>\n</li>\n<li><p><a href=\"https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\" title=\"Title\" target=\"_blank\" rel=\"external\">不同的层在CNN网络的作用原理</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-卷积神经网络简介（CNN）\"><a href=\"#1-卷积神经网络简介（CNN）\" class=\"headerlink\" title=\"1. 卷积神经网络简介（CNN）\"></a>1. 卷积神经网络简介（CNN）</h3><p>原图和公式说明来自：<a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\">零基础入门深度学习(4) - 卷积神经网络</a> </p>\n<hr>\n<center><img src=\"/image/DL/2/1.png\" alt=\"CNN\"></center> \n\n<hr>\n<p>一个卷积神经网络（Convolutional Neural Network）由若干卷积层、池化层、全连接层组成。</p>\n<h4 id=\"1-1-卷积神经网络输出值的计算\"><a href=\"#1-1-卷积神经网络输出值的计算\" class=\"headerlink\" title=\"1.1. 卷积神经网络输出值的计算\"></a>1.1. 卷积神经网络输出值的计算</h4><p>以一个5<em>5的图像，使用一个3</em>3的filter进行卷积，得到一个3*3的Feature Map为例：</p>\n<center><img src=\"/image/DL/2/2.png\" alt=\"CNN\"></center>\n\n<hr>\n<p>以步幅(stride)为1，依次计算出Feature Map中所有元素的值，计算过程：</p>\n<center><img src=\"/image/DL/2/3.gif\" alt=\"CNN\"></center>  \n\n<hr>\n<p>用例来自 - <a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\">零基础入门深度学习(4) - 卷积神经网络</a>   </p>\n<p>推荐阅读 - <a href=\"https://zh.gluon.ai/cnn-scratch.html\" title=\"Title\">Gluon - 卷积神经网络</a> </p>\n<h5 id=\"1-1-1-卷积层输出值的计算\"><a href=\"#1-1-1-卷积层输出值的计算\" class=\"headerlink\" title=\"1.1.1. 卷积层输出值的计算\"></a>1.1.1. 卷积层输出值的计算</h5><p>图像大小、步幅和卷积后的Feature Map大小，满足下面的关系：</p>\n<p>\\begin{align}<br>W_2 &amp;= (W_1 - F + 2P)/S + 1\\qquad\\\\<br>H_2 &amp;= (H_1 - F + 2P)/S + 1\\qquad<br>\\end{align}</p>\n<p>在上面两个公式中：W2是卷积后Feature Map的宽度；W1是卷积前图像的宽度；F是filter的宽度；P是Zero Padding数量，Zero Padding是指在原始图像周围补几圈0，如果P的值是1，那么就补1圈0；S是步幅；H2是卷积后Feature Map的高度；H1是卷积前图像的宽度。</p>\n<p>下面的显示包含两个filter的卷积层的计算。可以看到7<em>7</em>3输入，经过两个3<em>3</em>3filter的卷积(步幅为2)，得到了3<em>3</em>2的输出。另外也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。</p>\n<center><img src=\"/image/DL/2/4.gif\" alt=\"CNN\"></center>  \n\n<h5 id=\"1-1-2-池化层输出值的计算\"><a href=\"#1-1-2-池化层输出值的计算\" class=\"headerlink\" title=\"1.1.2. 池化层输出值的计算\"></a>1.1.2. 池化层输出值的计算</h5><p>池化层主要的作用是下采样，通过去掉Feature Map中不重要的样本，进一步减少参数数量。池化的方法最常用的是Max Pooling，即样本中取最大值，作为采样后的样本值。下例是2*2 max pooling：</p>\n<center><img src=\"/image/DL/2/5.png\" alt=\"CNN\"></center>  \n\n<hr>\n<p>还有Mean Pooling，即取各样本的平均值。</p>\n<h4 id=\"1-2-卷积公式的计算\"><a href=\"#1-2-卷积公式的计算\" class=\"headerlink\" title=\"1.2. 卷积公式的计算\"></a>1.2. 卷积公式的计算</h4><p>用X[i,j]表示图像的第i行第j列元素；对filter的每个权重进行编号，用W[m,n]表示第m行第n列权重，用Wb表示filter的偏置项；对Feature Map的每个元素进行编号，a[i,j]用表示Feature Map的第i行第j列元素；用f表示激活函数。</p>\n<p>$$a_{i,j}=f(\\sum_{m=0}^{2}\\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_b)$$</p>\n<p>深度大于1的卷积计算公式：  </p>\n<p>$$a_{i,j}=f(\\sum_{d=0}^{D-1}\\sum_{m=0}^{F-1}\\sum_{n=0}^{F-1}w_{d,m,n}x_{d,i+m,j+n}+w_b)$$</p>\n<h4 id=\"1-3-卷积神经网络的训练\"><a href=\"#1-3-卷积神经网络的训练\" class=\"headerlink\" title=\"1.3. 卷积神经网络的训练\"></a>1.3. 卷积神经网络的训练</h4><p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w</p>\n<h4 id=\"1-4-关于权重w与偏置项b的初始化\"><a href=\"#1-4-关于权重w与偏置项b的初始化\" class=\"headerlink\" title=\"1.4. 关于权重w与偏置项b的初始化\"></a>1.4. 关于权重w与偏置项b的初始化</h4><h5 id=\"1-4-1-权重w的初始化\"><a href=\"#1-4-1-权重w的初始化\" class=\"headerlink\" title=\"1.4.1. 权重w的初始化\"></a>1.4.1. 权重w的初始化</h5><p>① 均匀分布：tf.random_uniform()</p>\n<p>② 正太分布：tf.random_normal()</p>\n<p>③ 从截断的正态取值–横轴区间（μ-2σ，μ+2σ）95%面积：tf.truncated_normal()</p>\n<p>④ 还有一个经验公式：从[-y, y]取值，其中 $y=1/\\sqrt{n}$</p>\n<p>根据经验使用③效果较佳…</p>\n<h5 id=\"1-4-2-偏置项b的初始化\"><a href=\"#1-4-2-偏置项b的初始化\" class=\"headerlink\" title=\"1.4.2. 偏置项b的初始化\"></a>1.4.2. 偏置项b的初始化</h5><p>一般使用tf.zeros()来初始化为零值. </p>\n<p>阅读参考 - <a href=\"https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks\" title=\"Title\">Role of Bias in Neural Networks</a></p>\n<p>“a bias value allows you to shift the activation function to the left or right”</p>\n<h3 id=\"2-基于TensorFlow的实现（CNN-with-TF）\"><a href=\"#2-基于TensorFlow的实现（CNN-with-TF）\" class=\"headerlink\" title=\"2. 基于TensorFlow的实现（CNN with TF）\"></a>2. 基于TensorFlow的实现（CNN with TF）</h3><h4 id=\"2-1-TF卷积神经网络的基本实现\"><a href=\"#2-1-TF卷积神经网络的基本实现\" class=\"headerlink\" title=\"2.1. TF卷积神经网络的基本实现\"></a>2.1. TF卷积神经网络的基本实现</h4><h5 id=\"2-1-1-卷积层\"><a href=\"#2-1-1-卷积层\" class=\"headerlink\" title=\"2.1.1. 卷积层\"></a>2.1.1. 卷积层</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Input/Image</span></div><div class=\"line\">input = tf.placeholder(</div><div class=\"line\">    tf.float32,</div><div class=\"line\">    shape=[<span class=\"keyword\">None</span>, image_height, image_width, color_channels])</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Weight and bias</span></div><div class=\"line\">weight = tf.Variable(tf.truncated_normal(</div><div class=\"line\">    [filter_size_height, filter_size_width, color_channels, k_output]))</div><div class=\"line\">bias = tf.Variable(tf.zeros(k_output))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Apply Convolution</span></div><div class=\"line\">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>], padding=<span class=\"string\">'SAME'</span>)</div><div class=\"line\"><span class=\"comment\"># Add bias</span></div><div class=\"line\">conv_layer = tf.nn.bias_add(conv_layer, bias)</div><div class=\"line\"><span class=\"comment\"># Apply activation function</span></div><div class=\"line\">conv_layer = tf.nn.relu(conv_layer)</div></pre></td></tr></table></figure>\n<p>weights 作为滤波器，[1, 2, 2, 1] 作为 strides。TensorFlow 对每一个 input 维度使用一个单独的 stride 参数，[batch, input_height, input_width, input_channels]。通常把 batch 和 input_channels （strides 序列中的第一个第四个）的 stride 设为 1。</p>\n<h5 id=\"2-1-2-池化层\"><a href=\"#2-1-2-池化层\" class=\"headerlink\" title=\"2.1.2. 池化层\"></a>2.1.2. 池化层</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\"># Apply Max Pooling</span></div><div class=\"line\">conv_layer = tf.nn.max_pool(</div><div class=\"line\">    conv_layer,</div><div class=\"line\">    ksize=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>],</div><div class=\"line\">    strides=[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>],</div><div class=\"line\">    padding=<span class=\"string\">'SAME'</span>)</div></pre></td></tr></table></figure>\n<p>tf.nn.max_pool() 函数实现最大池化时， ksize参数是滤波器大小，strides参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。</p>\n<p>ksize 和 strides 参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 ([batch, height, width, channels])，对 ksize 和 strides 来说，batch 和 channel 通常都设置成 1。</p>\n<h4 id=\"2-2-TF卷积神经网络的实现例\"><a href=\"#2-2-TF卷积神经网络的实现例\" class=\"headerlink\" title=\"2.2. TF卷积神经网络的实现例\"></a>2.2. TF卷积神经网络的实现例</h4><p>对 <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" title=\"Title\">CIFAR-10 数据集</a> 中的图片进行分类。<br>该数据集包含飞机、猫狗和其他物体。先预处理这些图片，然后用所有样本训练一个卷积神经网络。图片需要标准化（normalized），标签需要采用 one-hot 编码。构建卷积（convolution）、最大池化（max pooling）、丢弃（dropout）和完全连接（fully connected）的层。最后在样本图片上看到神经网络的预测结果。</p>\n<p>具体实现不赘述，过程直接看Github，吾辈准确率不高，才57%左右 - <a href=\"https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb\" title=\"Title\">Github Link</a>   </p>\n<h4 id=\"2-3-卷积神经网络内部一窥\"><a href=\"#2-3-卷积神经网络内部一窥\" class=\"headerlink\" title=\"2.3. 卷积神经网络内部一窥\"></a>2.3. 卷积神经网络内部一窥</h4><hr>\n<center><img src=\"/image/DL/2/CNN.jpg\" alt=\"CNN\"></center> \n\n<hr>\n<p>瑞尔森大学的Adam Harley创建了一个交互式视觉化模型，能够帮助解释卷积神经网络内部每一层是如何工作的：<a href=\"http://scs.ryerson.ca/~aharley/vis/conv/\" title=\"Title\">Link</a>   </p>\n<h3 id=\"3-自编码器（Autoencoder）\"><a href=\"#3-自编码器（Autoencoder）\" class=\"headerlink\" title=\"3. 自编码器（Autoencoder）\"></a>3. 自编码器（Autoencoder）</h3><p>自编码器（Autoencoder）由于处理过程中有单元减少，解压缩效果不如MP3与JPEG，但是在图像去噪（Denoising）与降维（dimensionality reduction）方面取得不错的效果。</p>\n<center><img src=\"/image/DL/2/autoencoder_1.png\" alt=\"CNN\"></center> \n\n<hr>\n<h4 id=\"3-1-使用dense实现\"><a href=\"#3-1-使用dense实现\" class=\"headerlink\" title=\"3.1. 使用dense实现\"></a>3.1. 使用dense实现</h4><p>Encoder：使用tf.layers.dense与relu激活函数实现<br>Decoder：使用tf.layers.dense与sigmoid激活函数实现</p>\n<ul>\n<li>Autoencoder用例实现1 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Simple_Autoencoder_Solution.ipynb\" title=\"Title\">Github Link</a> </li>\n</ul>\n<h4 id=\"3-2-使用卷积实现\"><a href=\"#3-2-使用卷积实现\" class=\"headerlink\" title=\"3.2. 使用卷积实现\"></a>3.2. 使用卷积实现</h4><p>Encoder：使用tf.layers.conv2d与tf.layers.max_pooling2d（下采样）实现<br>Decoder：使用tf.layers.conv2d与tf.image.resize_nearest_neighbor（上采样）实现</p>\n<ul>\n<li>Autoencoder用例实现2 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/autoencoder/Convolutional_Autoencoder_Solution.ipynb\" title=\"Title\">Github Link</a> </li>\n</ul>\n<p>推荐：Deconvolution and Checkerboard Artifacts <a href=\"https://distill.pub/2016/deconv-checkerboard/\" title=\"Title\">Distill</a> </p>\n<h3 id=\"4-迁移学习（Transfer-Learning）\"><a href=\"#4-迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"4. 迁移学习（Transfer Learning）\"></a>4. 迁移学习（Transfer Learning）</h3><p>迁移学习（Transfer Learning）：在实际中，通常会使用预训练模型（比如 AlexNet，VGGNet，Google Inception Net，ResNet），将最后的几个全连接层（基本作用是分类器），替换成自己的分类器，再进行训练与测试。  </p>\n<p>介绍其中两种CNN网络模型 AlexNet 与 VGGNet：</p>\n<h4 id=\"4-1-AlexNet\"><a href=\"#4-1-AlexNet\" class=\"headerlink\" title=\"4.1 AlexNet\"></a>4.1 AlexNet</h4><p>AlexNet是由SuperVision设计，包括成员Alex Krizhevsky，Geoffrey Hinton，Ilya Sutskeve设计的CNN网络模型。该模型在2012年ImageNet Large Scale Visual Recognition Challenge的比赛中以15.3%错误率获得冠军，领先第二名10.8个百分点。</p>\n<p>输入要求256(图像大小)，均值是256的，减均值后再crop到227(输入图像大小)   </p>\n<p>参考 - <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" title=\"Title\">AlexNet</a> </p>\n<h4 id=\"4-2-VGGNet\"><a href=\"#4-2-VGGNet\" class=\"headerlink\" title=\"4.2 VGGNet\"></a>4.2 VGGNet</h4><p>VGGNet是牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发的的深度卷积神经网络，在ILSVRC 2014上取得了第二名的成绩，将Top-5错误率降到7.3%。</p>\n<p>输入要求256(图像大小)，均值是256的，减均值后再crop到224(输入图像大小)  </p>\n<p>参考 - <a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\" title=\"Title\">Visual Geometry Group</a> </p>\n<center><img src=\"/image/DL/2/cnnarchitecture.jpg\" alt=\"cnnarchitecture\"></center>  \n\n<hr>\n<ul>\n<li>VGGNet使用例 - Udacity <a href=\"https://github.com/udacity/cn-deep-learning/blob/master/tutorials/transfer-learning/Transfer_Learning_Solution.ipynb\" title=\"Title\">Github Link</a> </li>\n</ul>\n<hr>\n<h3 id=\"5-目标检测（Object-Detection）\"><a href=\"#5-目标检测（Object-Detection）\" class=\"headerlink\" title=\"5. 目标检测（Object Detection）\"></a>5. 目标检测（Object Detection）</h3><p>Object Detection是在给定的图片中精确找到物体所在位置，并标注出物体的类别。<br>Object Detection要解决的问题就是物体在哪里Where，是什么What这整个流程的问题。</p>\n<p>但是在现实生活中，问题不是那么简单，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，而且物体的类别还有多种。</p>\n<p>关于目标检测区域建议的演化历史：- <a href=\"https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\" title=\"Title\">A Brief History of CNNs</a></p>\n<p>CNN-&gt;RCNN-&gt;Fast-RCNN-&gt;Faster-RCNN-&gt;Mask-RCNN-&gt;…</p>\n<hr>\n<center><img src=\"/image/DL/2/Mask-RCNN.png\" alt=\"CNN\"></center> \n\n<hr>\n<ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27473413\" title=\"Title\">RCNN</a></p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27582096\" title=\"Title\">Fast-RCNN</a></p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/27988828\" title=\"Title\">Faster-RCNN</a></p>\n</li>\n<li><p><a href=\"https://arxiv.org/abs/1703.06870\" title=\"Title\">Mask-RCNN</a></p>\n</li>\n</ul>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><a href=\"https://github.com/HJTSO/image-classification/blob/master/dlnd_image_classification.ipynb\" title=\"Title\">Github Link</a> </li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zh.gluon.ai/cnn-scratch.html\" title=\"Title\">Gluon - 卷积神经网络</a> </p>\n</li>\n<li><p><a href=\"http://tensorflow.classcat.com/2017/05/08/tensorflow-network-in-network/\" title=\"Title\">Network-In-Network で CIFAR-10 精度 90%</a> </p>\n</li>\n<li><p><a href=\"https://www.zybuluo.com/hanbingtao/note/485480\" title=\"Title\">零基础入门深度学习(4) - 卷积神经网络</a> </p>\n</li>\n<li><p><a href=\"https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\" title=\"Title\">不同的层在CNN网络的作用原理</a> </p>\n</li>\n</ul>\n"},{"title":"Reinforcement Learning笔记4-Monte Carlo","lang":"zh","date":"2017-11-14T09:18:56.000Z","_content":"\n#### Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）\n\n蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。\n\n一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正比的。而采用蒙特卡罗方法是怎么计算的呢？首先你把图形放到一个已知面积的方框内，然后假想你有一些豆子，把豆子均匀地朝这个方框内撒，散好后数这个图形之中有多少颗豆子，再根据图形内外豆子的比例来计算面积。当你的豆子越小，撒的越多的时候，结果就越精确。\n\n强化学习方法分类：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/0-1.png)</center> \n\n-------------------------------------\n\n若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。\n\n无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。\n\n### 1. 蒙特卡罗方法（Monte Carlo）\n\n蒙特卡罗方法仅仅需要经验就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。\n\n要注意的是，仅将蒙特卡罗方法定义在episode task上，所谓的episode task就是指不管采取哪种策略π，都会在有限时间内到达终止状态并获得回报的任务。比如玩棋类游戏，在有限步数以后总能达到输赢或者平局的结果并获得相应回报。\n\n那么什么是经验呢？经验其实就是训练样本。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报，也就是状态值函数Vπ(s)了。蒙特卡罗方法就是依靠样本的平均回报来解决增强学习问题的。\n\n### 2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）\n\n内容来自：[强化学习入门 第三讲 蒙特卡罗方法](https://zhuanlan.zhihu.com/p/25743759 \"Title\") \n\n**探索的必要性：**状态值函数和行为值函数的计算实际上是计算返回值的期望。动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用 **经验平均** 代替随机变量的期望。\n\n如何获得充足的经验是无模型强化学习的核心所在。\n\n在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是探索性初始化。\n\n**探索性初始化：**是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。\n\n\n**蒙特卡罗策略改进：**蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态s ，通过最大化动作值函数，来进行策略的改进。即：\n\n\\begin{align} \n\\pi\\left(s\\right)=\\underset{a}{arg\\max\\textrm{\\ }}q\\left(s,a\\right)\n\\end{align}\n\n递增计算平均的方法：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-1.svg)</center> \n\n-------------------------------------\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-2.png)</center> \n\n-------------------------------------\n\n在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。\n\n● 如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？\n\n答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。\n\n● 可是如何精心地设计探索策略？符合要求的探索策略是什么样的？\n\n答案是：策略必须是温和的，即对所有的状态s 和a 满足：\n\n\\begin{align} \n\\pi\\left(a|s\\right)>0 \n\\end{align}\n\n也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是\n\n\\begin{align} \n\\varepsilon -soft\n\\end{align}\n\n即：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-3.svg)</center> \n\n-------------------------------------\n\n根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.\n\n● 若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。\n\n● 若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。\n\n\n**まとめ：** 以上大概讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。\n\n### 3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）\n\n内容来自：[知乎：蒙特卡罗算法是什么？- 孙天齐回答](https://www.zhihu.com/question/20254139/answer/33572009 \"Title\") \n\n这里把随机算法分成两类：\n\n- 蒙特卡罗算法：采样越多，越近似最优解；\n\n- 拉斯维加斯算法：采样越多，越有机会找到最优解；\n\n● 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。\n\n● 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。\n\n这两个词并不深奥，它只是概括了随机算法的特性，算法本身可能复杂，也可能简单。这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。\n\n对于机器围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以ZEN涉及的随机算法，肯定是蒙特卡罗式的。机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百（国际象棋只有几十）。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习入门 第三讲 蒙特卡罗方法](https://zhuanlan.zhihu.com/p/25743759 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n\n- [蒙特卡罗方法(Monte Carlo Methods)](http://www.cnblogs.com/jinxulin/p/3560737.html \"Title\") \n","source":"_posts/zh/Reinforcement Learning笔记4-Monte Carlo.md","raw":"\n---\ntitle: Reinforcement Learning笔记4-Monte Carlo\nlang: zh\ndate: 2017-11-14 18:18:56\ntags: Reinforcement Learning\n---\n\n#### Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）\n\n蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。\n\n一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正比的。而采用蒙特卡罗方法是怎么计算的呢？首先你把图形放到一个已知面积的方框内，然后假想你有一些豆子，把豆子均匀地朝这个方框内撒，散好后数这个图形之中有多少颗豆子，再根据图形内外豆子的比例来计算面积。当你的豆子越小，撒的越多的时候，结果就越精确。\n\n强化学习方法分类：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/0-1.png)</center> \n\n-------------------------------------\n\n若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。\n\n无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。\n\n### 1. 蒙特卡罗方法（Monte Carlo）\n\n蒙特卡罗方法仅仅需要经验就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。\n\n要注意的是，仅将蒙特卡罗方法定义在episode task上，所谓的episode task就是指不管采取哪种策略π，都会在有限时间内到达终止状态并获得回报的任务。比如玩棋类游戏，在有限步数以后总能达到输赢或者平局的结果并获得相应回报。\n\n那么什么是经验呢？经验其实就是训练样本。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报，也就是状态值函数Vπ(s)了。蒙特卡罗方法就是依靠样本的平均回报来解决增强学习问题的。\n\n### 2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）\n\n内容来自：[强化学习入门 第三讲 蒙特卡罗方法](https://zhuanlan.zhihu.com/p/25743759 \"Title\") \n\n**探索的必要性：**状态值函数和行为值函数的计算实际上是计算返回值的期望。动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用 **经验平均** 代替随机变量的期望。\n\n如何获得充足的经验是无模型强化学习的核心所在。\n\n在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是探索性初始化。\n\n**探索性初始化：**是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。\n\n\n**蒙特卡罗策略改进：**蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态s ，通过最大化动作值函数，来进行策略的改进。即：\n\n\\begin{align} \n\\pi\\left(s\\right)=\\underset{a}{arg\\max\\textrm{\\ }}q\\left(s,a\\right)\n\\end{align}\n\n递增计算平均的方法：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-1.svg)</center> \n\n-------------------------------------\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-2.png)</center> \n\n-------------------------------------\n\n在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。\n\n● 如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？\n\n答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。\n\n● 可是如何精心地设计探索策略？符合要求的探索策略是什么样的？\n\n答案是：策略必须是温和的，即对所有的状态s 和a 满足：\n\n\\begin{align} \n\\pi\\left(a|s\\right)>0 \n\\end{align}\n\n也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是\n\n\\begin{align} \n\\varepsilon -soft\n\\end{align}\n\n即：\n\n-------------------------------------\n\n<center>![RL](/image/RL/4/2-3.svg)</center> \n\n-------------------------------------\n\n根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.\n\n● 若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。\n\n● 若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。\n\n\n**まとめ：** 以上大概讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。\n\n### 3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）\n\n内容来自：[知乎：蒙特卡罗算法是什么？- 孙天齐回答](https://www.zhihu.com/question/20254139/answer/33572009 \"Title\") \n\n这里把随机算法分成两类：\n\n- 蒙特卡罗算法：采样越多，越近似最优解；\n\n- 拉斯维加斯算法：采样越多，越有机会找到最优解；\n\n● 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。\n\n● 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。\n\n这两个词并不深奥，它只是概括了随机算法的特性，算法本身可能复杂，也可能简单。这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。\n\n对于机器围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以ZEN涉及的随机算法，肯定是蒙特卡罗式的。机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百（国际象棋只有几十）。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。\n\n\n### 参考资料（Reference）\n\n- [Dissecting Reinforcement Learning-Part.1](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html \"Title\") \n\n- [ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312 \"Title\") \n\n- [强化学习入门 第三讲 蒙特卡罗方法](https://zhuanlan.zhihu.com/p/25743759 \"Title\") \n\n- [强化学习知识整理](https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&utm_medium=referral \"Title\") \n\n- [蒙特卡罗方法(Monte Carlo Methods)](http://www.cnblogs.com/jinxulin/p/3560737.html \"Title\") \n","slug":"zh-Reinforcement-Learning笔记4-Monte-Carlo","published":1,"updated":"2018-01-01T13:15:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zh0027og64gc9fy1zi","content":"<h4 id=\"Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）\"><a href=\"#Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）\" class=\"headerlink\" title=\"Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）\"></a>Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）</h4><p>蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。</p>\n<p>一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正比的。而采用蒙特卡罗方法是怎么计算的呢？首先你把图形放到一个已知面积的方框内，然后假想你有一些豆子，把豆子均匀地朝这个方框内撒，散好后数这个图形之中有多少颗豆子，再根据图形内外豆子的比例来计算面积。当你的豆子越小，撒的越多的时候，结果就越精确。</p>\n<p>强化学习方法分类：</p>\n<hr>\n<center><img src=\"/image/RL/4/0-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。</p>\n<p>无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。</p>\n<h3 id=\"1-蒙特卡罗方法（Monte-Carlo）\"><a href=\"#1-蒙特卡罗方法（Monte-Carlo）\" class=\"headerlink\" title=\"1. 蒙特卡罗方法（Monte Carlo）\"></a>1. 蒙特卡罗方法（Monte Carlo）</h3><p>蒙特卡罗方法仅仅需要经验就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。</p>\n<p>要注意的是，仅将蒙特卡罗方法定义在episode task上，所谓的episode task就是指不管采取哪种策略π，都会在有限时间内到达终止状态并获得回报的任务。比如玩棋类游戏，在有限步数以后总能达到输赢或者平局的结果并获得相应回报。</p>\n<p>那么什么是经验呢？经验其实就是训练样本。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报，也就是状态值函数Vπ(s)了。蒙特卡罗方法就是依靠样本的平均回报来解决增强学习问题的。</p>\n<h3 id=\"2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）\"><a href=\"#2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）\" class=\"headerlink\" title=\"2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）\"></a>2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）</h3><p>内容来自：<a href=\"https://zhuanlan.zhihu.com/p/25743759\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习入门 第三讲 蒙特卡罗方法</a> </p>\n<p><strong>探索的必要性：</strong>状态值函数和行为值函数的计算实际上是计算返回值的期望。动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用 <strong>经验平均</strong> 代替随机变量的期望。</p>\n<p>如何获得充足的经验是无模型强化学习的核心所在。</p>\n<p>在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是探索性初始化。</p>\n<p><strong>探索性初始化：</strong>是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。</p>\n<p><strong>蒙特卡罗策略改进：</strong>蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态s ，通过最大化动作值函数，来进行策略的改进。即：</p>\n<p>\\begin{align}<br>\\pi\\left(s\\right)=\\underset{a}{arg\\max\\textrm{ }}q\\left(s,a\\right)<br>\\end{align}</p>\n<p>递增计算平均的方法：</p>\n<hr>\n<center><img src=\"/image/RL/4/2-1.svg\" alt=\"RL\"></center> \n\n<hr>\n<hr>\n<center><img src=\"/image/RL/4/2-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。</p>\n<p>● 如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？</p>\n<p>答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。</p>\n<p>● 可是如何精心地设计探索策略？符合要求的探索策略是什么样的？</p>\n<p>答案是：策略必须是温和的，即对所有的状态s 和a 满足：</p>\n<p>\\begin{align}<br>\\pi\\left(a|s\\right)&gt;0<br>\\end{align}</p>\n<p>也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是</p>\n<p>\\begin{align}<br>\\varepsilon -soft<br>\\end{align}</p>\n<p>即：</p>\n<hr>\n<center><img src=\"/image/RL/4/2-3.svg\" alt=\"RL\"></center> \n\n<hr>\n<p>根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.</p>\n<p>● 若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。</p>\n<p>● 若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。</p>\n<p><strong>まとめ：</strong> 以上大概讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。</p>\n<h3 id=\"3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）\"><a href=\"#3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）\" class=\"headerlink\" title=\"3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）\"></a>3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）</h3><p>内容来自：<a href=\"https://www.zhihu.com/question/20254139/answer/33572009\" title=\"Title\" target=\"_blank\" rel=\"external\">知乎：蒙特卡罗算法是什么？- 孙天齐回答</a> </p>\n<p>这里把随机算法分成两类：</p>\n<ul>\n<li><p>蒙特卡罗算法：采样越多，越近似最优解；</p>\n</li>\n<li><p>拉斯维加斯算法：采样越多，越有机会找到最优解；</p>\n</li>\n</ul>\n<p>● 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。</p>\n<p>● 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p>\n<p>这两个词并不深奥，它只是概括了随机算法的特性，算法本身可能复杂，也可能简单。这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。</p>\n<p>对于机器围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以ZEN涉及的随机算法，肯定是蒙特卡罗式的。机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百（国际象棋只有几十）。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\" target=\"_blank\" rel=\"external\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\" target=\"_blank\" rel=\"external\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25743759\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习入门 第三讲 蒙特卡罗方法</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\" target=\"_blank\" rel=\"external\">强化学习知识整理</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/3560737.html\" title=\"Title\" target=\"_blank\" rel=\"external\">蒙特卡罗方法(Monte Carlo Methods)</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h4 id=\"Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）\"><a href=\"#Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）\" class=\"headerlink\" title=\"Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）\"></a>Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）</h4><p>蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。</p>\n<p>一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正比的。而采用蒙特卡罗方法是怎么计算的呢？首先你把图形放到一个已知面积的方框内，然后假想你有一些豆子，把豆子均匀地朝这个方框内撒，散好后数这个图形之中有多少颗豆子，再根据图形内外豆子的比例来计算面积。当你的豆子越小，撒的越多的时候，结果就越精确。</p>\n<p>强化学习方法分类：</p>\n<hr>\n<center><img src=\"/image/RL/4/0-1.png\" alt=\"RL\"></center> \n\n<hr>\n<p>若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。</p>\n<p>无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。</p>\n<h3 id=\"1-蒙特卡罗方法（Monte-Carlo）\"><a href=\"#1-蒙特卡罗方法（Monte-Carlo）\" class=\"headerlink\" title=\"1. 蒙特卡罗方法（Monte Carlo）\"></a>1. 蒙特卡罗方法（Monte Carlo）</h3><p>蒙特卡罗方法仅仅需要经验就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。</p>\n<p>要注意的是，仅将蒙特卡罗方法定义在episode task上，所谓的episode task就是指不管采取哪种策略π，都会在有限时间内到达终止状态并获得回报的任务。比如玩棋类游戏，在有限步数以后总能达到输赢或者平局的结果并获得相应回报。</p>\n<p>那么什么是经验呢？经验其实就是训练样本。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报，也就是状态值函数Vπ(s)了。蒙特卡罗方法就是依靠样本的平均回报来解决增强学习问题的。</p>\n<h3 id=\"2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）\"><a href=\"#2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）\" class=\"headerlink\" title=\"2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）\"></a>2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）</h3><p>内容来自：<a href=\"https://zhuanlan.zhihu.com/p/25743759\" title=\"Title\">强化学习入门 第三讲 蒙特卡罗方法</a> </p>\n<p><strong>探索的必要性：</strong>状态值函数和行为值函数的计算实际上是计算返回值的期望。动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用 <strong>经验平均</strong> 代替随机变量的期望。</p>\n<p>如何获得充足的经验是无模型强化学习的核心所在。</p>\n<p>在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是探索性初始化。</p>\n<p><strong>探索性初始化：</strong>是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。</p>\n<p><strong>蒙特卡罗策略改进：</strong>蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态s ，通过最大化动作值函数，来进行策略的改进。即：</p>\n<p>\\begin{align}<br>\\pi\\left(s\\right)=\\underset{a}{arg\\max\\textrm{ }}q\\left(s,a\\right)<br>\\end{align}</p>\n<p>递增计算平均的方法：</p>\n<hr>\n<center><img src=\"/image/RL/4/2-1.svg\" alt=\"RL\"></center> \n\n<hr>\n<hr>\n<center><img src=\"/image/RL/4/2-2.png\" alt=\"RL\"></center> \n\n<hr>\n<p>在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。</p>\n<p>● 如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？</p>\n<p>答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。</p>\n<p>● 可是如何精心地设计探索策略？符合要求的探索策略是什么样的？</p>\n<p>答案是：策略必须是温和的，即对所有的状态s 和a 满足：</p>\n<p>\\begin{align}<br>\\pi\\left(a|s\\right)&gt;0<br>\\end{align}</p>\n<p>也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是</p>\n<p>\\begin{align}<br>\\varepsilon -soft<br>\\end{align}</p>\n<p>即：</p>\n<hr>\n<center><img src=\"/image/RL/4/2-3.svg\" alt=\"RL\"></center> \n\n<hr>\n<p>根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.</p>\n<p>● 若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。</p>\n<p>● 若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。</p>\n<p><strong>まとめ：</strong> 以上大概讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。</p>\n<h3 id=\"3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）\"><a href=\"#3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）\" class=\"headerlink\" title=\"3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）\"></a>3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）</h3><p>内容来自：<a href=\"https://www.zhihu.com/question/20254139/answer/33572009\" title=\"Title\">知乎：蒙特卡罗算法是什么？- 孙天齐回答</a> </p>\n<p>这里把随机算法分成两类：</p>\n<ul>\n<li><p>蒙特卡罗算法：采样越多，越近似最优解；</p>\n</li>\n<li><p>拉斯维加斯算法：采样越多，越有机会找到最优解；</p>\n</li>\n</ul>\n<p>● 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。</p>\n<p>● 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p>\n<p>这两个词并不深奥，它只是概括了随机算法的特性，算法本身可能复杂，也可能简单。这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。</p>\n<p>对于机器围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以ZEN涉及的随机算法，肯定是蒙特卡罗式的。机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百（国际象棋只有几十）。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。</p>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\" title=\"Title\">Dissecting Reinforcement Learning-Part.1</a> </p>\n</li>\n<li><p><a href=\"https://qiita.com/icoxfog417/items/242439ecd1a477ece312\" title=\"Title\">ゼロからDeepまで学ぶ強化学習</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25743759\" title=\"Title\">强化学习入门 第三讲 蒙特卡罗方法</a> </p>\n</li>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral\" title=\"Title\">强化学习知识整理</a> </p>\n</li>\n<li><p><a href=\"http://www.cnblogs.com/jinxulin/p/3560737.html\" title=\"Title\">蒙特卡罗方法(Monte Carlo Methods)</a> </p>\n</li>\n</ul>\n"},{"title":"Deep learning笔记5-GAN生成式对抗网络","lang":"zh","date":"2017-11-01T09:18:56.000Z","_content":"\n### 1. 生成式对抗网络（GAN）\n\nGAN（Generative Adversarial Network）的思想：生成器和鉴别器两个网络彼此博弈。\n\n● 生成器的目标是生成一个对象，并使其看起来和真的一样。\n\n● 鉴别器的目标就是找到生成出的结果和真实图像之间的差异。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-1.jpg)</center> \n\n-------------------------------------\n\n内容部分来自：[はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n訓練データを学習し、それらのデータと似たような新しいデータを生成するモデルのことを生成モデルと呼びます。別の言い方をすると、訓練データの分布と生成データの分布が一致するように学習していくようなモデルです。\n\nGANではgeneratorとdiscriminatorという２つのネットワークが登場します。Generatorは訓練データと同じようなデータを生成しようとします。一方、discriminatorはデータが訓練データから来たものか、それとも生成モデルから来たものかを識別します。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-2.jpg)</center> \n\n-------------------------------------\n\n如上图所示，生成对抗网络会训练并更新判别分布，更新判别器后就能将数据真实分布从生成分布中判别出来。经过若干次训练后，如果 G 和 D 有足够的复杂度，那么它们就会到达一个均衡点。这个时候生成器的概率密度函数等于真实数据的概率密度函数，也即生成的数据和真实数据是一样的。在均衡点上 D 和 G 都不能得到进一步提升，并且判别器无法判断数据到底是来自真实样本还是伪造的数据，即 D（x）= 1/2。\n\nGAN学习的表征可用于多种应用，包括图像合成、语义图像编辑、风格迁移、图像超分辨率技术和分类。\n\neg. ベッドルームの画像のデータセットを使って学習した結果です。一見本物と見間違ってしまいそうなレベルの画像を生成できていることが分かります。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-3.png)</center> \n\n-------------------------------------\n\neg. Word2Vecという単語ベクトルで「王様」-「男」+「女」=「女王」という演算ができることは有名ですが、GANにおける入力であるzzベクトルを使っても同様の演算を行うことができます。下の例では、「サングラスをかけた男」-「男」+「女」=「サングラスをかけた女」という演算を行っています。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-4.png)</center> \n\n-------------------------------------\n\nGAN的类别 - 内容来自：[生成对抗网络综述：从架构到训练技巧](https://zhuanlan.zhihu.com/p/30346797 \"Title\") \n\n#### 1.1. 全连接 GAN\n\n首个 GAN 架构在生成器与鉴别器上皆使用全连接神经网络。这种架构类型被应用于相对简单的图像数据库，即 MNIST（手写数字）、CIFAR-10（自然图像）和多伦多人脸数据集（TFD）。\n\n#### 1.2.  卷积 GAN\n\n● LAPGAN拉普拉斯金字塔形对抗网络（Laplacian pyramid GAN）\n\n真值图像本身被分解成拉普拉斯金字塔（Laplacian pyramid），并且条件性卷积 GAN 被训练在给定上一层的情况下生成每一层。\n\n● DCGAN（Deep Convolutional GAN）\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-5.png)</center> \n\n-------------------------------------\n\nDCGAN允许训练一对深度卷积生成器和判别器网络。在训练中使用带步长的卷积（strided convolution）和小步长卷积（fractionally-strided convolution），并在训练中学习空间下采样和上采样算子。\n\n#### 1.3. 条件 GAN\n\n通过将生成器和判别器改造成条件类（class-conditional）而将（2D）GAN 框架扩展成条件设置。条件 GNN 的优势在于可以对多形式的数据生成提供更好的表征。条件 GAN 和 InfoGAN[16] 是平行的，它可以将噪声源分解为不可压缩源和一个「隐编码」（latent code），并可以通过最大化隐编码和生成器之间的交互信息而发现变化的隐藏因子。这个隐编码可用于在完全无监督的数据中发现目标类，即使这个隐编码是不明确的。由 InfoGAN 学到的表征看起来像是具备语义特征的，可以处理图貌中的复杂纠缠因素（包括姿势变化、光照和面部图像的情绪内容）。\n\n#### 1.4. GAN 推断模型\n\nGAN 的初始形式无法将给定的输入 x 映射为隐空间中的向量（在 GAN 的文献中，这通常被称为一种推断机制）。人们提出了几种反转预训练 GAN 的生成器的技术，比如各自独立提出的对抗性学习推断（Adversarially Learned Inference，ALI）和双向 GAN（Bidirectional GANs），它们能提供简单而有效的扩展，通过加入一个推断网络，使判别器共同测试数据空间和隐空间。\n\n这种形式下的生成器由两个网络组成：即编码器（推断网络）和解码器。它们同时被训练用于欺骗判别器。而判别器将接收到一个向量对（x,z），并决定其是否包含一个真实图像以及其编码，或者一个生成的图像样本以及相关的生成器的隐空间输入。\n\n#### 1.5. AAE 对抗自编码器\n\n自编码器是由编码器和解码器组成的网络，学习将数据映射到内部隐表征中，再映射出来，即从数据空间中学习将图像（或其它）通过编码映射到隐空间中，再通过解码从隐空间映射回数据空间。这两个映射形成了一种重构运算，而这两个映射将被训练直到重构图像尽可能的接近初始图像。\n\n### 2. 生成式对抗网络的训练（GAN Training）\n\n生成对抗网络（GAN）提供了一种不需要大量标注训练数据就能学习深度表征的方式。它们通过反向传播算法分别更新两个网络以执行竞争性学习而达到训练目的。\n\n#### 2.1. 训练 \n\n训练的代价由一个价值函数 V(G,D) 评估，其包含了生成器和判别器的参数。\n\n训练过程可表示如下：\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/2-1.png)</center> \n\n-------------------------------------\n\n训练过程中，其中一个模型的参数被更新，同时另一个模型的参数固定不变。\n\n算法描述：[Goodfellow et al. (2014)](https://arxiv.org/abs/1406.2661 \"Title\") \nより引用\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/2-2.png)</center> \n\n-------------------------------------\n\n推荐阅读 - [机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n推荐阅读 - [はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n#### 2.2. 训练技巧\n\n用于图像生成的 GAN 训练的第一个重大改进是 Radford et al.提出的 DCGAN 架构。具体到训练中，研究者推荐在两种网络中使用批量归一化，以稳定深层模型中的训练。另一个建议是最小化用于提升深层模型训练可行性的全连接层的数量。最后，Radford et al.认为在判别器中间层使用leaky ReLU激活函数的性能优于使用常规的 ReLU 函数。\n\n● 特征匹配稍稍改变生成器的目标，以增加可获取的信息量。\n\n● 小批量判别（mini-batch discrimination）向判别器额外添加输入，该特征对小批量中的给定样本和其他样本的距离进行编码，防止模式崩溃（mode collapse），因为判别器能够轻易判断生成器是否生成同样的输出。\n\n● 启发式平均（heuristic averaging），如果网络参数偏离之前值的运行平均值，则会受到惩罚，这有助于收敛到平衡态。\n\n● 虚拟批量归一化（virtual batch normalization），可减少小批量内样本对其他样本的依赖性，方法是使用训练开始就确定的固定参考小批量（reference mini-batch）样本计算归一化的批量统计（batch statistics）。\n\n● 单边标签平滑（one-sided label smoothing）将判别器的目标从 1 替换为 0.9，使判别器的分类边界变得平滑，从而阻止判别器过于自信，为生成器提供较差的梯度。\n\n\n### 3. 基于TensorFlow的Keras实现（Keras）\n\n内容来自：[机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n详见代码：[机器之心 - GAN GitHub实现地址](https://github.com/jiqizhixin/ML-Tutorial-Experiment \"Title\") \n\n#### ● 生成模型\n\n首先需要定义一个生成器 G，该生成器需要将输入的随机噪声变换为图像。以下是定义的生成模型，该模型首先输入有 100 个元素的向量，该向量随机生成于某分布。随后利用两个全连接层接连将该输入向量扩展到 1024 维和 128*7*7 维，后面就开始将全连接层所产生的一维张量重新塑造成二维张量，即 MNIST 中的灰度图。我们注意到该模型采用的激活函数为 tanh，所以也尝试过将其转换为 relu 函数，但发现生成模型如果转化为 relu 函数，那么它的输出就会成为一片灰色。\n\n由全连接传递的数据会经过几个上采样层和卷积层，我们注意到最后一个卷积层所采用的卷积核为 1，所以经过最后卷积层所生成的图像是一张二维灰度图像。\n\n```python\ndef generator_model():\n    #下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠\n    model = Sequential()\n    #添加一个全连接层，输入为100维向量，输出为1024维\n    model.add(Dense(input_dim=100, output_dim=1024))\n    #添加一个激活函数tanh\n    model.add(Activation('tanh'))\n    #添加一个全连接层，输出为128×7×7维度\n    model.add(Dense(128*7*7))\n    #添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    #Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量\n    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n    #2维上采样层，即将数据的行和列分别重复2次\n    model.add(UpSampling2D(size=(2, 2)))\n    #添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变\n    model.add(Conv2D(64, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    model.add(UpSampling2D(size=(2, 2)))\n    #卷积核设为1即输出图像的维度\n    model.add(Conv2D(1, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    return model\n```\n\n#### ● 判别模型\n\n判别模型相对来说就是比较传统的图像识别模型，前面我们可以按照经典的方法采用几个卷积层与最大池化层，而后再展开为一维张量并采用几个全连接层作为架构。我们尝试了将 tanh 激活函数改为 relu 激活函数，在前两个 epoch 基本上没有什么明显的变化。\n\n```python\ndef discriminator_model():\n    #下面搭建判别器架构，同样采用序贯模型\n    model = Sequential()\n    \n    #添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols）\n    #在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维\n    model.add(\n            Conv2D(64, (5, 5),\n            padding='same',\n            input_shape=(28, 28, 1))\n            )\n    model.add(Activation('tanh'))\n    #为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (5, 5)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('tanh'))\n    #一个结点进行二值分类，并采用sigmoid函数的输出作为概念\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n```\n\n#### ● 拼接\n\n前面定义的是可生成图像的模型 G，而我们在训练生成模型时，需要固定判别模型 D 以极小化价值函数而寻求更好的生成模型，这就意味着我们需要将生成模型与判别模型拼接在一起，并固定 D 的权重以训练 G 的权重。下面就定义了这一过程，我们先添加前面定义的生成模型，再将定义的判别模型拼接在生成模型下方，并且我们将判别模型设置为不可训练。因此，训练这个组合模型才能真正更新生成模型的参数。\n\n```python\ndef generator_containing_discriminator(g, d):\n    #将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片\n    model = Sequential()\n    #先添加生成器架构，再令d不可训练，即固定d\n    #因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器\n    model.add(g)\n    d.trainable = False\n    model.add(d)\n    return model\n```\n\n#### ● 训练\n\n以下训练过程可简述为：\n\n- 加载 MNIST 数据\n- 将数据分割为训练与测试集，并赋值给变量\n- 设置训练模型的超参数\n- 编译模型的训练过程\n- 在每一次迭代内，抽取生成图像与真实图像，并打上标注\n- 随后将数据投入到判别模型中，并进行训练与计算损失\n- 固定判别模型，训练生成模型并计算损失，结束这一次迭代\n\n```python\ndef train(BATCH_SIZE):\n    \n    # 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。\n    #下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    #iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。\n    #\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)\n    \n    #转换字段类型，并将数据导入变量中\n    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n    X_train = X_train[:, :, :, None]\n    X_test = X_test[:, :, :, None]\n    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n    \n    #将定义好的模型架构赋值给特定的变量\n    d = discriminator_model()\n    g = generator_model()\n    d_on_g = generator_containing_discriminator(g, d)\n    \n    #定义生成器模型判别器模型更新所使用的优化算法及超参数\n    d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n    g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n    \n    #编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的\n    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n    \n    #前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。\n    d.trainable = True\n    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n    \n    #下面在满足epoch条件下进行训练\n    for epoch in range(30):\n        print(\"Epoch is\", epoch)\n        \n        #计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度\n        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n        \n        #在一个epoch内进行迭代训练\n        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n            \n            #随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本\n            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n            \n            #抽取一个批量的真实图片\n            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n            \n            #生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录\n            generated_images = g.predict(noise, verbose=0)\n            \n            #每经过100次迭代输出一张生成的图片\n            if index % 100 == 0:\n                image = combine_images(generated_images)\n                image = image*127.5+127.5\n                Image.fromarray(image.astype(np.uint8)).save(\n                    \"./GAN/\"+str(epoch)+\"_\"+str(index)+\".png\")\n            \n            #将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下\n            X = np.concatenate((image_batch, generated_images))\n            \n            #生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片\n            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n            \n            #判别器的损失；在一个batch的数据上进行一次参数更新\n            d_loss = d.train_on_batch(X, y)\n            print(\"batch %d d_loss : %f\" % (index, d_loss))\n            \n            #随机生成的噪声服从均匀分布\n            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n            \n            #固定判别器\n            d.trainable = False\n            \n            #计算生成器损失；在一个batch的数据上进行一次参数更新\n            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n            \n            #令判别器可训练\n            d.trainable = True\n            print(\"batch %d g_loss : %f\" % (index, g_loss))\n            \n            #每100次迭代保存一次生成器和判别器的权重\n            if index % 100 == 9:\n                g.save_weights('generator', True)\n                d.save_weights('discriminator', True)\n```\n\n### 程序实例（Program Example）\n\n- [Github Link](https://github.com/HJTSO/face-generation/blob/master/dlnd_face_generation.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [生成对抗网络综述：从架构到训练技巧](https://zhuanlan.zhihu.com/p/30346797 \"Title\") \n\n- [机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n- [深度 | 生成对抗网络初学入门](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650730721&idx=2&sn=95b97b80188f507c409f4c72bd0a2767&chksm=871b349fb06cbd891771f72d77563f77986afc9b144f42c8232db44c7c56c1d2bc019458c4e4&scene=21#wechat_redirect \"Title\") \n\n- [はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n","source":"_posts/zh/Deep learning笔记5-GAN生成式对抗网络.md","raw":"\n---\ntitle: Deep learning笔记5-GAN生成式对抗网络\nlang: zh\ndate: 2017-11-01 18:18:56\ntags: Deep Learning\n---\n\n### 1. 生成式对抗网络（GAN）\n\nGAN（Generative Adversarial Network）的思想：生成器和鉴别器两个网络彼此博弈。\n\n● 生成器的目标是生成一个对象，并使其看起来和真的一样。\n\n● 鉴别器的目标就是找到生成出的结果和真实图像之间的差异。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-1.jpg)</center> \n\n-------------------------------------\n\n内容部分来自：[はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n訓練データを学習し、それらのデータと似たような新しいデータを生成するモデルのことを生成モデルと呼びます。別の言い方をすると、訓練データの分布と生成データの分布が一致するように学習していくようなモデルです。\n\nGANではgeneratorとdiscriminatorという２つのネットワークが登場します。Generatorは訓練データと同じようなデータを生成しようとします。一方、discriminatorはデータが訓練データから来たものか、それとも生成モデルから来たものかを識別します。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-2.jpg)</center> \n\n-------------------------------------\n\n如上图所示，生成对抗网络会训练并更新判别分布，更新判别器后就能将数据真实分布从生成分布中判别出来。经过若干次训练后，如果 G 和 D 有足够的复杂度，那么它们就会到达一个均衡点。这个时候生成器的概率密度函数等于真实数据的概率密度函数，也即生成的数据和真实数据是一样的。在均衡点上 D 和 G 都不能得到进一步提升，并且判别器无法判断数据到底是来自真实样本还是伪造的数据，即 D（x）= 1/2。\n\nGAN学习的表征可用于多种应用，包括图像合成、语义图像编辑、风格迁移、图像超分辨率技术和分类。\n\neg. ベッドルームの画像のデータセットを使って学習した結果です。一見本物と見間違ってしまいそうなレベルの画像を生成できていることが分かります。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-3.png)</center> \n\n-------------------------------------\n\neg. Word2Vecという単語ベクトルで「王様」-「男」+「女」=「女王」という演算ができることは有名ですが、GANにおける入力であるzzベクトルを使っても同様の演算を行うことができます。下の例では、「サングラスをかけた男」-「男」+「女」=「サングラスをかけた女」という演算を行っています。\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-4.png)</center> \n\n-------------------------------------\n\nGAN的类别 - 内容来自：[生成对抗网络综述：从架构到训练技巧](https://zhuanlan.zhihu.com/p/30346797 \"Title\") \n\n#### 1.1. 全连接 GAN\n\n首个 GAN 架构在生成器与鉴别器上皆使用全连接神经网络。这种架构类型被应用于相对简单的图像数据库，即 MNIST（手写数字）、CIFAR-10（自然图像）和多伦多人脸数据集（TFD）。\n\n#### 1.2.  卷积 GAN\n\n● LAPGAN拉普拉斯金字塔形对抗网络（Laplacian pyramid GAN）\n\n真值图像本身被分解成拉普拉斯金字塔（Laplacian pyramid），并且条件性卷积 GAN 被训练在给定上一层的情况下生成每一层。\n\n● DCGAN（Deep Convolutional GAN）\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/1-5.png)</center> \n\n-------------------------------------\n\nDCGAN允许训练一对深度卷积生成器和判别器网络。在训练中使用带步长的卷积（strided convolution）和小步长卷积（fractionally-strided convolution），并在训练中学习空间下采样和上采样算子。\n\n#### 1.3. 条件 GAN\n\n通过将生成器和判别器改造成条件类（class-conditional）而将（2D）GAN 框架扩展成条件设置。条件 GNN 的优势在于可以对多形式的数据生成提供更好的表征。条件 GAN 和 InfoGAN[16] 是平行的，它可以将噪声源分解为不可压缩源和一个「隐编码」（latent code），并可以通过最大化隐编码和生成器之间的交互信息而发现变化的隐藏因子。这个隐编码可用于在完全无监督的数据中发现目标类，即使这个隐编码是不明确的。由 InfoGAN 学到的表征看起来像是具备语义特征的，可以处理图貌中的复杂纠缠因素（包括姿势变化、光照和面部图像的情绪内容）。\n\n#### 1.4. GAN 推断模型\n\nGAN 的初始形式无法将给定的输入 x 映射为隐空间中的向量（在 GAN 的文献中，这通常被称为一种推断机制）。人们提出了几种反转预训练 GAN 的生成器的技术，比如各自独立提出的对抗性学习推断（Adversarially Learned Inference，ALI）和双向 GAN（Bidirectional GANs），它们能提供简单而有效的扩展，通过加入一个推断网络，使判别器共同测试数据空间和隐空间。\n\n这种形式下的生成器由两个网络组成：即编码器（推断网络）和解码器。它们同时被训练用于欺骗判别器。而判别器将接收到一个向量对（x,z），并决定其是否包含一个真实图像以及其编码，或者一个生成的图像样本以及相关的生成器的隐空间输入。\n\n#### 1.5. AAE 对抗自编码器\n\n自编码器是由编码器和解码器组成的网络，学习将数据映射到内部隐表征中，再映射出来，即从数据空间中学习将图像（或其它）通过编码映射到隐空间中，再通过解码从隐空间映射回数据空间。这两个映射形成了一种重构运算，而这两个映射将被训练直到重构图像尽可能的接近初始图像。\n\n### 2. 生成式对抗网络的训练（GAN Training）\n\n生成对抗网络（GAN）提供了一种不需要大量标注训练数据就能学习深度表征的方式。它们通过反向传播算法分别更新两个网络以执行竞争性学习而达到训练目的。\n\n#### 2.1. 训练 \n\n训练的代价由一个价值函数 V(G,D) 评估，其包含了生成器和判别器的参数。\n\n训练过程可表示如下：\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/2-1.png)</center> \n\n-------------------------------------\n\n训练过程中，其中一个模型的参数被更新，同时另一个模型的参数固定不变。\n\n算法描述：[Goodfellow et al. (2014)](https://arxiv.org/abs/1406.2661 \"Title\") \nより引用\n\n-------------------------------------\n\n<center>![GAN](/image/DL/5/2-2.png)</center> \n\n-------------------------------------\n\n推荐阅读 - [机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n推荐阅读 - [はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n#### 2.2. 训练技巧\n\n用于图像生成的 GAN 训练的第一个重大改进是 Radford et al.提出的 DCGAN 架构。具体到训练中，研究者推荐在两种网络中使用批量归一化，以稳定深层模型中的训练。另一个建议是最小化用于提升深层模型训练可行性的全连接层的数量。最后，Radford et al.认为在判别器中间层使用leaky ReLU激活函数的性能优于使用常规的 ReLU 函数。\n\n● 特征匹配稍稍改变生成器的目标，以增加可获取的信息量。\n\n● 小批量判别（mini-batch discrimination）向判别器额外添加输入，该特征对小批量中的给定样本和其他样本的距离进行编码，防止模式崩溃（mode collapse），因为判别器能够轻易判断生成器是否生成同样的输出。\n\n● 启发式平均（heuristic averaging），如果网络参数偏离之前值的运行平均值，则会受到惩罚，这有助于收敛到平衡态。\n\n● 虚拟批量归一化（virtual batch normalization），可减少小批量内样本对其他样本的依赖性，方法是使用训练开始就确定的固定参考小批量（reference mini-batch）样本计算归一化的批量统计（batch statistics）。\n\n● 单边标签平滑（one-sided label smoothing）将判别器的目标从 1 替换为 0.9，使判别器的分类边界变得平滑，从而阻止判别器过于自信，为生成器提供较差的梯度。\n\n\n### 3. 基于TensorFlow的Keras实现（Keras）\n\n内容来自：[机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n详见代码：[机器之心 - GAN GitHub实现地址](https://github.com/jiqizhixin/ML-Tutorial-Experiment \"Title\") \n\n#### ● 生成模型\n\n首先需要定义一个生成器 G，该生成器需要将输入的随机噪声变换为图像。以下是定义的生成模型，该模型首先输入有 100 个元素的向量，该向量随机生成于某分布。随后利用两个全连接层接连将该输入向量扩展到 1024 维和 128*7*7 维，后面就开始将全连接层所产生的一维张量重新塑造成二维张量，即 MNIST 中的灰度图。我们注意到该模型采用的激活函数为 tanh，所以也尝试过将其转换为 relu 函数，但发现生成模型如果转化为 relu 函数，那么它的输出就会成为一片灰色。\n\n由全连接传递的数据会经过几个上采样层和卷积层，我们注意到最后一个卷积层所采用的卷积核为 1，所以经过最后卷积层所生成的图像是一张二维灰度图像。\n\n```python\ndef generator_model():\n    #下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠\n    model = Sequential()\n    #添加一个全连接层，输入为100维向量，输出为1024维\n    model.add(Dense(input_dim=100, output_dim=1024))\n    #添加一个激活函数tanh\n    model.add(Activation('tanh'))\n    #添加一个全连接层，输出为128×7×7维度\n    model.add(Dense(128*7*7))\n    #添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    #Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量\n    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n    #2维上采样层，即将数据的行和列分别重复2次\n    model.add(UpSampling2D(size=(2, 2)))\n    #添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变\n    model.add(Conv2D(64, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    model.add(UpSampling2D(size=(2, 2)))\n    #卷积核设为1即输出图像的维度\n    model.add(Conv2D(1, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    return model\n```\n\n#### ● 判别模型\n\n判别模型相对来说就是比较传统的图像识别模型，前面我们可以按照经典的方法采用几个卷积层与最大池化层，而后再展开为一维张量并采用几个全连接层作为架构。我们尝试了将 tanh 激活函数改为 relu 激活函数，在前两个 epoch 基本上没有什么明显的变化。\n\n```python\ndef discriminator_model():\n    #下面搭建判别器架构，同样采用序贯模型\n    model = Sequential()\n    \n    #添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols）\n    #在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维\n    model.add(\n            Conv2D(64, (5, 5),\n            padding='same',\n            input_shape=(28, 28, 1))\n            )\n    model.add(Activation('tanh'))\n    #为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (5, 5)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    #Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('tanh'))\n    #一个结点进行二值分类，并采用sigmoid函数的输出作为概念\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n```\n\n#### ● 拼接\n\n前面定义的是可生成图像的模型 G，而我们在训练生成模型时，需要固定判别模型 D 以极小化价值函数而寻求更好的生成模型，这就意味着我们需要将生成模型与判别模型拼接在一起，并固定 D 的权重以训练 G 的权重。下面就定义了这一过程，我们先添加前面定义的生成模型，再将定义的判别模型拼接在生成模型下方，并且我们将判别模型设置为不可训练。因此，训练这个组合模型才能真正更新生成模型的参数。\n\n```python\ndef generator_containing_discriminator(g, d):\n    #将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片\n    model = Sequential()\n    #先添加生成器架构，再令d不可训练，即固定d\n    #因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器\n    model.add(g)\n    d.trainable = False\n    model.add(d)\n    return model\n```\n\n#### ● 训练\n\n以下训练过程可简述为：\n\n- 加载 MNIST 数据\n- 将数据分割为训练与测试集，并赋值给变量\n- 设置训练模型的超参数\n- 编译模型的训练过程\n- 在每一次迭代内，抽取生成图像与真实图像，并打上标注\n- 随后将数据投入到判别模型中，并进行训练与计算损失\n- 固定判别模型，训练生成模型并计算损失，结束这一次迭代\n\n```python\ndef train(BATCH_SIZE):\n    \n    # 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。\n    #下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    #iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。\n    #\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)\n    \n    #转换字段类型，并将数据导入变量中\n    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n    X_train = X_train[:, :, :, None]\n    X_test = X_test[:, :, :, None]\n    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n    \n    #将定义好的模型架构赋值给特定的变量\n    d = discriminator_model()\n    g = generator_model()\n    d_on_g = generator_containing_discriminator(g, d)\n    \n    #定义生成器模型判别器模型更新所使用的优化算法及超参数\n    d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n    g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n    \n    #编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的\n    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n    \n    #前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。\n    d.trainable = True\n    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n    \n    #下面在满足epoch条件下进行训练\n    for epoch in range(30):\n        print(\"Epoch is\", epoch)\n        \n        #计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度\n        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n        \n        #在一个epoch内进行迭代训练\n        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n            \n            #随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本\n            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n            \n            #抽取一个批量的真实图片\n            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n            \n            #生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录\n            generated_images = g.predict(noise, verbose=0)\n            \n            #每经过100次迭代输出一张生成的图片\n            if index % 100 == 0:\n                image = combine_images(generated_images)\n                image = image*127.5+127.5\n                Image.fromarray(image.astype(np.uint8)).save(\n                    \"./GAN/\"+str(epoch)+\"_\"+str(index)+\".png\")\n            \n            #将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下\n            X = np.concatenate((image_batch, generated_images))\n            \n            #生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片\n            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n            \n            #判别器的损失；在一个batch的数据上进行一次参数更新\n            d_loss = d.train_on_batch(X, y)\n            print(\"batch %d d_loss : %f\" % (index, d_loss))\n            \n            #随机生成的噪声服从均匀分布\n            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n            \n            #固定判别器\n            d.trainable = False\n            \n            #计算生成器损失；在一个batch的数据上进行一次参数更新\n            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n            \n            #令判别器可训练\n            d.trainable = True\n            print(\"batch %d g_loss : %f\" % (index, g_loss))\n            \n            #每100次迭代保存一次生成器和判别器的权重\n            if index % 100 == 9:\n                g.save_weights('generator', True)\n                d.save_weights('discriminator', True)\n```\n\n### 程序实例（Program Example）\n\n- [Github Link](https://github.com/HJTSO/face-generation/blob/master/dlnd_face_generation.ipynb \"Title\") \n\n### 参考资料（Reference）\n\n- [生成对抗网络综述：从架构到训练技巧](https://zhuanlan.zhihu.com/p/30346797 \"Title\") \n\n- [机器之心GitHub项目：GAN完整理论推导与实现](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650731540&idx=1&sn=193457603fe11b89f3d298ac1799b9fd&chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&scene=21#wechat_redirect \"Title\") \n\n- [深度 | 生成对抗网络初学入门](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650730721&idx=2&sn=95b97b80188f507c409f4c72bd0a2767&chksm=871b349fb06cbd891771f72d77563f77986afc9b144f42c8232db44c7c56c1d2bc019458c4e4&scene=21#wechat_redirect \"Title\") \n\n- [はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html \"Title\") \n\n","slug":"zh-Deep-learning笔记5-GAN生成式对抗网络","published":1,"updated":"2018-01-01T13:14:23.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zo002aog64kvwzikoz","content":"<h3 id=\"1-生成式对抗网络（GAN）\"><a href=\"#1-生成式对抗网络（GAN）\" class=\"headerlink\" title=\"1. 生成式对抗网络（GAN）\"></a>1. 生成式对抗网络（GAN）</h3><p>GAN（Generative Adversarial Network）的思想：生成器和鉴别器两个网络彼此博弈。</p>\n<p>● 生成器的目标是生成一个对象，并使其看起来和真的一样。</p>\n<p>● 鉴别器的目标就是找到生成出的结果和真实图像之间的差异。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-1.jpg\" alt=\"GAN\"></center> \n\n<hr>\n<p>内容部分来自：<a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\" target=\"_blank\" rel=\"external\">はじめてのGAN</a> </p>\n<p>訓練データを学習し、それらのデータと似たような新しいデータを生成するモデルのことを生成モデルと呼びます。別の言い方をすると、訓練データの分布と生成データの分布が一致するように学習していくようなモデルです。</p>\n<p>GANではgeneratorとdiscriminatorという２つのネットワークが登場します。Generatorは訓練データと同じようなデータを生成しようとします。一方、discriminatorはデータが訓練データから来たものか、それとも生成モデルから来たものかを識別します。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-2.jpg\" alt=\"GAN\"></center> \n\n<hr>\n<p>如上图所示，生成对抗网络会训练并更新判别分布，更新判别器后就能将数据真实分布从生成分布中判别出来。经过若干次训练后，如果 G 和 D 有足够的复杂度，那么它们就会到达一个均衡点。这个时候生成器的概率密度函数等于真实数据的概率密度函数，也即生成的数据和真实数据是一样的。在均衡点上 D 和 G 都不能得到进一步提升，并且判别器无法判断数据到底是来自真实样本还是伪造的数据，即 D（x）= 1/2。</p>\n<p>GAN学习的表征可用于多种应用，包括图像合成、语义图像编辑、风格迁移、图像超分辨率技术和分类。</p>\n<p>eg. ベッドルームの画像のデータセットを使って学習した結果です。一見本物と見間違ってしまいそうなレベルの画像を生成できていることが分かります。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-3.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>eg. Word2Vecという単語ベクトルで「王様」-「男」+「女」=「女王」という演算ができることは有名ですが、GANにおける入力であるzzベクトルを使っても同様の演算を行うことができます。下の例では、「サングラスをかけた男」-「男」+「女」=「サングラスをかけた女」という演算を行っています。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-4.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>GAN的类别 - 内容来自：<a href=\"https://zhuanlan.zhihu.com/p/30346797\" title=\"Title\" target=\"_blank\" rel=\"external\">生成对抗网络综述：从架构到训练技巧</a> </p>\n<h4 id=\"1-1-全连接-GAN\"><a href=\"#1-1-全连接-GAN\" class=\"headerlink\" title=\"1.1. 全连接 GAN\"></a>1.1. 全连接 GAN</h4><p>首个 GAN 架构在生成器与鉴别器上皆使用全连接神经网络。这种架构类型被应用于相对简单的图像数据库，即 MNIST（手写数字）、CIFAR-10（自然图像）和多伦多人脸数据集（TFD）。</p>\n<h4 id=\"1-2-卷积-GAN\"><a href=\"#1-2-卷积-GAN\" class=\"headerlink\" title=\"1.2.  卷积 GAN\"></a>1.2.  卷积 GAN</h4><p>● LAPGAN拉普拉斯金字塔形对抗网络（Laplacian pyramid GAN）</p>\n<p>真值图像本身被分解成拉普拉斯金字塔（Laplacian pyramid），并且条件性卷积 GAN 被训练在给定上一层的情况下生成每一层。</p>\n<p>● DCGAN（Deep Convolutional GAN）</p>\n<hr>\n<center><img src=\"/image/DL/5/1-5.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>DCGAN允许训练一对深度卷积生成器和判别器网络。在训练中使用带步长的卷积（strided convolution）和小步长卷积（fractionally-strided convolution），并在训练中学习空间下采样和上采样算子。</p>\n<h4 id=\"1-3-条件-GAN\"><a href=\"#1-3-条件-GAN\" class=\"headerlink\" title=\"1.3. 条件 GAN\"></a>1.3. 条件 GAN</h4><p>通过将生成器和判别器改造成条件类（class-conditional）而将（2D）GAN 框架扩展成条件设置。条件 GNN 的优势在于可以对多形式的数据生成提供更好的表征。条件 GAN 和 InfoGAN[16] 是平行的，它可以将噪声源分解为不可压缩源和一个「隐编码」（latent code），并可以通过最大化隐编码和生成器之间的交互信息而发现变化的隐藏因子。这个隐编码可用于在完全无监督的数据中发现目标类，即使这个隐编码是不明确的。由 InfoGAN 学到的表征看起来像是具备语义特征的，可以处理图貌中的复杂纠缠因素（包括姿势变化、光照和面部图像的情绪内容）。</p>\n<h4 id=\"1-4-GAN-推断模型\"><a href=\"#1-4-GAN-推断模型\" class=\"headerlink\" title=\"1.4. GAN 推断模型\"></a>1.4. GAN 推断模型</h4><p>GAN 的初始形式无法将给定的输入 x 映射为隐空间中的向量（在 GAN 的文献中，这通常被称为一种推断机制）。人们提出了几种反转预训练 GAN 的生成器的技术，比如各自独立提出的对抗性学习推断（Adversarially Learned Inference，ALI）和双向 GAN（Bidirectional GANs），它们能提供简单而有效的扩展，通过加入一个推断网络，使判别器共同测试数据空间和隐空间。</p>\n<p>这种形式下的生成器由两个网络组成：即编码器（推断网络）和解码器。它们同时被训练用于欺骗判别器。而判别器将接收到一个向量对（x,z），并决定其是否包含一个真实图像以及其编码，或者一个生成的图像样本以及相关的生成器的隐空间输入。</p>\n<h4 id=\"1-5-AAE-对抗自编码器\"><a href=\"#1-5-AAE-对抗自编码器\" class=\"headerlink\" title=\"1.5. AAE 对抗自编码器\"></a>1.5. AAE 对抗自编码器</h4><p>自编码器是由编码器和解码器组成的网络，学习将数据映射到内部隐表征中，再映射出来，即从数据空间中学习将图像（或其它）通过编码映射到隐空间中，再通过解码从隐空间映射回数据空间。这两个映射形成了一种重构运算，而这两个映射将被训练直到重构图像尽可能的接近初始图像。</p>\n<h3 id=\"2-生成式对抗网络的训练（GAN-Training）\"><a href=\"#2-生成式对抗网络的训练（GAN-Training）\" class=\"headerlink\" title=\"2. 生成式对抗网络的训练（GAN Training）\"></a>2. 生成式对抗网络的训练（GAN Training）</h3><p>生成对抗网络（GAN）提供了一种不需要大量标注训练数据就能学习深度表征的方式。它们通过反向传播算法分别更新两个网络以执行竞争性学习而达到训练目的。</p>\n<h4 id=\"2-1-训练\"><a href=\"#2-1-训练\" class=\"headerlink\" title=\"2.1. 训练\"></a>2.1. 训练</h4><p>训练的代价由一个价值函数 V(G,D) 评估，其包含了生成器和判别器的参数。</p>\n<p>训练过程可表示如下：</p>\n<hr>\n<center><img src=\"/image/DL/5/2-1.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>训练过程中，其中一个模型的参数被更新，同时另一个模型的参数固定不变。</p>\n<p>算法描述：<a href=\"https://arxiv.org/abs/1406.2661\" title=\"Title\" target=\"_blank\" rel=\"external\">Goodfellow et al. (2014)</a><br>より引用</p>\n<hr>\n<center><img src=\"/image/DL/5/2-2.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>推荐阅读 - <a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\" target=\"_blank\" rel=\"external\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n<p>推荐阅读 - <a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\" target=\"_blank\" rel=\"external\">はじめてのGAN</a> </p>\n<h4 id=\"2-2-训练技巧\"><a href=\"#2-2-训练技巧\" class=\"headerlink\" title=\"2.2. 训练技巧\"></a>2.2. 训练技巧</h4><p>用于图像生成的 GAN 训练的第一个重大改进是 Radford et al.提出的 DCGAN 架构。具体到训练中，研究者推荐在两种网络中使用批量归一化，以稳定深层模型中的训练。另一个建议是最小化用于提升深层模型训练可行性的全连接层的数量。最后，Radford et al.认为在判别器中间层使用leaky ReLU激活函数的性能优于使用常规的 ReLU 函数。</p>\n<p>● 特征匹配稍稍改变生成器的目标，以增加可获取的信息量。</p>\n<p>● 小批量判别（mini-batch discrimination）向判别器额外添加输入，该特征对小批量中的给定样本和其他样本的距离进行编码，防止模式崩溃（mode collapse），因为判别器能够轻易判断生成器是否生成同样的输出。</p>\n<p>● 启发式平均（heuristic averaging），如果网络参数偏离之前值的运行平均值，则会受到惩罚，这有助于收敛到平衡态。</p>\n<p>● 虚拟批量归一化（virtual batch normalization），可减少小批量内样本对其他样本的依赖性，方法是使用训练开始就确定的固定参考小批量（reference mini-batch）样本计算归一化的批量统计（batch statistics）。</p>\n<p>● 单边标签平滑（one-sided label smoothing）将判别器的目标从 1 替换为 0.9，使判别器的分类边界变得平滑，从而阻止判别器过于自信，为生成器提供较差的梯度。</p>\n<h3 id=\"3-基于TensorFlow的Keras实现（Keras）\"><a href=\"#3-基于TensorFlow的Keras实现（Keras）\" class=\"headerlink\" title=\"3. 基于TensorFlow的Keras实现（Keras）\"></a>3. 基于TensorFlow的Keras实现（Keras）</h3><p>内容来自：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\" target=\"_blank\" rel=\"external\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n<p>详见代码：<a href=\"https://github.com/jiqizhixin/ML-Tutorial-Experiment\" title=\"Title\" target=\"_blank\" rel=\"external\">机器之心 - GAN GitHub实现地址</a> </p>\n<h4 id=\"●-生成模型\"><a href=\"#●-生成模型\" class=\"headerlink\" title=\"● 生成模型\"></a>● 生成模型</h4><p>首先需要定义一个生成器 G，该生成器需要将输入的随机噪声变换为图像。以下是定义的生成模型，该模型首先输入有 100 个元素的向量，该向量随机生成于某分布。随后利用两个全连接层接连将该输入向量扩展到 1024 维和 128<em>7</em>7 维，后面就开始将全连接层所产生的一维张量重新塑造成二维张量，即 MNIST 中的灰度图。我们注意到该模型采用的激活函数为 tanh，所以也尝试过将其转换为 relu 函数，但发现生成模型如果转化为 relu 函数，那么它的输出就会成为一片灰色。</p>\n<p>由全连接传递的数据会经过几个上采样层和卷积层，我们注意到最后一个卷积层所采用的卷积核为 1，所以经过最后卷积层所生成的图像是一张二维灰度图像。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator_model</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"comment\">#下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    <span class=\"comment\">#添加一个全连接层，输入为100维向量，输出为1024维</span></div><div class=\"line\">    model.add(Dense(input_dim=<span class=\"number\">100</span>, output_dim=<span class=\"number\">1024</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个激活函数tanh</span></div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个全连接层，输出为128×7×7维度</span></div><div class=\"line\">    model.add(Dense(<span class=\"number\">128</span>*<span class=\"number\">7</span>*<span class=\"number\">7</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1</span></div><div class=\"line\">    model.add(BatchNormalization())</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量</span></div><div class=\"line\">    model.add(Reshape((<span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">128</span>), input_shape=(<span class=\"number\">128</span>*<span class=\"number\">7</span>*<span class=\"number\">7</span>,)))</div><div class=\"line\">    <span class=\"comment\">#2维上采样层，即将数据的行和列分别重复2次</span></div><div class=\"line\">    model.add(UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变</span></div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), padding=<span class=\"string\">'same'</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    model.add(UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#卷积核设为1即输出图像的维度</span></div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">1</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), padding=<span class=\"string\">'same'</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-判别模型\"><a href=\"#●-判别模型\" class=\"headerlink\" title=\"● 判别模型\"></a>● 判别模型</h4><p>判别模型相对来说就是比较传统的图像识别模型，前面我们可以按照经典的方法采用几个卷积层与最大池化层，而后再展开为一维张量并采用几个全连接层作为架构。我们尝试了将 tanh 激活函数改为 relu 激活函数，在前两个 epoch 基本上没有什么明显的变化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator_model</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"comment\">#下面搭建判别器架构，同样采用序贯模型</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols）</span></div><div class=\"line\">    <span class=\"comment\">#在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维</span></div><div class=\"line\">    model.add(</div><div class=\"line\">            Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            padding=<span class=\"string\">'same'</span>,</div><div class=\"line\">            input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))</div><div class=\"line\">            )</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半</span></div><div class=\"line\">    model.add(MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>)))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    model.add(MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡</span></div><div class=\"line\">    model.add(Flatten())</div><div class=\"line\">    model.add(Dense(<span class=\"number\">1024</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#一个结点进行二值分类，并采用sigmoid函数的输出作为概念</span></div><div class=\"line\">    model.add(Dense(<span class=\"number\">1</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'sigmoid'</span>))</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-拼接\"><a href=\"#●-拼接\" class=\"headerlink\" title=\"● 拼接\"></a>● 拼接</h4><p>前面定义的是可生成图像的模型 G，而我们在训练生成模型时，需要固定判别模型 D 以极小化价值函数而寻求更好的生成模型，这就意味着我们需要将生成模型与判别模型拼接在一起，并固定 D 的权重以训练 G 的权重。下面就定义了这一过程，我们先添加前面定义的生成模型，再将定义的判别模型拼接在生成模型下方，并且我们将判别模型设置为不可训练。因此，训练这个组合模型才能真正更新生成模型的参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator_containing_discriminator</span><span class=\"params\">(g, d)</span>:</span></div><div class=\"line\">    <span class=\"comment\">#将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    <span class=\"comment\">#先添加生成器架构，再令d不可训练，即固定d</span></div><div class=\"line\">    <span class=\"comment\">#因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器</span></div><div class=\"line\">    model.add(g)</div><div class=\"line\">    d.trainable = <span class=\"keyword\">False</span></div><div class=\"line\">    model.add(d)</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-训练\"><a href=\"#●-训练\" class=\"headerlink\" title=\"● 训练\"></a>● 训练</h4><p>以下训练过程可简述为：</p>\n<ul>\n<li>加载 MNIST 数据</li>\n<li>将数据分割为训练与测试集，并赋值给变量</li>\n<li>设置训练模型的超参数</li>\n<li>编译模型的训练过程</li>\n<li>在每一次迭代内，抽取生成图像与真实图像，并打上标注</li>\n<li>随后将数据投入到判别模型中，并进行训练与计算损失</li>\n<li>固定判别模型，训练生成模型并计算损失，结束这一次迭代</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(BATCH_SIZE)</span>:</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\"># 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。</span></div><div class=\"line\">    <span class=\"comment\">#下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz</span></div><div class=\"line\">    (X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class=\"line\">    <span class=\"comment\">#iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。</span></div><div class=\"line\">    <span class=\"comment\">#\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#转换字段类型，并将数据导入变量中</span></div><div class=\"line\">    X_train = (X_train.astype(np.float32) - <span class=\"number\">127.5</span>)/<span class=\"number\">127.5</span></div><div class=\"line\">    X_train = X_train[:, :, :, <span class=\"keyword\">None</span>]</div><div class=\"line\">    X_test = X_test[:, :, :, <span class=\"keyword\">None</span>]</div><div class=\"line\">    <span class=\"comment\"># X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#将定义好的模型架构赋值给特定的变量</span></div><div class=\"line\">    d = discriminator_model()</div><div class=\"line\">    g = generator_model()</div><div class=\"line\">    d_on_g = generator_containing_discriminator(g, d)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#定义生成器模型判别器模型更新所使用的优化算法及超参数</span></div><div class=\"line\">    d_optim = SGD(lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>, nesterov=<span class=\"keyword\">True</span>)</div><div class=\"line\">    g_optim = SGD(lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>, nesterov=<span class=\"keyword\">True</span>)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的</span></div><div class=\"line\">    g.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=<span class=\"string\">\"SGD\"</span>)</div><div class=\"line\">    d_on_g.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=g_optim)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。</span></div><div class=\"line\">    d.trainable = <span class=\"keyword\">True</span></div><div class=\"line\">    d.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=d_optim)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#下面在满足epoch条件下进行训练</span></div><div class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">30</span>):</div><div class=\"line\">        print(<span class=\"string\">\"Epoch is\"</span>, epoch)</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"comment\">#计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度</span></div><div class=\"line\">        print(<span class=\"string\">\"Number of batches\"</span>, int(X_train.shape[<span class=\"number\">0</span>]/BATCH_SIZE))</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"comment\">#在一个epoch内进行迭代训练</span></div><div class=\"line\">        <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(int(X_train.shape[<span class=\"number\">0</span>]/BATCH_SIZE)):</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本</span></div><div class=\"line\">            noise = np.random.uniform(<span class=\"number\">-1</span>, <span class=\"number\">1</span>, size=(BATCH_SIZE, <span class=\"number\">100</span>))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#抽取一个批量的真实图片</span></div><div class=\"line\">            image_batch = X_train[index*BATCH_SIZE:(index+<span class=\"number\">1</span>)*BATCH_SIZE]</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录</span></div><div class=\"line\">            generated_images = g.predict(noise, verbose=<span class=\"number\">0</span>)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#每经过100次迭代输出一张生成的图片</span></div><div class=\"line\">            <span class=\"keyword\">if</span> index % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">                image = combine_images(generated_images)</div><div class=\"line\">                image = image*<span class=\"number\">127.5</span>+<span class=\"number\">127.5</span></div><div class=\"line\">                Image.fromarray(image.astype(np.uint8)).save(</div><div class=\"line\">                    <span class=\"string\">\"./GAN/\"</span>+str(epoch)+<span class=\"string\">\"_\"</span>+str(index)+<span class=\"string\">\".png\"</span>)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下</span></div><div class=\"line\">            X = np.concatenate((image_batch, generated_images))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片</span></div><div class=\"line\">            y = [<span class=\"number\">1</span>] * BATCH_SIZE + [<span class=\"number\">0</span>] * BATCH_SIZE</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#判别器的损失；在一个batch的数据上进行一次参数更新</span></div><div class=\"line\">            d_loss = d.train_on_batch(X, y)</div><div class=\"line\">            print(<span class=\"string\">\"batch %d d_loss : %f\"</span> % (index, d_loss))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#随机生成的噪声服从均匀分布</span></div><div class=\"line\">            noise = np.random.uniform(<span class=\"number\">-1</span>, <span class=\"number\">1</span>, (BATCH_SIZE, <span class=\"number\">100</span>))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#固定判别器</span></div><div class=\"line\">            d.trainable = <span class=\"keyword\">False</span></div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#计算生成器损失；在一个batch的数据上进行一次参数更新</span></div><div class=\"line\">            g_loss = d_on_g.train_on_batch(noise, [<span class=\"number\">1</span>] * BATCH_SIZE)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#令判别器可训练</span></div><div class=\"line\">            d.trainable = <span class=\"keyword\">True</span></div><div class=\"line\">            print(<span class=\"string\">\"batch %d g_loss : %f\"</span> % (index, g_loss))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#每100次迭代保存一次生成器和判别器的权重</span></div><div class=\"line\">            <span class=\"keyword\">if</span> index % <span class=\"number\">100</span> == <span class=\"number\">9</span>:</div><div class=\"line\">                g.save_weights(<span class=\"string\">'generator'</span>, <span class=\"keyword\">True</span>)</div><div class=\"line\">                d.save_weights(<span class=\"string\">'discriminator'</span>, <span class=\"keyword\">True</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><a href=\"https://github.com/HJTSO/face-generation/blob/master/dlnd_face_generation.ipynb\" title=\"Title\" target=\"_blank\" rel=\"external\">Github Link</a> </li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/30346797\" title=\"Title\" target=\"_blank\" rel=\"external\">生成对抗网络综述：从架构到训练技巧</a> </p>\n</li>\n<li><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\" target=\"_blank\" rel=\"external\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n</li>\n<li><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650730721&amp;idx=2&amp;sn=95b97b80188f507c409f4c72bd0a2767&amp;chksm=871b349fb06cbd891771f72d77563f77986afc9b144f42c8232db44c7c56c1d2bc019458c4e4&amp;scene=21#wechat_redirect\" title=\"Title\" target=\"_blank\" rel=\"external\">深度 | 生成对抗网络初学入门</a> </p>\n</li>\n<li><p><a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\" target=\"_blank\" rel=\"external\">はじめてのGAN</a> </p>\n</li>\n</ul>\n","excerpt":"","more":"<h3 id=\"1-生成式对抗网络（GAN）\"><a href=\"#1-生成式对抗网络（GAN）\" class=\"headerlink\" title=\"1. 生成式对抗网络（GAN）\"></a>1. 生成式对抗网络（GAN）</h3><p>GAN（Generative Adversarial Network）的思想：生成器和鉴别器两个网络彼此博弈。</p>\n<p>● 生成器的目标是生成一个对象，并使其看起来和真的一样。</p>\n<p>● 鉴别器的目标就是找到生成出的结果和真实图像之间的差异。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-1.jpg\" alt=\"GAN\"></center> \n\n<hr>\n<p>内容部分来自：<a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\">はじめてのGAN</a> </p>\n<p>訓練データを学習し、それらのデータと似たような新しいデータを生成するモデルのことを生成モデルと呼びます。別の言い方をすると、訓練データの分布と生成データの分布が一致するように学習していくようなモデルです。</p>\n<p>GANではgeneratorとdiscriminatorという２つのネットワークが登場します。Generatorは訓練データと同じようなデータを生成しようとします。一方、discriminatorはデータが訓練データから来たものか、それとも生成モデルから来たものかを識別します。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-2.jpg\" alt=\"GAN\"></center> \n\n<hr>\n<p>如上图所示，生成对抗网络会训练并更新判别分布，更新判别器后就能将数据真实分布从生成分布中判别出来。经过若干次训练后，如果 G 和 D 有足够的复杂度，那么它们就会到达一个均衡点。这个时候生成器的概率密度函数等于真实数据的概率密度函数，也即生成的数据和真实数据是一样的。在均衡点上 D 和 G 都不能得到进一步提升，并且判别器无法判断数据到底是来自真实样本还是伪造的数据，即 D（x）= 1/2。</p>\n<p>GAN学习的表征可用于多种应用，包括图像合成、语义图像编辑、风格迁移、图像超分辨率技术和分类。</p>\n<p>eg. ベッドルームの画像のデータセットを使って学習した結果です。一見本物と見間違ってしまいそうなレベルの画像を生成できていることが分かります。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-3.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>eg. Word2Vecという単語ベクトルで「王様」-「男」+「女」=「女王」という演算ができることは有名ですが、GANにおける入力であるzzベクトルを使っても同様の演算を行うことができます。下の例では、「サングラスをかけた男」-「男」+「女」=「サングラスをかけた女」という演算を行っています。</p>\n<hr>\n<center><img src=\"/image/DL/5/1-4.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>GAN的类别 - 内容来自：<a href=\"https://zhuanlan.zhihu.com/p/30346797\" title=\"Title\">生成对抗网络综述：从架构到训练技巧</a> </p>\n<h4 id=\"1-1-全连接-GAN\"><a href=\"#1-1-全连接-GAN\" class=\"headerlink\" title=\"1.1. 全连接 GAN\"></a>1.1. 全连接 GAN</h4><p>首个 GAN 架构在生成器与鉴别器上皆使用全连接神经网络。这种架构类型被应用于相对简单的图像数据库，即 MNIST（手写数字）、CIFAR-10（自然图像）和多伦多人脸数据集（TFD）。</p>\n<h4 id=\"1-2-卷积-GAN\"><a href=\"#1-2-卷积-GAN\" class=\"headerlink\" title=\"1.2.  卷积 GAN\"></a>1.2.  卷积 GAN</h4><p>● LAPGAN拉普拉斯金字塔形对抗网络（Laplacian pyramid GAN）</p>\n<p>真值图像本身被分解成拉普拉斯金字塔（Laplacian pyramid），并且条件性卷积 GAN 被训练在给定上一层的情况下生成每一层。</p>\n<p>● DCGAN（Deep Convolutional GAN）</p>\n<hr>\n<center><img src=\"/image/DL/5/1-5.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>DCGAN允许训练一对深度卷积生成器和判别器网络。在训练中使用带步长的卷积（strided convolution）和小步长卷积（fractionally-strided convolution），并在训练中学习空间下采样和上采样算子。</p>\n<h4 id=\"1-3-条件-GAN\"><a href=\"#1-3-条件-GAN\" class=\"headerlink\" title=\"1.3. 条件 GAN\"></a>1.3. 条件 GAN</h4><p>通过将生成器和判别器改造成条件类（class-conditional）而将（2D）GAN 框架扩展成条件设置。条件 GNN 的优势在于可以对多形式的数据生成提供更好的表征。条件 GAN 和 InfoGAN[16] 是平行的，它可以将噪声源分解为不可压缩源和一个「隐编码」（latent code），并可以通过最大化隐编码和生成器之间的交互信息而发现变化的隐藏因子。这个隐编码可用于在完全无监督的数据中发现目标类，即使这个隐编码是不明确的。由 InfoGAN 学到的表征看起来像是具备语义特征的，可以处理图貌中的复杂纠缠因素（包括姿势变化、光照和面部图像的情绪内容）。</p>\n<h4 id=\"1-4-GAN-推断模型\"><a href=\"#1-4-GAN-推断模型\" class=\"headerlink\" title=\"1.4. GAN 推断模型\"></a>1.4. GAN 推断模型</h4><p>GAN 的初始形式无法将给定的输入 x 映射为隐空间中的向量（在 GAN 的文献中，这通常被称为一种推断机制）。人们提出了几种反转预训练 GAN 的生成器的技术，比如各自独立提出的对抗性学习推断（Adversarially Learned Inference，ALI）和双向 GAN（Bidirectional GANs），它们能提供简单而有效的扩展，通过加入一个推断网络，使判别器共同测试数据空间和隐空间。</p>\n<p>这种形式下的生成器由两个网络组成：即编码器（推断网络）和解码器。它们同时被训练用于欺骗判别器。而判别器将接收到一个向量对（x,z），并决定其是否包含一个真实图像以及其编码，或者一个生成的图像样本以及相关的生成器的隐空间输入。</p>\n<h4 id=\"1-5-AAE-对抗自编码器\"><a href=\"#1-5-AAE-对抗自编码器\" class=\"headerlink\" title=\"1.5. AAE 对抗自编码器\"></a>1.5. AAE 对抗自编码器</h4><p>自编码器是由编码器和解码器组成的网络，学习将数据映射到内部隐表征中，再映射出来，即从数据空间中学习将图像（或其它）通过编码映射到隐空间中，再通过解码从隐空间映射回数据空间。这两个映射形成了一种重构运算，而这两个映射将被训练直到重构图像尽可能的接近初始图像。</p>\n<h3 id=\"2-生成式对抗网络的训练（GAN-Training）\"><a href=\"#2-生成式对抗网络的训练（GAN-Training）\" class=\"headerlink\" title=\"2. 生成式对抗网络的训练（GAN Training）\"></a>2. 生成式对抗网络的训练（GAN Training）</h3><p>生成对抗网络（GAN）提供了一种不需要大量标注训练数据就能学习深度表征的方式。它们通过反向传播算法分别更新两个网络以执行竞争性学习而达到训练目的。</p>\n<h4 id=\"2-1-训练\"><a href=\"#2-1-训练\" class=\"headerlink\" title=\"2.1. 训练\"></a>2.1. 训练</h4><p>训练的代价由一个价值函数 V(G,D) 评估，其包含了生成器和判别器的参数。</p>\n<p>训练过程可表示如下：</p>\n<hr>\n<center><img src=\"/image/DL/5/2-1.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>训练过程中，其中一个模型的参数被更新，同时另一个模型的参数固定不变。</p>\n<p>算法描述：<a href=\"https://arxiv.org/abs/1406.2661\" title=\"Title\">Goodfellow et al. (2014)</a><br>より引用</p>\n<hr>\n<center><img src=\"/image/DL/5/2-2.png\" alt=\"GAN\"></center> \n\n<hr>\n<p>推荐阅读 - <a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n<p>推荐阅读 - <a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\">はじめてのGAN</a> </p>\n<h4 id=\"2-2-训练技巧\"><a href=\"#2-2-训练技巧\" class=\"headerlink\" title=\"2.2. 训练技巧\"></a>2.2. 训练技巧</h4><p>用于图像生成的 GAN 训练的第一个重大改进是 Radford et al.提出的 DCGAN 架构。具体到训练中，研究者推荐在两种网络中使用批量归一化，以稳定深层模型中的训练。另一个建议是最小化用于提升深层模型训练可行性的全连接层的数量。最后，Radford et al.认为在判别器中间层使用leaky ReLU激活函数的性能优于使用常规的 ReLU 函数。</p>\n<p>● 特征匹配稍稍改变生成器的目标，以增加可获取的信息量。</p>\n<p>● 小批量判别（mini-batch discrimination）向判别器额外添加输入，该特征对小批量中的给定样本和其他样本的距离进行编码，防止模式崩溃（mode collapse），因为判别器能够轻易判断生成器是否生成同样的输出。</p>\n<p>● 启发式平均（heuristic averaging），如果网络参数偏离之前值的运行平均值，则会受到惩罚，这有助于收敛到平衡态。</p>\n<p>● 虚拟批量归一化（virtual batch normalization），可减少小批量内样本对其他样本的依赖性，方法是使用训练开始就确定的固定参考小批量（reference mini-batch）样本计算归一化的批量统计（batch statistics）。</p>\n<p>● 单边标签平滑（one-sided label smoothing）将判别器的目标从 1 替换为 0.9，使判别器的分类边界变得平滑，从而阻止判别器过于自信，为生成器提供较差的梯度。</p>\n<h3 id=\"3-基于TensorFlow的Keras实现（Keras）\"><a href=\"#3-基于TensorFlow的Keras实现（Keras）\" class=\"headerlink\" title=\"3. 基于TensorFlow的Keras实现（Keras）\"></a>3. 基于TensorFlow的Keras实现（Keras）</h3><p>内容来自：<a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n<p>详见代码：<a href=\"https://github.com/jiqizhixin/ML-Tutorial-Experiment\" title=\"Title\">机器之心 - GAN GitHub实现地址</a> </p>\n<h4 id=\"●-生成模型\"><a href=\"#●-生成模型\" class=\"headerlink\" title=\"● 生成模型\"></a>● 生成模型</h4><p>首先需要定义一个生成器 G，该生成器需要将输入的随机噪声变换为图像。以下是定义的生成模型，该模型首先输入有 100 个元素的向量，该向量随机生成于某分布。随后利用两个全连接层接连将该输入向量扩展到 1024 维和 128<em>7</em>7 维，后面就开始将全连接层所产生的一维张量重新塑造成二维张量，即 MNIST 中的灰度图。我们注意到该模型采用的激活函数为 tanh，所以也尝试过将其转换为 relu 函数，但发现生成模型如果转化为 relu 函数，那么它的输出就会成为一片灰色。</p>\n<p>由全连接传递的数据会经过几个上采样层和卷积层，我们注意到最后一个卷积层所采用的卷积核为 1，所以经过最后卷积层所生成的图像是一张二维灰度图像。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator_model</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"comment\">#下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    <span class=\"comment\">#添加一个全连接层，输入为100维向量，输出为1024维</span></div><div class=\"line\">    model.add(Dense(input_dim=<span class=\"number\">100</span>, output_dim=<span class=\"number\">1024</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个激活函数tanh</span></div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个全连接层，输出为128×7×7维度</span></div><div class=\"line\">    model.add(Dense(<span class=\"number\">128</span>*<span class=\"number\">7</span>*<span class=\"number\">7</span>))</div><div class=\"line\">    <span class=\"comment\">#添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1</span></div><div class=\"line\">    model.add(BatchNormalization())</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量</span></div><div class=\"line\">    model.add(Reshape((<span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">128</span>), input_shape=(<span class=\"number\">128</span>*<span class=\"number\">7</span>*<span class=\"number\">7</span>,)))</div><div class=\"line\">    <span class=\"comment\">#2维上采样层，即将数据的行和列分别重复2次</span></div><div class=\"line\">    model.add(UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变</span></div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), padding=<span class=\"string\">'same'</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    model.add(UpSampling2D(size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#卷积核设为1即输出图像的维度</span></div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">1</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>), padding=<span class=\"string\">'same'</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-判别模型\"><a href=\"#●-判别模型\" class=\"headerlink\" title=\"● 判别模型\"></a>● 判别模型</h4><p>判别模型相对来说就是比较传统的图像识别模型，前面我们可以按照经典的方法采用几个卷积层与最大池化层，而后再展开为一维张量并采用几个全连接层作为架构。我们尝试了将 tanh 激活函数改为 relu 激活函数，在前两个 epoch 基本上没有什么明显的变化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">discriminator_model</span><span class=\"params\">()</span>:</span></div><div class=\"line\">    <span class=\"comment\">#下面搭建判别器架构，同样采用序贯模型</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols）</span></div><div class=\"line\">    <span class=\"comment\">#在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维</span></div><div class=\"line\">    model.add(</div><div class=\"line\">            Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            padding=<span class=\"string\">'same'</span>,</div><div class=\"line\">            input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))</div><div class=\"line\">            )</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半</span></div><div class=\"line\">    model.add(MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    model.add(Conv2D(<span class=\"number\">128</span>, (<span class=\"number\">5</span>, <span class=\"number\">5</span>)))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    model.add(MaxPooling2D(pool_size=(<span class=\"number\">2</span>, <span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"comment\">#Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡</span></div><div class=\"line\">    model.add(Flatten())</div><div class=\"line\">    model.add(Dense(<span class=\"number\">1024</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'tanh'</span>))</div><div class=\"line\">    <span class=\"comment\">#一个结点进行二值分类，并采用sigmoid函数的输出作为概念</span></div><div class=\"line\">    model.add(Dense(<span class=\"number\">1</span>))</div><div class=\"line\">    model.add(Activation(<span class=\"string\">'sigmoid'</span>))</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-拼接\"><a href=\"#●-拼接\" class=\"headerlink\" title=\"● 拼接\"></a>● 拼接</h4><p>前面定义的是可生成图像的模型 G，而我们在训练生成模型时，需要固定判别模型 D 以极小化价值函数而寻求更好的生成模型，这就意味着我们需要将生成模型与判别模型拼接在一起，并固定 D 的权重以训练 G 的权重。下面就定义了这一过程，我们先添加前面定义的生成模型，再将定义的判别模型拼接在生成模型下方，并且我们将判别模型设置为不可训练。因此，训练这个组合模型才能真正更新生成模型的参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generator_containing_discriminator</span><span class=\"params\">(g, d)</span>:</span></div><div class=\"line\">    <span class=\"comment\">#将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片</span></div><div class=\"line\">    model = Sequential()</div><div class=\"line\">    <span class=\"comment\">#先添加生成器架构，再令d不可训练，即固定d</span></div><div class=\"line\">    <span class=\"comment\">#因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器</span></div><div class=\"line\">    model.add(g)</div><div class=\"line\">    d.trainable = <span class=\"keyword\">False</span></div><div class=\"line\">    model.add(d)</div><div class=\"line\">    <span class=\"keyword\">return</span> model</div></pre></td></tr></table></figure>\n<h4 id=\"●-训练\"><a href=\"#●-训练\" class=\"headerlink\" title=\"● 训练\"></a>● 训练</h4><p>以下训练过程可简述为：</p>\n<ul>\n<li>加载 MNIST 数据</li>\n<li>将数据分割为训练与测试集，并赋值给变量</li>\n<li>设置训练模型的超参数</li>\n<li>编译模型的训练过程</li>\n<li>在每一次迭代内，抽取生成图像与真实图像，并打上标注</li>\n<li>随后将数据投入到判别模型中，并进行训练与计算损失</li>\n<li>固定判别模型，训练生成模型并计算损失，结束这一次迭代</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(BATCH_SIZE)</span>:</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\"># 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。</span></div><div class=\"line\">    <span class=\"comment\">#下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz</span></div><div class=\"line\">    (X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class=\"line\">    <span class=\"comment\">#iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。</span></div><div class=\"line\">    <span class=\"comment\">#\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#转换字段类型，并将数据导入变量中</span></div><div class=\"line\">    X_train = (X_train.astype(np.float32) - <span class=\"number\">127.5</span>)/<span class=\"number\">127.5</span></div><div class=\"line\">    X_train = X_train[:, :, :, <span class=\"keyword\">None</span>]</div><div class=\"line\">    X_test = X_test[:, :, :, <span class=\"keyword\">None</span>]</div><div class=\"line\">    <span class=\"comment\"># X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#将定义好的模型架构赋值给特定的变量</span></div><div class=\"line\">    d = discriminator_model()</div><div class=\"line\">    g = generator_model()</div><div class=\"line\">    d_on_g = generator_containing_discriminator(g, d)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#定义生成器模型判别器模型更新所使用的优化算法及超参数</span></div><div class=\"line\">    d_optim = SGD(lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>, nesterov=<span class=\"keyword\">True</span>)</div><div class=\"line\">    g_optim = SGD(lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>, nesterov=<span class=\"keyword\">True</span>)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的</span></div><div class=\"line\">    g.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=<span class=\"string\">\"SGD\"</span>)</div><div class=\"line\">    d_on_g.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=g_optim)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。</span></div><div class=\"line\">    d.trainable = <span class=\"keyword\">True</span></div><div class=\"line\">    d.compile(loss=<span class=\"string\">'binary_crossentropy'</span>, optimizer=d_optim)</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">#下面在满足epoch条件下进行训练</span></div><div class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">30</span>):</div><div class=\"line\">        print(<span class=\"string\">\"Epoch is\"</span>, epoch)</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"comment\">#计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度</span></div><div class=\"line\">        print(<span class=\"string\">\"Number of batches\"</span>, int(X_train.shape[<span class=\"number\">0</span>]/BATCH_SIZE))</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"comment\">#在一个epoch内进行迭代训练</span></div><div class=\"line\">        <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(int(X_train.shape[<span class=\"number\">0</span>]/BATCH_SIZE)):</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本</span></div><div class=\"line\">            noise = np.random.uniform(<span class=\"number\">-1</span>, <span class=\"number\">1</span>, size=(BATCH_SIZE, <span class=\"number\">100</span>))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#抽取一个批量的真实图片</span></div><div class=\"line\">            image_batch = X_train[index*BATCH_SIZE:(index+<span class=\"number\">1</span>)*BATCH_SIZE]</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录</span></div><div class=\"line\">            generated_images = g.predict(noise, verbose=<span class=\"number\">0</span>)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#每经过100次迭代输出一张生成的图片</span></div><div class=\"line\">            <span class=\"keyword\">if</span> index % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</div><div class=\"line\">                image = combine_images(generated_images)</div><div class=\"line\">                image = image*<span class=\"number\">127.5</span>+<span class=\"number\">127.5</span></div><div class=\"line\">                Image.fromarray(image.astype(np.uint8)).save(</div><div class=\"line\">                    <span class=\"string\">\"./GAN/\"</span>+str(epoch)+<span class=\"string\">\"_\"</span>+str(index)+<span class=\"string\">\".png\"</span>)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下</span></div><div class=\"line\">            X = np.concatenate((image_batch, generated_images))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片</span></div><div class=\"line\">            y = [<span class=\"number\">1</span>] * BATCH_SIZE + [<span class=\"number\">0</span>] * BATCH_SIZE</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#判别器的损失；在一个batch的数据上进行一次参数更新</span></div><div class=\"line\">            d_loss = d.train_on_batch(X, y)</div><div class=\"line\">            print(<span class=\"string\">\"batch %d d_loss : %f\"</span> % (index, d_loss))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#随机生成的噪声服从均匀分布</span></div><div class=\"line\">            noise = np.random.uniform(<span class=\"number\">-1</span>, <span class=\"number\">1</span>, (BATCH_SIZE, <span class=\"number\">100</span>))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#固定判别器</span></div><div class=\"line\">            d.trainable = <span class=\"keyword\">False</span></div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#计算生成器损失；在一个batch的数据上进行一次参数更新</span></div><div class=\"line\">            g_loss = d_on_g.train_on_batch(noise, [<span class=\"number\">1</span>] * BATCH_SIZE)</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#令判别器可训练</span></div><div class=\"line\">            d.trainable = <span class=\"keyword\">True</span></div><div class=\"line\">            print(<span class=\"string\">\"batch %d g_loss : %f\"</span> % (index, g_loss))</div><div class=\"line\">            </div><div class=\"line\">            <span class=\"comment\">#每100次迭代保存一次生成器和判别器的权重</span></div><div class=\"line\">            <span class=\"keyword\">if</span> index % <span class=\"number\">100</span> == <span class=\"number\">9</span>:</div><div class=\"line\">                g.save_weights(<span class=\"string\">'generator'</span>, <span class=\"keyword\">True</span>)</div><div class=\"line\">                d.save_weights(<span class=\"string\">'discriminator'</span>, <span class=\"keyword\">True</span>)</div></pre></td></tr></table></figure>\n<h3 id=\"程序实例（Program-Example）\"><a href=\"#程序实例（Program-Example）\" class=\"headerlink\" title=\"程序实例（Program Example）\"></a>程序实例（Program Example）</h3><ul>\n<li><a href=\"https://github.com/HJTSO/face-generation/blob/master/dlnd_face_generation.ipynb\" title=\"Title\">Github Link</a> </li>\n</ul>\n<h3 id=\"参考资料（Reference）\"><a href=\"#参考资料（Reference）\" class=\"headerlink\" title=\"参考资料（Reference）\"></a>参考资料（Reference）</h3><ul>\n<li><p><a href=\"https://zhuanlan.zhihu.com/p/30346797\" title=\"Title\">生成对抗网络综述：从架构到训练技巧</a> </p>\n</li>\n<li><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731540&amp;idx=1&amp;sn=193457603fe11b89f3d298ac1799b9fd&amp;chksm=871b306ab06cb97c502af9552657b8e73f1f5286bc4cc71b021f64604fd53dae3f026bc9ac69&amp;scene=21#wechat_redirect\" title=\"Title\">机器之心GitHub项目：GAN完整理论推导与实现</a> </p>\n</li>\n<li><p><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650730721&amp;idx=2&amp;sn=95b97b80188f507c409f4c72bd0a2767&amp;chksm=871b349fb06cbd891771f72d77563f77986afc9b144f42c8232db44c7c56c1d2bc019458c4e4&amp;scene=21#wechat_redirect\" title=\"Title\">深度 | 生成对抗网络初学入门</a> </p>\n</li>\n<li><p><a href=\"https://elix-tech.github.io/ja/2017/02/06/gan.html\" title=\"Title\">はじめてのGAN</a> </p>\n</li>\n</ul>\n"},{"title":"JUDO-Kodokan 柔道-講道館","lang":"zh","date":"2017-11-02T10:18:50.000Z","_content":"\n<center>![kodokan](/image/kodokan/kodokan.png)</center> \n\n--------------------------------- \n\n## 講道館简介\n\n&#8195;&#8195;讲道馆是在1882年由嘉纳治五郎所创立，坐落在日本东京文京区。讲道馆是所有柔道界的总本部，“講道館”的意思是学习兼传播柔道理念的道场。讲道馆对于喜爱柔道者来说是个特殊的地方，应该说，对柔道运动员来说，讲道馆一游应该会列为自己人生旅程清单，而道场也迎来世界各国的众多柔道修行者。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/1/kano_1.jpg)</center> \n\n--------------------------------- \n\n## 講道館内部 \n\n&#8195;&#8195;讲道馆由本馆和新馆两部分组成。本馆(7层)有全日本柔道联盟以及相关的事务局，而新馆(8层)有柔道图书馆以及用于柔道的训练道场。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/two_buildings.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;讲道馆的本馆大楼一共有七层，由于现在目前功能主要用于处理柔道事务。在本馆一楼会引导想参观或者见习讲道馆的人到新馆。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_1.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_3.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_4.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano1.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;嘉纳治五郎本尊像。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano3.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;讲道馆的新馆大楼一共有八层楼与一层地下室。本文重点介绍新馆。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/newbuilding.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/nanai.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_2.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;地下一楼里有自助餐厅和会议厅，一楼则有停车场和商店。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_1.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;二楼则有图书馆和会议室。可细分为:嘉纳治五郎纪念厅、柔道历史纪念厅、文物展示厅、储藏室。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_1.jpg)</center> \n\n--------------------------------- \n\n● 纪念讲道馆创立100周年之际建成的讲道馆国际柔道中心2楼的资料馆。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_2.jpg)</center> \n\n--------------------------------- \n\n● 资料展览室。“无心而入自然之妙，无为而穷变化之神”。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_3.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_4.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n&#8195;&#8195;纪念厅里展示许多关于柔道发展史以及许多知名的柔道者的纪念品，例如:手稿、相片。图书馆则收藏有超过7000本关于柔道的书籍。其中二楼又有四间的研究室，一号研究室主要研究有关于柔道的历史与理论、二号研究室研究柔道心理学、三号研究室研究运动生理学、四号研究室研究有关柔道的哲学。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_5.jpg)</center> \n\n--------------------------------- \n\n● 展示嘉纳治五郎的资料的师范室。照片为金井平三作肖像油画和本人的笔迹、常用的大衣等。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n&#8195;&#8195;每一年讲道馆会聚集世界各地研究柔道的学者举办一次研讨会。\n\n&#8195;&#8195;三楼则可提供住宿（讲道馆可专门为来自海外的修行者开设专业班，可安排专家私人授课及在大型道场中的训练等。新馆的三楼即是住宿的地方）。一共有五间大宿舍间，每间可容纳二十人。  \n\n&#8195;&#8195;四楼为更衣室，给教练与学员用于更换柔道服的地方，有提供洗浴。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/4.jpg)</center> \n\n--------------------------------- \n\n● 四楼的柔道课程表\n\n&#8195;&#8195;五楼、六楼、七楼则为柔道的练习场地。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/5_female_1.jpg)</center> \n\n--------------------------------- \n\n● 五楼的女子部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/5_female_2.jpg)</center> \n\n--------------------------------- \n\n● 五楼的女子部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_international.jpg)</center> \n\n--------------------------------- \n\n● 六楼的国际部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_1.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_2.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_3.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_0.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;七楼大道场是讲道馆最大的道场，也是最重要的道场，道垫非常舒服，楼顶直通天花板，光线非常的充足。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_00.jpg)</center> \n\n--------------------------------- \n\n● 进入柔道馆前，要记得先柔道礼仪（即90度弯腰行礼）\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_kano_2.jpg)</center> \n\n--------------------------------- \n\n● 位于大道场的嘉纳治五郎的头像，柔道练习开始和结束都要向嘉纳先生行座礼。\n\n--------------------------------- \n\n&#8195;&#8195;八楼则提供座位给参观者观看七楼的柔道者练习。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_0.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/all.jpg)</center> \n\n--------------------------------- \n\n● 先上一张从八楼看七楼大道场的全景图\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_1.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_3.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_4.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_5.jpg)</center>  \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_6.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_7.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/all_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/Summer.png)</center> \n\n--------------------------------- \n\n● 大道馆每在暑期的时候会举办暑期讲习会，吸引来自日本以及国外的训练营柔道者\n\n\n--------------------------------- \n\n&#8195;&#8195;在新馆一楼处有一家柔道用品店，除了柔道相关的用具外，也有其它武术的器具。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_1.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_2.jpg)</center> \n\n---------------------------------\n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_3.jpg)</center> \n\n---------------------------------\n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n---------------------------------\n\n&#8195;&#8195;在新馆的一层地下室处有健身房，练习之前要先到4喽登记处登记，周一至周五是到20:00为止，周六19:00。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_1.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_2.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_3.jpg)</center> \n\n\n--------------------------------- \n\n## 講道館段位\n\n&#8195;&#8195;讲道馆的段位需要经过考试，成绩合格后才可以晋升，并且每个段位要满足最小年龄，也就是说，如果太年轻还不能考。而且，讲道馆的考段资格中有一项是积分，而积分必须通过规定的比赛获得。另外，讲道馆的男女柔道段位系统也有所差别。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/male.jpg)</center> \n\n---------------------------------\n\n● 男子段位评定说明（部分）\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/female.jpg)</center> \n\n---------------------------------\n\n● 女子段位评定说明（部分）  \n\n● 具体请参考讲道馆官网：講道館 - [昇段資格について](http://kodokanjudoinstitute.org/activity/grade/ \"Title\")\n\n&#8195;&#8195;柔道依选手的水准分段位，通常以腰带的颜色来分辨段位的高低，未入段的新手为白带，一到五段为黑带，六到八段为红白间隔带，九到十段为红带，目前世界上只有极少数人到达红带的地位。目前为止个人观察，讲道馆的教练大部分是红白间隔带，少数是黑带。未入段的柔道者带色分别是：咖啡色、蓝色、橘色、绿色、黄色、白色。但是讲道馆的人一般都是白带一直系着直到黑带。（另外，讲道馆和国际柔联段位系统还有点不太一样，关于国际柔联段位系统这边了解清楚到时再更新）\n\n&#8195;&#8195;对于游客来说，可以进入道讲道馆里参观或者亲身练习。但是如果想要参加练习需要至柜台办理手续，一般来说如果只有练习一堂课，通常都是学习受身课程。\n\n● 关于学费\n\n&#8195;&#8195;参考下图，单次练习的话是800日元（折合人民币五十元左右）；如果要长期在讲道馆学习，只能选择普通班以及特修班，除了刚开始第一次要额外提交卡费以及入馆费之外，其余的每月只需交5000日元（折合人民币三百多）。其实这个价格相对国内来说已经很优惠，讲道馆不是以盈利为目的，更多的是希望推广柔道。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/fee.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/info.jpg)</center> \n\n--------------------------------- \n\n## 講道館感想\n  \n&#8195;&#8195;吾辈之前在深圳开始练习柔道，2017年初开始在讲道馆学习。目前为止的对于讲道馆学习柔道感受是教练普遍很细心，被发现一个动作不对的话会让反复练习；而且能与来自世界不同国家的朋友一起练习柔道很开心。吾辈所在的是国际柔道部的普通班，来自其它国家的学生比较多，授课的教练大部分还是用日语讲，一些基本动作和身体部位的表达有时会用英语表达，估计讲道馆的教练有受过专门的培训。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/4/eyecatch.jpg)</center> \n\n---------------------------------\n\n&#8195;&#8195;周一至周六普通班的上课时间是17:30-19:00，跟在之前在国内练习柔道课相比（吾辈国内只在一家柔道馆练习过，所以不能代表全部），讲道馆普通课的热身时间比较长，基本的热身动作、前后滚翻、倒立、受身动作……这些一般都会超半小时，剩下的时间会教一到三个基本的投技或者固技。\n\n&#8195;&#8195;普通班的次数上满40回后，可以进行特修班申请，经过考试之后就可以晋级。特修班还是在白带系统內……但是相对普通班，可以有实战练习（乱取り）的时间，即两人柔道实战对位练习。特修班后，可以考试申请黑带的资格。\n\n&#8195;&#8195;普通班与特修班周一至周六的柔道教练都不一样，有些教练的会比较严格，热身阶段已经是体力大耗损；有些则会更偏重动作的讲解，循循善诱，热身轻松但是跟讲很多柔道理论。另外，日本有私人的柔道馆，比如讲道馆一些教练自己会有另外自己的柔道馆，训练风格因教练而异。\n\n&#8195;&#8195;讲道馆每个月会举行一次“柔之形”的课程，加深学员对柔道动作的记忆。日本有办关于“柔之形”的全国大赛，像是讲道馆有柔道形国际竞技大会，比赛一般是两人为一队，一个叫受、一个叫取，两个人一起演练招式，考核内容除了招式、技术外，还要考核礼法等。因为柔道十分注重“心技体”。\n\n&#8195;&#8195;刚开始在这里练习柔道不习惯的地方应该是柔道用语，因为之前在国内使用的都是中文发音用语，像“背负投”，“大腰”，“袈裟固”，国际柔联和日本使用的柔道用语是的发音是“Seoi-nage せおいなげ”，“O-goshi おおごし”，“Kesa-gatame けさがため”。后来花了一些时间专门去记住。\n\n&#8195;&#8195;这边整理了柔道的中日英的柔道用语，希望对有需要的人有所帮助：  \n\n&#8195;&#8195;● 柔道用語： [柔道用語](https://hjtso.github.io/2016/11/30/ja-%E6%9F%94%E9%81%93%E7%94%A8%E8%AA%9E-Judo-terminology/ja/ \"Title\")\n\n---------------------------------\n\n&#8195;&#8195;最后推荐一部关于练习柔道的书《柔道-世界で勝つための実戦的稽古 (差がつく練習法)》，作者：林田和孝\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/4/book.jpg)</center> \n\n---------------------------------\n\n● [亚马逊购买链接](https://www.amazon.co.jp/gp/product/4583108419/ref=oh_aui_detailpage_o00_s00?ie=UTF8&psc=1 \"Title\")\n\n--------------------------------- \n\n#### ● 講道館について：\n\n&#8195;&#8195;Bunkyo-ku,Tokyo,Japan\n&#8195;&#8195;東京都文京区春日1-16-30\n&#8195;&#8195;道场开馆时间∶周一至周六\n&#8195;&#8195;入馆费∶免费\n\n&#8195;&#8195;[講道館官网](http://kodokanjudoinstitute.org/ \"Title\")\n&#8195;&#8195;[講道館Facebook主页](https://www.facebook.com/kodokanjudoinstitute/ \"Title\")\n\n#### ● 推荐视频について：\n\n&#8195;&#8195;[柔道-大野将平](https://www.bilibili.com/video/av14169956/?zw \"Title\")\n&#8195;&#8195;[講道館形研修部／Weekly Kata Class](https://www.facebook.com/pg/kodokanjudoinstitute/videos/?ref=page_internal \"Title\")\n&#8195;&#8195;[讲道馆柔道套路全集](https://v.qq.com/x/page/a0377r8qmso.html \"Title\")\n&#8195;&#8195;[柔道的创始者-嘉纳治五郎](http://open.163.com/movie/2016/6/E/P/MBPKPMI4N_MBPKUO1EP.html \"Title\")  \n\n\n<center><object width=\"640\" height=\"360\"><param name=\"movie\" value=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\"></param><param name=\"allowScriptAccess\" value=\"always\"></param><param name=\"wmode\" value=\"transparent\"></param><embed src=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\" type=\"application/x-shockwave-flash\" width=\"640\" height=\"360\" allowFullScreen=\"true\" wmode=\"transparent\" allowScriptAccess=\"always\"></embed></object></center>  \n\n","source":"_posts/zh/JUDO-Kodokan 柔道-講道館.md","raw":"\n---\ntitle: JUDO-Kodokan 柔道-講道館\nlang: zh\ndate: 2017-11-02 19:18:50\ntags: Judo\n---\n\n<center>![kodokan](/image/kodokan/kodokan.png)</center> \n\n--------------------------------- \n\n## 講道館简介\n\n&#8195;&#8195;讲道馆是在1882年由嘉纳治五郎所创立，坐落在日本东京文京区。讲道馆是所有柔道界的总本部，“講道館”的意思是学习兼传播柔道理念的道场。讲道馆对于喜爱柔道者来说是个特殊的地方，应该说，对柔道运动员来说，讲道馆一游应该会列为自己人生旅程清单，而道场也迎来世界各国的众多柔道修行者。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/1/kano_1.jpg)</center> \n\n--------------------------------- \n\n## 講道館内部 \n\n&#8195;&#8195;讲道馆由本馆和新馆两部分组成。本馆(7层)有全日本柔道联盟以及相关的事务局，而新馆(8层)有柔道图书馆以及用于柔道的训练道场。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/two_buildings.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;讲道馆的本馆大楼一共有七层，由于现在目前功能主要用于处理柔道事务。在本馆一楼会引导想参观或者见习讲道馆的人到新馆。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_1.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_3.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/main_4.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano1.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;嘉纳治五郎本尊像。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/main/kano3.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;讲道馆的新馆大楼一共有八层楼与一层地下室。本文重点介绍新馆。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/newbuilding.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/nanai.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_2.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;地下一楼里有自助餐厅和会议厅，一楼则有停车场和商店。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_1.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;二楼则有图书馆和会议室。可细分为:嘉纳治五郎纪念厅、柔道历史纪念厅、文物展示厅、储藏室。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_1.jpg)</center> \n\n--------------------------------- \n\n● 纪念讲道馆创立100周年之际建成的讲道馆国际柔道中心2楼的资料馆。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_2.jpg)</center> \n\n--------------------------------- \n\n● 资料展览室。“无心而入自然之妙，无为而穷变化之神”。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_3.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_4.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n&#8195;&#8195;纪念厅里展示许多关于柔道发展史以及许多知名的柔道者的纪念品，例如:手稿、相片。图书馆则收藏有超过7000本关于柔道的书籍。其中二楼又有四间的研究室，一号研究室主要研究有关于柔道的历史与理论、二号研究室研究柔道心理学、三号研究室研究运动生理学、四号研究室研究有关柔道的哲学。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/2_library_5.jpg)</center> \n\n--------------------------------- \n\n● 展示嘉纳治五郎的资料的师范室。照片为金井平三作肖像油画和本人的笔迹、常用的大衣等。  \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n&#8195;&#8195;每一年讲道馆会聚集世界各地研究柔道的学者举办一次研讨会。\n\n&#8195;&#8195;三楼则可提供住宿（讲道馆可专门为来自海外的修行者开设专业班，可安排专家私人授课及在大型道场中的训练等。新馆的三楼即是住宿的地方）。一共有五间大宿舍间，每间可容纳二十人。  \n\n&#8195;&#8195;四楼为更衣室，给教练与学员用于更换柔道服的地方，有提供洗浴。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/4.jpg)</center> \n\n--------------------------------- \n\n● 四楼的柔道课程表\n\n&#8195;&#8195;五楼、六楼、七楼则为柔道的练习场地。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/5_female_1.jpg)</center> \n\n--------------------------------- \n\n● 五楼的女子部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/5_female_2.jpg)</center> \n\n--------------------------------- \n\n● 五楼的女子部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_international.jpg)</center> \n\n--------------------------------- \n\n● 六楼的国际部道场\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_1.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_2.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/6_school_3.jpg)</center> \n\n--------------------------------- \n\n● 六楼的学校道场，家长正在看孩子练柔道\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_0.jpg)</center> \n\n--------------------------------- \n\n&#8195;&#8195;七楼大道场是讲道馆最大的道场，也是最重要的道场，道垫非常舒服，楼顶直通天花板，光线非常的充足。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_00.jpg)</center> \n\n--------------------------------- \n\n● 进入柔道馆前，要记得先柔道礼仪（即90度弯腰行礼）\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/7_kano_2.jpg)</center> \n\n--------------------------------- \n\n● 位于大道场的嘉纳治五郎的头像，柔道练习开始和结束都要向嘉纳先生行座礼。\n\n--------------------------------- \n\n&#8195;&#8195;八楼则提供座位给参观者观看七楼的柔道者练习。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_0.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/all.jpg)</center> \n\n--------------------------------- \n\n● 先上一张从八楼看七楼大道场的全景图\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_1.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_3.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_4.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_5.jpg)</center>  \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_6.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/8_7.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/all_2.jpg)</center> \n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/7-8/Summer.png)</center> \n\n--------------------------------- \n\n● 大道馆每在暑期的时候会举办暑期讲习会，吸引来自日本以及国外的训练营柔道者\n\n\n--------------------------------- \n\n&#8195;&#8195;在新馆一楼处有一家柔道用品店，除了柔道相关的用具外，也有其它武术的器具。\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_1.jpg)</center> \n\n--------------------------------- \n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_2.jpg)</center> \n\n---------------------------------\n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n--------------------------------- \n\n<center>![kodokan](/image/kodokan/2/1-6/1_shop_3.jpg)</center> \n\n---------------------------------\n\n● 此图来自 - [stroll-tips](https://www.stroll-tips.com/zh/kodokan-judo/ \"Title\")\n\n---------------------------------\n\n&#8195;&#8195;在新馆的一层地下室处有健身房，练习之前要先到4喽登记处登记，周一至周五是到20:00为止，周六19:00。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_1.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_2.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/2/gym/gym_3.jpg)</center> \n\n\n--------------------------------- \n\n## 講道館段位\n\n&#8195;&#8195;讲道馆的段位需要经过考试，成绩合格后才可以晋升，并且每个段位要满足最小年龄，也就是说，如果太年轻还不能考。而且，讲道馆的考段资格中有一项是积分，而积分必须通过规定的比赛获得。另外，讲道馆的男女柔道段位系统也有所差别。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/male.jpg)</center> \n\n---------------------------------\n\n● 男子段位评定说明（部分）\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/female.jpg)</center> \n\n---------------------------------\n\n● 女子段位评定说明（部分）  \n\n● 具体请参考讲道馆官网：講道館 - [昇段資格について](http://kodokanjudoinstitute.org/activity/grade/ \"Title\")\n\n&#8195;&#8195;柔道依选手的水准分段位，通常以腰带的颜色来分辨段位的高低，未入段的新手为白带，一到五段为黑带，六到八段为红白间隔带，九到十段为红带，目前世界上只有极少数人到达红带的地位。目前为止个人观察，讲道馆的教练大部分是红白间隔带，少数是黑带。未入段的柔道者带色分别是：咖啡色、蓝色、橘色、绿色、黄色、白色。但是讲道馆的人一般都是白带一直系着直到黑带。（另外，讲道馆和国际柔联段位系统还有点不太一样，关于国际柔联段位系统这边了解清楚到时再更新）\n\n&#8195;&#8195;对于游客来说，可以进入道讲道馆里参观或者亲身练习。但是如果想要参加练习需要至柜台办理手续，一般来说如果只有练习一堂课，通常都是学习受身课程。\n\n● 关于学费\n\n&#8195;&#8195;参考下图，单次练习的话是800日元（折合人民币五十元左右）；如果要长期在讲道馆学习，只能选择普通班以及特修班，除了刚开始第一次要额外提交卡费以及入馆费之外，其余的每月只需交5000日元（折合人民币三百多）。其实这个价格相对国内来说已经很优惠，讲道馆不是以盈利为目的，更多的是希望推广柔道。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/fee.jpg)</center> \n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/3/info.jpg)</center> \n\n--------------------------------- \n\n## 講道館感想\n  \n&#8195;&#8195;吾辈之前在深圳开始练习柔道，2017年初开始在讲道馆学习。目前为止的对于讲道馆学习柔道感受是教练普遍很细心，被发现一个动作不对的话会让反复练习；而且能与来自世界不同国家的朋友一起练习柔道很开心。吾辈所在的是国际柔道部的普通班，来自其它国家的学生比较多，授课的教练大部分还是用日语讲，一些基本动作和身体部位的表达有时会用英语表达，估计讲道馆的教练有受过专门的培训。\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/4/eyecatch.jpg)</center> \n\n---------------------------------\n\n&#8195;&#8195;周一至周六普通班的上课时间是17:30-19:00，跟在之前在国内练习柔道课相比（吾辈国内只在一家柔道馆练习过，所以不能代表全部），讲道馆普通课的热身时间比较长，基本的热身动作、前后滚翻、倒立、受身动作……这些一般都会超半小时，剩下的时间会教一到三个基本的投技或者固技。\n\n&#8195;&#8195;普通班的次数上满40回后，可以进行特修班申请，经过考试之后就可以晋级。特修班还是在白带系统內……但是相对普通班，可以有实战练习（乱取り）的时间，即两人柔道实战对位练习。特修班后，可以考试申请黑带的资格。\n\n&#8195;&#8195;普通班与特修班周一至周六的柔道教练都不一样，有些教练的会比较严格，热身阶段已经是体力大耗损；有些则会更偏重动作的讲解，循循善诱，热身轻松但是跟讲很多柔道理论。另外，日本有私人的柔道馆，比如讲道馆一些教练自己会有另外自己的柔道馆，训练风格因教练而异。\n\n&#8195;&#8195;讲道馆每个月会举行一次“柔之形”的课程，加深学员对柔道动作的记忆。日本有办关于“柔之形”的全国大赛，像是讲道馆有柔道形国际竞技大会，比赛一般是两人为一队，一个叫受、一个叫取，两个人一起演练招式，考核内容除了招式、技术外，还要考核礼法等。因为柔道十分注重“心技体”。\n\n&#8195;&#8195;刚开始在这里练习柔道不习惯的地方应该是柔道用语，因为之前在国内使用的都是中文发音用语，像“背负投”，“大腰”，“袈裟固”，国际柔联和日本使用的柔道用语是的发音是“Seoi-nage せおいなげ”，“O-goshi おおごし”，“Kesa-gatame けさがため”。后来花了一些时间专门去记住。\n\n&#8195;&#8195;这边整理了柔道的中日英的柔道用语，希望对有需要的人有所帮助：  \n\n&#8195;&#8195;● 柔道用語： [柔道用語](https://hjtso.github.io/2016/11/30/ja-%E6%9F%94%E9%81%93%E7%94%A8%E8%AA%9E-Judo-terminology/ja/ \"Title\")\n\n---------------------------------\n\n&#8195;&#8195;最后推荐一部关于练习柔道的书《柔道-世界で勝つための実戦的稽古 (差がつく練習法)》，作者：林田和孝\n\n---------------------------------\n\n<center>![kodokan](/image/kodokan/4/book.jpg)</center> \n\n---------------------------------\n\n● [亚马逊购买链接](https://www.amazon.co.jp/gp/product/4583108419/ref=oh_aui_detailpage_o00_s00?ie=UTF8&psc=1 \"Title\")\n\n--------------------------------- \n\n#### ● 講道館について：\n\n&#8195;&#8195;Bunkyo-ku,Tokyo,Japan\n&#8195;&#8195;東京都文京区春日1-16-30\n&#8195;&#8195;道场开馆时间∶周一至周六\n&#8195;&#8195;入馆费∶免费\n\n&#8195;&#8195;[講道館官网](http://kodokanjudoinstitute.org/ \"Title\")\n&#8195;&#8195;[講道館Facebook主页](https://www.facebook.com/kodokanjudoinstitute/ \"Title\")\n\n#### ● 推荐视频について：\n\n&#8195;&#8195;[柔道-大野将平](https://www.bilibili.com/video/av14169956/?zw \"Title\")\n&#8195;&#8195;[講道館形研修部／Weekly Kata Class](https://www.facebook.com/pg/kodokanjudoinstitute/videos/?ref=page_internal \"Title\")\n&#8195;&#8195;[讲道馆柔道套路全集](https://v.qq.com/x/page/a0377r8qmso.html \"Title\")\n&#8195;&#8195;[柔道的创始者-嘉纳治五郎](http://open.163.com/movie/2016/6/E/P/MBPKPMI4N_MBPKUO1EP.html \"Title\")  \n\n\n<center><object width=\"640\" height=\"360\"><param name=\"movie\" value=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\"></param><param name=\"allowScriptAccess\" value=\"always\"></param><param name=\"wmode\" value=\"transparent\"></param><embed src=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\" type=\"application/x-shockwave-flash\" width=\"640\" height=\"360\" allowFullScreen=\"true\" wmode=\"transparent\" allowScriptAccess=\"always\"></embed></object></center>  \n\n","slug":"zh-JUDO-Kodokan-柔道-講道館","published":1,"updated":"2018-01-01T13:13:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckipsq8zo002bog64jsy0jmv1","content":"<center><img src=\"/image/kodokan/kodokan.png\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館简介\"><a href=\"#講道館简介\" class=\"headerlink\" title=\"講道館简介\"></a>講道館简介</h2><p>&#8195;&#8195;讲道馆是在1882年由嘉纳治五郎所创立，坐落在日本东京文京区。讲道馆是所有柔道界的总本部，“講道館”的意思是学习兼传播柔道理念的道场。讲道馆对于喜爱柔道者来说是个特殊的地方，应该说，对柔道运动员来说，讲道馆一游应该会列为自己人生旅程清单，而道场也迎来世界各国的众多柔道修行者。</p>\n<hr>\n<center><img src=\"/image/kodokan/1/kano_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館内部\"><a href=\"#講道館内部\" class=\"headerlink\" title=\"講道館内部\"></a>講道館内部</h2><p>&#8195;&#8195;讲道馆由本馆和新馆两部分组成。本馆(7层)有全日本柔道联盟以及相关的事务局，而新馆(8层)有柔道图书馆以及用于柔道的训练道场。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/two_buildings.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;讲道馆的本馆大楼一共有七层，由于现在目前功能主要用于处理柔道事务。在本馆一楼会引导想参观或者见习讲道馆的人到新馆。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;嘉纳治五郎本尊像。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;讲道馆的新馆大楼一共有八层楼与一层地下室。本文重点介绍新馆。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/newbuilding.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/nanai.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;地下一楼里有自助餐厅和会议厅，一楼则有停车场和商店。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;二楼则有图书馆和会议室。可细分为:嘉纳治五郎纪念厅、柔道历史纪念厅、文物展示厅、储藏室。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 纪念讲道馆创立100周年之际建成的讲道馆国际柔道中心2楼的资料馆。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 资料展览室。“无心而入自然之妙，无为而穷变化之神”。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<p>&#8195;&#8195;纪念厅里展示许多关于柔道发展史以及许多知名的柔道者的纪念品，例如:手稿、相片。图书馆则收藏有超过7000本关于柔道的书籍。其中二楼又有四间的研究室，一号研究室主要研究有关于柔道的历史与理论、二号研究室研究柔道心理学、三号研究室研究运动生理学、四号研究室研究有关柔道的哲学。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_5.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 展示嘉纳治五郎的资料的师范室。照片为金井平三作肖像油画和本人的笔迹、常用的大衣等。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<p>&#8195;&#8195;每一年讲道馆会聚集世界各地研究柔道的学者举办一次研讨会。</p>\n<p>&#8195;&#8195;三楼则可提供住宿（讲道馆可专门为来自海外的修行者开设专业班，可安排专家私人授课及在大型道场中的训练等。新馆的三楼即是住宿的地方）。一共有五间大宿舍间，每间可容纳二十人。  </p>\n<p>&#8195;&#8195;四楼为更衣室，给教练与学员用于更换柔道服的地方，有提供洗浴。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 四楼的柔道课程表</p>\n<p>&#8195;&#8195;五楼、六楼、七楼则为柔道的练习场地。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/5_female_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 五楼的女子部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/5_female_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 五楼的女子部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_international.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的国际部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_0.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;七楼大道场是讲道馆最大的道场，也是最重要的道场，道垫非常舒服，楼顶直通天花板，光线非常的充足。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_00.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 进入柔道馆前，要记得先柔道礼仪（即90度弯腰行礼）</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_kano_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 位于大道场的嘉纳治五郎的头像，柔道练习开始和结束都要向嘉纳先生行座礼。</p>\n<hr>\n<p>&#8195;&#8195;八楼则提供座位给参观者观看七楼的柔道者练习。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_0.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/all.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 先上一张从八楼看七楼大道场的全景图</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_5.jpg\" alt=\"kodokan\"></center>  \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_6.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_7.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/all_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/Summer.png\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 大道馆每在暑期的时候会举办暑期讲习会，吸引来自日本以及国外的训练营柔道者</p>\n<hr>\n<p>&#8195;&#8195;在新馆一楼处有一家柔道用品店，除了柔道相关的用具外，也有其它武术的器具。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\" target=\"_blank\" rel=\"external\">stroll-tips</a></p>\n<hr>\n<p>&#8195;&#8195;在新馆的一层地下室处有健身房，练习之前要先到4喽登记处登记，周一至周五是到20:00为止，周六19:00。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_3.jpg\" alt=\"kodokan\"></center> \n\n\n<hr>\n<h2 id=\"講道館段位\"><a href=\"#講道館段位\" class=\"headerlink\" title=\"講道館段位\"></a>講道館段位</h2><p>&#8195;&#8195;讲道馆的段位需要经过考试，成绩合格后才可以晋升，并且每个段位要满足最小年龄，也就是说，如果太年轻还不能考。而且，讲道馆的考段资格中有一项是积分，而积分必须通过规定的比赛获得。另外，讲道馆的男女柔道段位系统也有所差别。</p>\n<hr>\n<center><img src=\"/image/kodokan/3/male.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 男子段位评定说明（部分）</p>\n<hr>\n<center><img src=\"/image/kodokan/3/female.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 女子段位评定说明（部分）  </p>\n<p>● 具体请参考讲道馆官网：講道館 - <a href=\"http://kodokanjudoinstitute.org/activity/grade/\" title=\"Title\" target=\"_blank\" rel=\"external\">昇段資格について</a></p>\n<p>&#8195;&#8195;柔道依选手的水准分段位，通常以腰带的颜色来分辨段位的高低，未入段的新手为白带，一到五段为黑带，六到八段为红白间隔带，九到十段为红带，目前世界上只有极少数人到达红带的地位。目前为止个人观察，讲道馆的教练大部分是红白间隔带，少数是黑带。未入段的柔道者带色分别是：咖啡色、蓝色、橘色、绿色、黄色、白色。但是讲道馆的人一般都是白带一直系着直到黑带。（另外，讲道馆和国际柔联段位系统还有点不太一样，关于国际柔联段位系统这边了解清楚到时再更新）</p>\n<p>&#8195;&#8195;对于游客来说，可以进入道讲道馆里参观或者亲身练习。但是如果想要参加练习需要至柜台办理手续，一般来说如果只有练习一堂课，通常都是学习受身课程。</p>\n<p>● 关于学费</p>\n<p>&#8195;&#8195;参考下图，单次练习的话是800日元（折合人民币五十元左右）；如果要长期在讲道馆学习，只能选择普通班以及特修班，除了刚开始第一次要额外提交卡费以及入馆费之外，其余的每月只需交5000日元（折合人民币三百多）。其实这个价格相对国内来说已经很优惠，讲道馆不是以盈利为目的，更多的是希望推广柔道。</p>\n<hr>\n<center><img src=\"/image/kodokan/3/fee.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/3/info.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館感想\"><a href=\"#講道館感想\" class=\"headerlink\" title=\"講道館感想\"></a>講道館感想</h2><p>&#8195;&#8195;吾辈之前在深圳开始练习柔道，2017年初开始在讲道馆学习。目前为止的对于讲道馆学习柔道感受是教练普遍很细心，被发现一个动作不对的话会让反复练习；而且能与来自世界不同国家的朋友一起练习柔道很开心。吾辈所在的是国际柔道部的普通班，来自其它国家的学生比较多，授课的教练大部分还是用日语讲，一些基本动作和身体部位的表达有时会用英语表达，估计讲道馆的教练有受过专门的培训。</p>\n<hr>\n<center><img src=\"/image/kodokan/4/eyecatch.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;周一至周六普通班的上课时间是17:30-19:00，跟在之前在国内练习柔道课相比（吾辈国内只在一家柔道馆练习过，所以不能代表全部），讲道馆普通课的热身时间比较长，基本的热身动作、前后滚翻、倒立、受身动作……这些一般都会超半小时，剩下的时间会教一到三个基本的投技或者固技。</p>\n<p>&#8195;&#8195;普通班的次数上满40回后，可以进行特修班申请，经过考试之后就可以晋级。特修班还是在白带系统內……但是相对普通班，可以有实战练习（乱取り）的时间，即两人柔道实战对位练习。特修班后，可以考试申请黑带的资格。</p>\n<p>&#8195;&#8195;普通班与特修班周一至周六的柔道教练都不一样，有些教练的会比较严格，热身阶段已经是体力大耗损；有些则会更偏重动作的讲解，循循善诱，热身轻松但是跟讲很多柔道理论。另外，日本有私人的柔道馆，比如讲道馆一些教练自己会有另外自己的柔道馆，训练风格因教练而异。</p>\n<p>&#8195;&#8195;讲道馆每个月会举行一次“柔之形”的课程，加深学员对柔道动作的记忆。日本有办关于“柔之形”的全国大赛，像是讲道馆有柔道形国际竞技大会，比赛一般是两人为一队，一个叫受、一个叫取，两个人一起演练招式，考核内容除了招式、技术外，还要考核礼法等。因为柔道十分注重“心技体”。</p>\n<p>&#8195;&#8195;刚开始在这里练习柔道不习惯的地方应该是柔道用语，因为之前在国内使用的都是中文发音用语，像“背负投”，“大腰”，“袈裟固”，国际柔联和日本使用的柔道用语是的发音是“Seoi-nage せおいなげ”，“O-goshi おおごし”，“Kesa-gatame けさがため”。后来花了一些时间专门去记住。</p>\n<p>&#8195;&#8195;这边整理了柔道的中日英的柔道用语，希望对有需要的人有所帮助：  </p>\n<p>&#8195;&#8195;● 柔道用語： <a href=\"https://hjtso.github.io/2016/11/30/ja-%E6%9F%94%E9%81%93%E7%94%A8%E8%AA%9E-Judo-terminology/ja/\" title=\"Title\" target=\"_blank\" rel=\"external\">柔道用語</a></p>\n<hr>\n<p>&#8195;&#8195;最后推荐一部关于练习柔道的书《柔道-世界で勝つための実戦的稽古 (差がつく練習法)》，作者：林田和孝</p>\n<hr>\n<center><img src=\"/image/kodokan/4/book.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● <a href=\"https://www.amazon.co.jp/gp/product/4583108419/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;psc=1\" title=\"Title\" target=\"_blank\" rel=\"external\">亚马逊购买链接</a></p>\n<hr>\n<h4 id=\"●-講道館について：\"><a href=\"#●-講道館について：\" class=\"headerlink\" title=\"● 講道館について：\"></a>● 講道館について：</h4><p>&#8195;&#8195;Bunkyo-ku,Tokyo,Japan<br>&#8195;&#8195;東京都文京区春日1-16-30<br>&#8195;&#8195;道场开馆时间∶周一至周六<br>&#8195;&#8195;入馆费∶免费</p>\n<p>&#8195;&#8195;<a href=\"http://kodokanjudoinstitute.org/\" title=\"Title\" target=\"_blank\" rel=\"external\">講道館官网</a><br>&#8195;&#8195;<a href=\"https://www.facebook.com/kodokanjudoinstitute/\" title=\"Title\" target=\"_blank\" rel=\"external\">講道館Facebook主页</a></p>\n<h4 id=\"●-推荐视频について：\"><a href=\"#●-推荐视频について：\" class=\"headerlink\" title=\"● 推荐视频について：\"></a>● 推荐视频について：</h4><p>&#8195;&#8195;<a href=\"https://www.bilibili.com/video/av14169956/?zw\" title=\"Title\" target=\"_blank\" rel=\"external\">柔道-大野将平</a><br>&#8195;&#8195;<a href=\"https://www.facebook.com/pg/kodokanjudoinstitute/videos/?ref=page_internal\" title=\"Title\" target=\"_blank\" rel=\"external\">講道館形研修部／Weekly Kata Class</a><br>&#8195;&#8195;<a href=\"https://v.qq.com/x/page/a0377r8qmso.html\" title=\"Title\" target=\"_blank\" rel=\"external\">讲道馆柔道套路全集</a><br>&#8195;&#8195;<a href=\"http://open.163.com/movie/2016/6/E/P/MBPKPMI4N_MBPKUO1EP.html\" title=\"Title\" target=\"_blank\" rel=\"external\">柔道的创始者-嘉纳治五郎</a>  </p>\n<center><object width=\"640\" height=\"360\"><param name=\"movie\" value=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\"><param name=\"allowScriptAccess\" value=\"always\"><param name=\"wmode\" value=\"transparent\"><embed src=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\" type=\"application/x-shockwave-flash\" width=\"640\" height=\"360\" allowfullscreen=\"true\" wmode=\"transparent\" allowscriptaccess=\"always\"></object></center>  \n\n","excerpt":"","more":"<center><img src=\"/image/kodokan/kodokan.png\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館简介\"><a href=\"#講道館简介\" class=\"headerlink\" title=\"講道館简介\"></a>講道館简介</h2><p>&#8195;&#8195;讲道馆是在1882年由嘉纳治五郎所创立，坐落在日本东京文京区。讲道馆是所有柔道界的总本部，“講道館”的意思是学习兼传播柔道理念的道场。讲道馆对于喜爱柔道者来说是个特殊的地方，应该说，对柔道运动员来说，讲道馆一游应该会列为自己人生旅程清单，而道场也迎来世界各国的众多柔道修行者。</p>\n<hr>\n<center><img src=\"/image/kodokan/1/kano_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館内部\"><a href=\"#講道館内部\" class=\"headerlink\" title=\"講道館内部\"></a>講道館内部</h2><p>&#8195;&#8195;讲道馆由本馆和新馆两部分组成。本馆(7层)有全日本柔道联盟以及相关的事务局，而新馆(8层)有柔道图书馆以及用于柔道的训练道场。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/two_buildings.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;讲道馆的本馆大楼一共有七层，由于现在目前功能主要用于处理柔道事务。在本馆一楼会引导想参观或者见习讲道馆的人到新馆。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/main_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;嘉纳治五郎本尊像。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/main/kano3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;讲道馆的新馆大楼一共有八层楼与一层地下室。本文重点介绍新馆。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/newbuilding.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/nanai.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;地下一楼里有自助餐厅和会议厅，一楼则有停车场和商店。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;二楼则有图书馆和会议室。可细分为:嘉纳治五郎纪念厅、柔道历史纪念厅、文物展示厅、储藏室。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 纪念讲道馆创立100周年之际建成的讲道馆国际柔道中心2楼的资料馆。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 资料展览室。“无心而入自然之妙，无为而穷变化之神”。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<p>&#8195;&#8195;纪念厅里展示许多关于柔道发展史以及许多知名的柔道者的纪念品，例如:手稿、相片。图书馆则收藏有超过7000本关于柔道的书籍。其中二楼又有四间的研究室，一号研究室主要研究有关于柔道的历史与理论、二号研究室研究柔道心理学、三号研究室研究运动生理学、四号研究室研究有关柔道的哲学。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/2_library_5.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 展示嘉纳治五郎的资料的师范室。照片为金井平三作肖像油画和本人的笔迹、常用的大衣等。  </p>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<p>&#8195;&#8195;每一年讲道馆会聚集世界各地研究柔道的学者举办一次研讨会。</p>\n<p>&#8195;&#8195;三楼则可提供住宿（讲道馆可专门为来自海外的修行者开设专业班，可安排专家私人授课及在大型道场中的训练等。新馆的三楼即是住宿的地方）。一共有五间大宿舍间，每间可容纳二十人。  </p>\n<p>&#8195;&#8195;四楼为更衣室，给教练与学员用于更换柔道服的地方，有提供洗浴。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 四楼的柔道课程表</p>\n<p>&#8195;&#8195;五楼、六楼、七楼则为柔道的练习场地。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/5_female_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 五楼的女子部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/5_female_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 五楼的女子部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_international.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的国际部道场</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/6_school_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 六楼的学校道场，家长正在看孩子练柔道</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_0.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;七楼大道场是讲道馆最大的道场，也是最重要的道场，道垫非常舒服，楼顶直通天花板，光线非常的充足。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_00.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 进入柔道馆前，要记得先柔道礼仪（即90度弯腰行礼）</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/7_kano_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 位于大道场的嘉纳治五郎的头像，柔道练习开始和结束都要向嘉纳先生行座礼。</p>\n<hr>\n<p>&#8195;&#8195;八楼则提供座位给参观者观看七楼的柔道者练习。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_0.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/all.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 先上一张从八楼看七楼大道场的全景图</p>\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_4.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_5.jpg\" alt=\"kodokan\"></center>  \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_6.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/8_7.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/all_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/7-8/Summer.png\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 大道馆每在暑期的时候会举办暑期讲习会，吸引来自日本以及国外的训练营柔道者</p>\n<hr>\n<p>&#8195;&#8195;在新馆一楼处有一家柔道用品店，除了柔道相关的用具外，也有其它武术的器具。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<center><img src=\"/image/kodokan/2/1-6/1_shop_3.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 此图来自 - <a href=\"https://www.stroll-tips.com/zh/kodokan-judo/\" title=\"Title\">stroll-tips</a></p>\n<hr>\n<p>&#8195;&#8195;在新馆的一层地下室处有健身房，练习之前要先到4喽登记处登记，周一至周五是到20:00为止，周六19:00。</p>\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_1.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_2.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/2/gym/gym_3.jpg\" alt=\"kodokan\"></center> \n\n\n<hr>\n<h2 id=\"講道館段位\"><a href=\"#講道館段位\" class=\"headerlink\" title=\"講道館段位\"></a>講道館段位</h2><p>&#8195;&#8195;讲道馆的段位需要经过考试，成绩合格后才可以晋升，并且每个段位要满足最小年龄，也就是说，如果太年轻还不能考。而且，讲道馆的考段资格中有一项是积分，而积分必须通过规定的比赛获得。另外，讲道馆的男女柔道段位系统也有所差别。</p>\n<hr>\n<center><img src=\"/image/kodokan/3/male.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 男子段位评定说明（部分）</p>\n<hr>\n<center><img src=\"/image/kodokan/3/female.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● 女子段位评定说明（部分）  </p>\n<p>● 具体请参考讲道馆官网：講道館 - <a href=\"http://kodokanjudoinstitute.org/activity/grade/\" title=\"Title\">昇段資格について</a></p>\n<p>&#8195;&#8195;柔道依选手的水准分段位，通常以腰带的颜色来分辨段位的高低，未入段的新手为白带，一到五段为黑带，六到八段为红白间隔带，九到十段为红带，目前世界上只有极少数人到达红带的地位。目前为止个人观察，讲道馆的教练大部分是红白间隔带，少数是黑带。未入段的柔道者带色分别是：咖啡色、蓝色、橘色、绿色、黄色、白色。但是讲道馆的人一般都是白带一直系着直到黑带。（另外，讲道馆和国际柔联段位系统还有点不太一样，关于国际柔联段位系统这边了解清楚到时再更新）</p>\n<p>&#8195;&#8195;对于游客来说，可以进入道讲道馆里参观或者亲身练习。但是如果想要参加练习需要至柜台办理手续，一般来说如果只有练习一堂课，通常都是学习受身课程。</p>\n<p>● 关于学费</p>\n<p>&#8195;&#8195;参考下图，单次练习的话是800日元（折合人民币五十元左右）；如果要长期在讲道馆学习，只能选择普通班以及特修班，除了刚开始第一次要额外提交卡费以及入馆费之外，其余的每月只需交5000日元（折合人民币三百多）。其实这个价格相对国内来说已经很优惠，讲道馆不是以盈利为目的，更多的是希望推广柔道。</p>\n<hr>\n<center><img src=\"/image/kodokan/3/fee.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<center><img src=\"/image/kodokan/3/info.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<h2 id=\"講道館感想\"><a href=\"#講道館感想\" class=\"headerlink\" title=\"講道館感想\"></a>講道館感想</h2><p>&#8195;&#8195;吾辈之前在深圳开始练习柔道，2017年初开始在讲道馆学习。目前为止的对于讲道馆学习柔道感受是教练普遍很细心，被发现一个动作不对的话会让反复练习；而且能与来自世界不同国家的朋友一起练习柔道很开心。吾辈所在的是国际柔道部的普通班，来自其它国家的学生比较多，授课的教练大部分还是用日语讲，一些基本动作和身体部位的表达有时会用英语表达，估计讲道馆的教练有受过专门的培训。</p>\n<hr>\n<center><img src=\"/image/kodokan/4/eyecatch.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>&#8195;&#8195;周一至周六普通班的上课时间是17:30-19:00，跟在之前在国内练习柔道课相比（吾辈国内只在一家柔道馆练习过，所以不能代表全部），讲道馆普通课的热身时间比较长，基本的热身动作、前后滚翻、倒立、受身动作……这些一般都会超半小时，剩下的时间会教一到三个基本的投技或者固技。</p>\n<p>&#8195;&#8195;普通班的次数上满40回后，可以进行特修班申请，经过考试之后就可以晋级。特修班还是在白带系统內……但是相对普通班，可以有实战练习（乱取り）的时间，即两人柔道实战对位练习。特修班后，可以考试申请黑带的资格。</p>\n<p>&#8195;&#8195;普通班与特修班周一至周六的柔道教练都不一样，有些教练的会比较严格，热身阶段已经是体力大耗损；有些则会更偏重动作的讲解，循循善诱，热身轻松但是跟讲很多柔道理论。另外，日本有私人的柔道馆，比如讲道馆一些教练自己会有另外自己的柔道馆，训练风格因教练而异。</p>\n<p>&#8195;&#8195;讲道馆每个月会举行一次“柔之形”的课程，加深学员对柔道动作的记忆。日本有办关于“柔之形”的全国大赛，像是讲道馆有柔道形国际竞技大会，比赛一般是两人为一队，一个叫受、一个叫取，两个人一起演练招式，考核内容除了招式、技术外，还要考核礼法等。因为柔道十分注重“心技体”。</p>\n<p>&#8195;&#8195;刚开始在这里练习柔道不习惯的地方应该是柔道用语，因为之前在国内使用的都是中文发音用语，像“背负投”，“大腰”，“袈裟固”，国际柔联和日本使用的柔道用语是的发音是“Seoi-nage せおいなげ”，“O-goshi おおごし”，“Kesa-gatame けさがため”。后来花了一些时间专门去记住。</p>\n<p>&#8195;&#8195;这边整理了柔道的中日英的柔道用语，希望对有需要的人有所帮助：  </p>\n<p>&#8195;&#8195;● 柔道用語： <a href=\"https://hjtso.github.io/2016/11/30/ja-%E6%9F%94%E9%81%93%E7%94%A8%E8%AA%9E-Judo-terminology/ja/\" title=\"Title\">柔道用語</a></p>\n<hr>\n<p>&#8195;&#8195;最后推荐一部关于练习柔道的书《柔道-世界で勝つための実戦的稽古 (差がつく練習法)》，作者：林田和孝</p>\n<hr>\n<center><img src=\"/image/kodokan/4/book.jpg\" alt=\"kodokan\"></center> \n\n<hr>\n<p>● <a href=\"https://www.amazon.co.jp/gp/product/4583108419/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;psc=1\" title=\"Title\">亚马逊购买链接</a></p>\n<hr>\n<h4 id=\"●-講道館について：\"><a href=\"#●-講道館について：\" class=\"headerlink\" title=\"● 講道館について：\"></a>● 講道館について：</h4><p>&#8195;&#8195;Bunkyo-ku,Tokyo,Japan<br>&#8195;&#8195;東京都文京区春日1-16-30<br>&#8195;&#8195;道场开馆时间∶周一至周六<br>&#8195;&#8195;入馆费∶免费</p>\n<p>&#8195;&#8195;<a href=\"http://kodokanjudoinstitute.org/\" title=\"Title\">講道館官网</a><br>&#8195;&#8195;<a href=\"https://www.facebook.com/kodokanjudoinstitute/\" title=\"Title\">講道館Facebook主页</a></p>\n<h4 id=\"●-推荐视频について：\"><a href=\"#●-推荐视频について：\" class=\"headerlink\" title=\"● 推荐视频について：\"></a>● 推荐视频について：</h4><p>&#8195;&#8195;<a href=\"https://www.bilibili.com/video/av14169956/?zw\" title=\"Title\">柔道-大野将平</a><br>&#8195;&#8195;<a href=\"https://www.facebook.com/pg/kodokanjudoinstitute/videos/?ref=page_internal\" title=\"Title\">講道館形研修部／Weekly Kata Class</a><br>&#8195;&#8195;<a href=\"https://v.qq.com/x/page/a0377r8qmso.html\" title=\"Title\">讲道馆柔道套路全集</a><br>&#8195;&#8195;<a href=\"http://open.163.com/movie/2016/6/E/P/MBPKPMI4N_MBPKUO1EP.html\" title=\"Title\">柔道的创始者-嘉纳治五郎</a>  </p>\n<center><object width=\"640\" height=\"360\"><param name=\"movie\" value=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\"></param><param name=\"allowScriptAccess\" value=\"always\"></param><param name=\"wmode\" value=\"transparent\"></param><embed src=\"http://swf.ws.126.net/openplayer/v01/-0-2_MBPKPMI4N_MBPKUO1EP-vimg1_ws_126_net//image/snapshot_movie/2016/6/T/Q/MBPKUNMTQ-1430711943278.swf\" type=\"application/x-shockwave-flash\" width=\"640\" height=\"360\" allowFullScreen=\"true\" wmode=\"transparent\" allowScriptAccess=\"always\"></embed></object></center>  \n\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ckipsq8yb0005og64sczpjjti","tag_id":"ckipsq8yd0007og64scbfge14","_id":"ckipsq8yg000cog64198bbbsk"},{"post_id":"ckipsq8yc0006og644301qpmg","tag_id":"ckipsq8yg000bog64sboq7ytl","_id":"ckipsq8yk000hog64t16arhpf"},{"post_id":"ckipsq8ye0008og64nqoh4sjz","tag_id":"ckipsq8yg000bog64sboq7ytl","_id":"ckipsq8ym000log64fjsyromw"},{"post_id":"ckipsq8yf000aog64qpy67cu0","tag_id":"ckipsq8yl000jog64s27ghffo","_id":"ckipsq8yn000pog6413t1xzwa"},{"post_id":"ckipsq8yj000gog6444l2ogdu","tag_id":"ckipsq8yn000nog64eddlxb39","_id":"ckipsq8yp000tog64jq9u2y1t"},{"post_id":"ckipsq8yk000iog64j7zalswt","tag_id":"ckipsq8yn000nog64eddlxb39","_id":"ckipsq8yr000xog64hinf8jsj"},{"post_id":"ckipsq8yl000kog64npxts221","tag_id":"ckipsq8yq000vog646vy83c33","_id":"ckipsq8yt0011og649lbbnq7z"},{"post_id":"ckipsq8yn000oog64sy873j2v","tag_id":"ckipsq8yt0010og64i2fqr0df","_id":"ckipsq8yw0016og64j8kwi2ez"},{"post_id":"ckipsq8yn000qog64xtrg1j9j","tag_id":"ckipsq8yt0010og64i2fqr0df","_id":"ckipsq8z0001aog646dggqqmt"},{"post_id":"ckipsq8yo000sog64era89nlw","tag_id":"ckipsq8yt0010og64i2fqr0df","_id":"ckipsq8z1001eog641m6ou1j6"},{"post_id":"ckipsq8yp000uog64e4iy38jx","tag_id":"ckipsq8yt0010og64i2fqr0df","_id":"ckipsq8z3001iog643cc05p9m"},{"post_id":"ckipsq8yr000wog640t5n4hyu","tag_id":"ckipsq8yt0010og64i2fqr0df","_id":"ckipsq8z4001mog64auau5fpi"},{"post_id":"ckipsq8yr000yog64nbnppcs9","tag_id":"ckipsq8z3001kog6410vcff5h","_id":"ckipsq8z4001oog64v1gwu56d"},{"post_id":"ckipsq8ys000zog64dgnj0l5v","tag_id":"ckipsq8z3001kog6410vcff5h","_id":"ckipsq8z5001qog64icisq4x0"},{"post_id":"ckipsq8yu0012og64fv3azzcs","tag_id":"ckipsq8z3001kog6410vcff5h","_id":"ckipsq8z5001sog64rqdlpa53"},{"post_id":"ckipsq8yu0013og648k3tyjxm","tag_id":"ckipsq8z3001kog6410vcff5h","_id":"ckipsq8z5001uog644ynf9y8i"},{"post_id":"ckipsq8yw0017og64paeuagu3","tag_id":"ckipsq8z5001tog646gwnq5rh","_id":"ckipsq8z6001wog64n4l127qf"},{"post_id":"ckipsq8yz0019og64ce0l2d8n","tag_id":"ckipsq8z5001tog646gwnq5rh","_id":"ckipsq8z6001yog642112fwa2"},{"post_id":"ckipsq8z0001bog64pmqubld1","tag_id":"ckipsq8z5001tog646gwnq5rh","_id":"ckipsq8z70020og647daoltis"},{"post_id":"ckipsq8z1001dog64g012l81d","tag_id":"ckipsq8z5001tog646gwnq5rh","_id":"ckipsq8z70021og64tut7aj55"},{"post_id":"ckipsq8zf0022og64ij8upyk1","tag_id":"ckipsq8yq000vog646vy83c33","_id":"ckipsq8zg0024og64r4z9mh5t"},{"post_id":"ckipsq8zf0023og648fvmxnax","tag_id":"ckipsq8yn000nog64eddlxb39","_id":"ckipsq8zg0026og642vxpk6o5"},{"post_id":"ckipsq8zg0025og6407nx5md1","tag_id":"ckipsq8yn000nog64eddlxb39","_id":"ckipsq8zh0028og646omr32an"},{"post_id":"ckipsq8zh0027og64gc9fy1zi","tag_id":"ckipsq8z3001kog6410vcff5h","_id":"ckipsq8zi0029og64d9d7m4ch"},{"post_id":"ckipsq8zo002aog64kvwzikoz","tag_id":"ckipsq8yn000nog64eddlxb39","_id":"ckipsq8zp002cog648ndkb5yv"},{"post_id":"ckipsq8zo002bog64jsy0jmv1","tag_id":"ckipsq8yq000vog646vy83c33","_id":"ckipsq8zp002dog644n3gxwsx"}],"Tag":[{"name":"PM","_id":"ckipsq8yd0007og64scbfge14"},{"name":"Blockchain","_id":"ckipsq8yg000bog64sboq7ytl"},{"name":"Kendo","_id":"ckipsq8yl000jog64s27ghffo"},{"name":"Deep Learning","_id":"ckipsq8yn000nog64eddlxb39"},{"name":"Judo","_id":"ckipsq8yq000vog646vy83c33"},{"name":"Machine Learning","_id":"ckipsq8yt0010og64i2fqr0df"},{"name":"Reinforcement Learning","_id":"ckipsq8z3001kog6410vcff5h"},{"name":"Sketches","_id":"ckipsq8z5001tog646gwnq5rh"}]}}